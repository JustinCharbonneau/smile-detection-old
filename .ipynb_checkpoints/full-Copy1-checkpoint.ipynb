{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(2)\n",
    "\n",
    "import collections\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing import image\n",
    "\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import labels and images to arrays**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv('./SMILE_Dataset/annotations.csv', header=None, names=['fname','label'])\n",
    "# Shuffle data\n",
    "labels = labels.sample(frac=1).reset_index()\n",
    "\n",
    "x = np.array([image.img_to_array(image.load_img('./SMILE_Dataset/all/'+fname, target_size=(64, 64))) for fname in labels['fname']])\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(labels['label'])\n",
    "y = integer_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create train and validation set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(x, y, test_size=0.10, random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My models are having time using this split of data. Therefore, part of my error analysis will be investigating why!\n",
    "# And not use kfold because it takes too long\n",
    "\n",
    "X_train = np.concatenate((x[0:120], x[160::]), axis = 0)\n",
    "y_train = np.concatenate((y[0:120], y[160::]), axis = 0)\n",
    "\n",
    "X_test = x[120:160]\n",
    "y_test = y[120:160]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "12/11 [================================] - 26s 2s/step - loss: 4.8182 - acc: 0.5722 - val_loss: 48.7558 - val_acc: 0.5250\n",
      "Epoch 2/10\n",
      "12/11 [================================] - 22s 2s/step - loss: 0.8080 - acc: 0.8028 - val_loss: 124.3037 - val_acc: 0.5250\n",
      "Epoch 3/10\n",
      "12/11 [================================] - 22s 2s/step - loss: 0.8563 - acc: 0.8528 - val_loss: 56.4504 - val_acc: 0.7750\n",
      "Epoch 4/10\n",
      "12/11 [================================] - 21s 2s/step - loss: 0.5918 - acc: 0.8889 - val_loss: 56.1945 - val_acc: 0.7250\n",
      "Epoch 5/10\n",
      "12/11 [================================] - 22s 2s/step - loss: 0.4193 - acc: 0.8861 - val_loss: 130.5070 - val_acc: 0.5750\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-171-96d7819fabd4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m history = model.fit_generator(datagen.flow(X_train, y_train, batch_size = 32), epochs = 10, \n\u001b[0;32m     27\u001b[0m                           \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                           callbacks=[learning_rate_reduction])\n\u001b[0m",
      "\u001b[1;32mc:\\users\\justin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\justin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1732\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1733\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\justin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m                                             reset_metrics=False)\n\u001b[0m\u001b[0;32m    221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\justin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1514\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1516\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\justin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3292\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32mc:\\users\\justin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                  rotation_range=10,\n",
    "                                  width_shift_range=0.1,\n",
    "                                  shear_range=0.1,\n",
    "                                  zoom_range=0.1,\n",
    "                                  horizontal_flip=True,\n",
    "                                  fill_mode='nearest')\n",
    "\n",
    "model = build_model4()\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255,\n",
    "                             rotation_range=10, # randomly rotate images in the range (degrees, 0 to 180)\n",
    "                             width_shift_range=0.1, # randomly shift images horizontally (fraction of total width)\n",
    "                             height_shift_range=0.1, \n",
    "                             shear_range=0.1,\n",
    "                             zoom_range=0.1)   \n",
    "\n",
    "datagen.fit(X_train)\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "                                        patience=3, \n",
    "                                        verbose=1, \n",
    "                                        factor=0.5, \n",
    "                                        min_lr=0.00001)\n",
    "\n",
    "history = model.fit_generator(datagen.flow(X_train, y_train, batch_size = 32), epochs = 10, \n",
    "                          validation_data = (X_test,y_test), steps_per_epoch=len(X_train) / 32,\n",
    "                          callbacks=[learning_rate_reduction])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgAAAAHwCAYAAADQAtd+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOzdd3zV1fnA8c+TRQhhhI0gQ0FlGAJGcCMyxAUoiCC4LdW22l+tVuqo/Wn9lVrrblFr3QiiluHAAThbRUAZEkSGgIGwAoSQhKz7/P4434SbcJPcQG5ukvu8X6+8cr/7uTfjPN9zzvccUVWMMcYYE1miwh2AMcYYY2qfJQDGGGNMBLIEwBhjjIlAlgAYY4wxEcgSAGOMMSYCWQJgjDHGRCBLAIwJgohEi8gBEelck/uGk4h0F5GQPAdc/twi8qGITAxFHCJyr4g8faTHGxOpLAEwDZJXAJd8+UQkz285YEFUGVUtVtVEVd1Sk/vWVSKyUET+EGD9GBHZKiLV+t+hqsNVdXoNxDVURDaVO/cDqnrT0Z67imuqiNwWqmsYEw6WAJgGySuAE1U1EdgCXOK37rCCSERiaj/KOu1F4KoA668CXlVVX+2GE1bXAHu877XKfi9NKFkCYCKSiPxJRF4XkRkikg1MEpHTReQrEdknIhki8oSIxHr7x3h3gV295Ve97fNFJFtEvhSRbtXd19t+gYj8ICJZIvKkiPxHRK6tIO5gYvy5iKwXkb0i8oTfsdEi8qiIZIrIBmBEJR/Rv4H2InKG3/GtgAuBl73lkSKy3HtPW0Tk3ko+7y9K3lNVcYjIjSKyxjvvBhG50VvfHHgb6OxXm9PW+1m+6Hf8aBFZ7X1Gi0TkRL9t6SJym4is8j7vGSLSqJK4E4HLgJuBXiKSUm77Od7PI0tEfhKRq7z1Cd573OJt+0xEGgWqwfBiOtd7Xa3fS++Yk0VkgYjsEZHtIvI7EekoIrki0sJvv4HedksqDGAJgIlslwKvAc2B14Ei4NdAa+BMXMH080qOvxK4F2iJq2V4oLr7ikhbYBZwh3fdH4EBlZwnmBgvBE4B+uEKkKHe+puB4UBf7xrjKrqIquYAbwJX+60eD6xU1dXe8gFgEu7zuwT4tYhcXEnsJaqKYwdwEdAM+BnwpIgkq2qWd50tfrU5O/0PFJGewKvALUAbYAHwtn+B6V1vGHAc7nMKVNNR4nJgL+6zWIDf5+Elce8CjwCtcJ/3Km/zo0AyMBD3M78LCLbWJOjfSy8pWoBLjDoAJwCfqOpW4Asv/hKTgBmqWhRkHKaBswTARLIvVPVtVfWpap6qLlHVxapapKobgWeBQZUc/6aqLlXVQmA6kHIE+14MLFfVud62R4HdFZ0kyBj/rKpZqroJ+MTvWuOAR1U1XVUzgamVxAvwEjDO7w75am9dSSyLVPU77/NbAcwMEEsglcbh/Uw2qrMIWAicHcR5wSUp87zYCr1zN8MVxCUeU9Xt3rXfofKf2zXATK/J4zVgot8d9CTgfVWd5f08dqvqchGJBq4FblXVDK9PyBdePMGozu/lSOAnVX1cVfNVdb+qfu1te8mLsaQp4QrglSBjMBHAEgATyX7yXxCRk0TkXa+adD9wP+6uqyLb/V7nAolHsO8x/nGom50rvaKTBBljUNcCNlcSL8CnQBZwiYicgLvDneEXy+ki8omI7BKRLODGALEEUmkcInKxiCz2qrT34WoLgjlvyblLz+cV3OlAR799gvq5iWvCOQeXsAHM9vYtabI4FtgQ4NB2QFwF24JRnd/LY4H1FZxnNtBX3NMoI4BdqvrNEcZkGiBLAEwkK//o2TPAd0B3VW0G/AGQEMeQAXQqWRARoWxhVd7RxJiBKzBKVPqYopeMvIK7878KeE9V/WsnZgJvAceqanPguSBjqTAOEWmMq27/M9BOVVsAH/qdt6rHBbcBXfzOF4X7fLcGEVd5V3vXnS8i23EFbRyHmgF+Ao4PcNwOoKCCbTlAgl98MbjmA3/V+b2sKAZUNRf385mI+/nZ3b8pwxIAYw5pirvjzfHakitr/68p7wD9ReQSrzD4Na7tOhQxzgL+x+sg1gq4M4hjXsLdPV6PX/W/Xyx7VPWgiJyGq34/2jga4QrZXUCx16dgiN/2HUBrEWlayblHisi5Xrv/HUA2sDjI2PxdjStsU/y+rvDOn4TrazBC3KORMSLSWkT6qmox7imKx0Skvdfp8Uwvnu+BpiJyvrd8HxAb4Nr+KvuZz8N1ivyViMSJSDMR8e9D8jLuZ3eRF68xpSwBMOaQ3+LafLNxd12vh/qCqroDV6g8AmTi7ua+BfJDEOM0XHv6KmAJ7k67qvg2AF8D8bgOb/5uBv7s9Va/C1f4HlUcqroP+A2u+noPMBaXJJVs/w53V7vJ6xXftly8q3GfzzRcEjECGFmN9ncAROQsXHPC373+AttVdbsX1ybgClX9Edcp8U4v1m+Ak71T/AZYAyzztv0fIKq6F9dB8SVcrcQeyjZJBFLhz9zrGDkMGAPsBH6gbD+Mz4BoYLGqVti0ZCKTuFo+Y0xd4HUg2waMVdXPwx2Pqf9E5DPgeVV9MdyxmLrFagCMCTMRGSEizb3e9vfiHvv6uorDjKmS1zTTB3gj3LGYuiesCYCIPC8iO0Xkuwq2izfoxXoRWSki/f22XSMi67yvWh+hy5gadBawEff43whgtKpW1ARgTFBEZDrwPvBrb1wHY8oIaxOAiJyDG0zkZVXtE2D7hbj2sgtxz/E+rqoDRaQlsBRIxfWYXQac4rWvGWOMMaYKYa0BUNXPcJ1gKjIKlxyoqn4FtBCRDsD5wEequscr9D+i8mFNjTHGGOOnrvcB6EjZQTFKBvSoaL0xxhhjglDXJ4UINKiIVrL+8BOITAYmAzRp0uSUk046qeaiM8YYY+qwZcuW7VbVgGOL1PUEIJ2yI4Z1wj0ilQ6cW279J4FOoKrP4sbOJjU1VZcuXRqKOI0xxpg6R0QqHPK7rjcBzAOu9p4GOA3IUtUM4ANguIgkeSNyDffWGWOMMSYIYa0BEJEZuDv51iKSjt+wmKr6NPAe7gmA9bhJO67ztu0RkQdwo4gB3K+qlXUmNMYYY4yfsCYAqjqhiu0K/LKCbc8Dz4ciLmOMMaahq+tNAMYYY4wJAUsAjDHGmAhU158CMMYYEwF8PiWvsJjcgmLyCorJLSw69LqgmNyCIg4Wlrw+tD7P2+/QOrd8sLCY6CihSaMYmsTF0KRRNE0axZAQF0Nio2jvewwJjaLd95J94mLcMSX7x0YTE90w75UtATDGmKNUUOSrsNAqfV1YzMGCYvIKiwGIjhKiRIiOwvvu9yVClPc9Oqrsa/fld0zJvn7HHjp31eePKnecOxZEDh9upar3WaYA9/bLC1A4++/nXhdxsNBXrc88SiAhLobGcdEkxEXTONZ9T4iLoWWTRiTERVPsU3IKisjJL2LbvkLvdTE5+UWlP4dgxMdGlSYGCXFewtDoUCLRJC7aSxrca7fNb99yyUajmKiAn29tswTAGNPgldxd5hVWXDjnFRy6kyx7p1m20Cqzzltf5Gt406pHCWUSiYIiX7XfZ1xMlCuUY6O9gtoV2EkJcXRsEV1aeCfExfgV4NE0jovxvvsf6633lo+2EC32aWlCciC/iNx873tBETkFLklwX+53xW07tE9WXiEZ+/LcPt7+wX4+0VFCQlx0mZqJktepXVty06Djj/h9VYclAMaYekNV2ZWdz/qdB9iw6wA/7s4l+2ChV4CXK9SP4u5SBK+giSktlOK9AiopIbZMQVS+0Dp0N+oKu7IFm1sGKFbF53Pfi32Kz6feOqXI561T/++Uvj5su3ds6fZiv2O8cxaXO3/Jsf7nL1lXVC6eYp+WFuaB32cUjWNjDnufdbnqPDpKaBofS9P4WNrVwPlUlYJin18i4ZdQeImEfw1ESc1ETkExud72jKyDbM86WAPRBMcSAGNMnVNU7GPLnlyvoM8pLfA37DpA9sGi0v0ax0bTvHHsobvFuGiaJ8TRoXl0mXXlC61D1cZlC60Eb7/aqKKtU/98iwogPxvy93vf/V/vh4JcOP48aG9DqVdERGgUE02jmGiSmsSFO5yg1KnfQWNMZMnJL2LjrhzW78pmw85DBf2mzBwKiw9Vp7Zt2ojubRMZndKR7m0TOb5NIt3bJtKuWaM60ZYaFqpQmBegwM6uYl2A9cX5VV/voz9A3wlw3t3QvFPo358JOUsAjDEhparsOpDvCvhdB9hQcje/8wDb/Ko7o6OELi0TOL5tIkN6tvMK+iYc3zaRZvGxYXwHNczng4IDQRTUFRXefts0iI5s0Y2gUVO/r2bQrGO5dd76itYBfPkULH4GVv8bBt4EZ98G8c1D+1mZkBI32F5kiOjJgHL3wLu3wZbF0GsU9JsE7fuEOyrTgBQV+/hpbx4bdh4oLehLvu/3q7ZPiIsuvYM/vk2T0jv6Lq2aEBdTd9uMKS6Cgirupg8GqkIv91WQHdz1YhOCLKRL1pff1gwaJUJMo5r7DPZtgUV/gpWvQ+OWMOh3kHoDxNSPKu9IJCLLVDU14DZLACLAxk9h9k2QswuOGwQ/fgbFBdAhBfpfBX3GQuMW4Y7S1BO5Ba7afoNfIb9+5wE27c6loPhQZ7vWiY3o3rZJmSr749sk0qF5fO1W25e2b2cdeRV5fjYU5gZxMamggPYrlOMSIb5Z5YV6XFOIrsMVtBkr4MN74cdPIakrDLkPel/qek+aOsUSAE/EJQBFBbDoAfjvk9CqO4x5Do5JcbUBK2fBt6/Aju8gJh56XuJqBbqeA1F1+C7M1ApVJTOnwO8u/lD1/dZ9eaX7RQl0bplQWrgfX1LYt0mkecJRVNvXdvu2RFdxh11FFXlJgR7bJHL+flRhw0L48A+wczV0PAWGPQBdzwx3ZMaPJQCeiEoAdv0Ab90A21dC6vUw/EGISyi7j6rL5L99FVbNgoNZ0KIzpEyClAnutWnwdmXnsyZjP2u3Z7O+pMDfdYB9uYWl+zSOjea4NoffzXdtnUCjmOhDJ6v19u24cgVzZVXklayLbWx3r0fKVwwrZrqmgextcOKFMPSP0ObEcEdmsASgVEQkAKqw7AV4/y73T23UU3DSRVUfV3gQvn/H1Qps/AQQOO5cVytw0sUQGx/auE3IFRT52LDrAN9v38+ajGzWZOxnTcZ+dh8oAKAJeXRrUsCJSdCjhdIt0cexTYrpEF9AczlIVEEQBfgRt283C66gDmX7tjk6BbmweBp8/igU5kD/q+Hc30PT9uGOLKJZAuBp8AlATibMuwXWvgvHDYZLnz6yP769m2HFDPh2OmRtgfgWkDzOJQMd+tZ83KbGZR7IP1TIewX++p3ZpY/WxUVH0aNdIj07NCOllY+h256m3frXESr7fxBE+3Yw6+p6+7Y5Ojm74bO/wpLn3BMIZ9zivholhjuyiGQJgKdBJwAbFrmOfnl7XfXbwJuPvi3S53OdfL59Fda87dpS258M/a6Cky+HhJY1Ebk5CkXFPjbuzmFNxn7SMvbzvVfo78w+1O7dtmkjTurQjJ4dmtKrQzN6dmhGt9ZNiBXg25dhwR9d7/VTb3AJXkWFdyS1b5ujl7kBFt4PaXOgSVs4dwr0v8aSv1pWZxMAERkBPA5EA8+p6tRy2x8FBnuLCUBbVW3hbSsGVnnbtqjqyKqu1yATgKJ890f25VPQ5iTX0a/9yTV/nby9sOpN10SQscK1vZ50sasVOO5ciIqu6gzmKO3NKSi9my+pvl+38wAFRa7nfWy00L1tU3p2aErP9q6g79mhKa0SA1STb/sW3v0tbF0GXc6ECx+Gdr1q+R2ZiJC+FD68B7Z8Ca16wLD/df0ErM9FWTu/h7S5kNjG9duqIXUyARCRaOAHYBiQDiwBJqhqWgX73wL0U9XrveUDqlqtOqUGlwDs/N519NvxHZz6Mxj+gGv3D7WMlbB8unsWOG8vNOsEKVe6r5bdQn/9Bq7Yp/y4+wBpGdl87xX0azKy2b7/0KA5rRPjvALeFfIntW/G8W0Sq36OPneP66y19Hlo0gbOf9DV5tg/YxNKqrD2PfjoPshcB53PcP+vOgUslyKDKuxMc4X+6jmwey0g7v/o6H/U2GXqagJwOvBHVT3fW/49gKr+uYL9/wvcp6ofecuRmwCouva1D+9xzxSP+jucOKL24yjKd3/U374K6xcCCt3OcU0EPS+pnWSknsvKLfTu6r3q++2uN36+d1cfEyV0b5vISe2b+hX4zWjTtJqd33w+WPGaG841by8M+DkM/r2N5GZqV3GRa3b6+M+QsxN6jYYhf4BWtTP7Xdipuiez0ua6r8z1IFGuFq7XKFer2qxDjV6yriYAY4ERqnqjt3wVMFBVfxVg3y7AV0AnVfdskIgUAcuBImCqqs6p6poNIgE4sAvm/hLWfQDdh7lMMbFtuKOCrHRYPsM1EezbDI2aw8ljXBPBMf0j/g6z2KdszswpU33//fbsMs/Ut2wSV3o3X3Jn371tYtnH7I5ExkpX3Z/+NRx7Glz0cGiaiYwJVv4BNz7Jf590fYtSb3CjCjZpHe7Iap6qa3IrKfT3/ujGneh29qFCP4T/w+tqAnA5cH65BGCAqt4SYN87cYX/LX7rjlHVbSJyHLAIGKKqGwIcOxmYDNC5c+dTNm/eHJo3VBvWLYA5N7vn9YfdDwN/XvcKVp8PNv/H1QqkzYWiPGjb2yUCyePqzR/4YVOmHjalKhT5fGWnc/WbXjUnv4i1O7K9znnZ/LA9m7xC91x7dJRwXOsmpXfzJ3md89o2reGJbfL2wcf/B0v+6YZtHf4AJI+3jnym7sjeDp9MhW9ehrgmcNb/uA7M5ccsqW98Pte/Jm0OpM1zT1NFxUC3QYcK/SataiWUupoABN0EICLfAr9U1f9WcK4XgXdU9c3KrllvawAKD8KC+2Dx09C2l+vo1653uKOq2sEs+O4tlwxsXQZRsXDiBa6J4PjzIDqGomIf+/IK2ZdbyL7cAvZ63/flFrLXWz6QX0Sxz+cVrpSZI72o2K9wVv950UsKcR8+pdy85+XmVPcr1EsK85rSvHGs65TnFfa9OjSje9tE4mND2GlS1Q3M8tG9kJsJp94Ig++24Z5N3bVrrXsaZe170PQYN+Ng3wn1q3Oxz+dq2Uru9Pdvdf/zjj/PFfonXhCWJ6fqagIQg+sEOATYiusEeKWqri6334nAB0A39YIVkSQgV1XzRaQ18CUwqqIOhCXqZQKwYzW8daPrLDLwJveIXx1sW1dV9h8sIqu04C4oU6jHZX5Prx1vc0rWBzTzZbFLWjJbBzE9/2w2a+CxCqKjhKSEWJrGxxIdJUSLEBUlREdR+jomSogScdv9XrvvEBMV5Y4RvO/edr/XZY89dO5oEaKj/Y457FiIjooiOopDMXjHxsdGc0K7RNo3q+Vx73esdtX9W76ETqe63v3HpNTe9Y05Gpv+4xLXrctczeGw+6H7kLpX01nCV+z+1tLmujv9A9vd2Afdh7hC/4QRYU+862QCACAiFwKP4R4DfF5VHxSR+4GlqjrP2+ePQLyqTvE77gzgGcAHRAGPqeq/qrpevUoAVN3Umx/9wXXUGv0P6DGsVi6dV1DMvrwC9ub43ZXneXflOQXeHbtbvze3gKzcQvblFVZ659wsPoakJnG0jodB8g1D8j6kZ87XROFje9IpbOs2hpzuF9O8WXOSEuJonhBL00YxkTvXe3Ud3A+f/Nn9zjRuAUP/F1ImWnW/qX9UYfVsWPi/sHeTqzYfdn/dSWSLi1wzZ9pcNz5Kzk43n0qPYa5TY4/hbm6IOqLOJgC1rd4kAAd2urb+9Qugx/mul39im6M+7ebMHBb/uMevqr2kIC+5W3cFekkP9EAax0aTlBBLi4Q4kprE0qJxHC0SYklKcN9bJMR520tex9EsPoaY6AAF0f4M1zP921dhz0Y3Qlyfy1wTQafUupv11yWqbnyGD+92vzep18F599ogTab+Kypwj6t++hfI2wPJV8B594RnjpLiQjeLatpcN2R6bqYbzrrHcHen32N4nR3p0BIAT71IAH74AOb8wk2oMvxPrv32KArComIfC9bsZPrizXy+bnfp+thooUVCHC0a+xfeJa9LCvVYv9dxNG8cG5q2a1VXjfbtqy7zL8yF1ie6joN9x9eNpxzqop1r4N3bYfMX7kmLi/4GHfuHOypjatbBLPjiUfhqGqjPdX4++7fQOCm01y0qcCOhrp7jCv2D+9xj1yeMcIV+96H1orOiJQCeOp0AFOa5+bWX/BPa9XEd/dr2POLTZWTlMfPrn5i5ZAs79ufToXk8EwZ05uLkDrRtFk+TuOi6Wb2en+2SgG9ecR1qomJcdt37UveHV4eq1sImP9vdFX01zQ3RO+Q+N/FKfeowZUx1ZaW7p1qWv+aaRc+5Awb8rGYnhCo8CBs/9u7034P8LDck9okXuEL/+PPqZB+sylgC4KmzCcD2Va6j367v4bRfwtD7juiX2udTPl+/m+lfbWbh9zvxqXJOjzZMOq0Lg09sE7gavi7btdabqvgNyM5www8fP+RQj9pI69WuCqv/DR/c7T6P/te4wr+WHicypk7YvsqNKLhhoWsOGHIf9L7syPu7FOa5gczS5sDa992MlvHN3aN6vUa5oc7r8ayTlgB46lwC4PO56TMX/NFVZ42e5nqPVlPmgXzeWJbOa4u3sGVPLq2axDHu1GOZcGpnOreq+1VUVfL5IH2J3+M16e7xmuPO9Z6pvajht3nv+gHeu91VSXboCxc9EtnDqBqzYZHrJL19FXRIceNcdDsnuGMLcmDdR+7/yQ8fuOmLGyd5hf5od56YuNDGX0ssAfDUqQQge7vr6LdhkZsYY+RT1bqTU1WWbNrL9MWbmb9qOwXFPgZ0a8mk07pwfu92Rz96XF2lClu/gbTZ7o93X8kAG+f4DbBRPwYbCkpBDnz6EHz5d9feOOQPcMp1Vt1vDLibg1VvwKIHIOsn11w49H8DT2yVf8CNoJo21xX+hbmQ0NoNW95rFHQ9C6Jja/89hJglAJ46kwB8/y7M/ZWrehrxf+4fepDt8fsPFjL7m61MX7yZH3YcoGl8DGP6d2LiwM70aNc0xIHXMaqQsfzQZBp7f3Tjanc9y/1B9xxZfzsQqsKaefD+Xa7GI2WSGwOiBp4GMabBKTwIXz8Dn/3NVeGnTITBd7nRBX/wCv31C6DooJuauNdI9z+i8xkNfnpiSwA8YU8ACnLhg7tg2QvQPhnG/AvanBDUod9tzeLVrzYzd/k28gqLSe7UnIkDO3NJ32NIiGvYv8BBUXWzIq6e49ryMtcDcmiSjZ6X1PgkGyGzez3Mv8PVDrU72fXu7zww3FEZU/fl7oHPHoavn3W1ZOqD4gJo2sH9H+g1Co4dGFE1aJYAeMKaAGSsgDdvcFNhnnGre561io4leQXFvL1iG9MXb2ZFehbxsVGM6tuRiad1JrlThHWAqw5V94hcSZ+BXWsAcX/4vUa57L95p3BHebiCXPj8YTdBSky8+x1JvaHB36EYU+P2boL/PA4xjd3ffKdTI3ZQLEsAPGFJAHw++PIpWHi/a5u+9GnXea0S63dmM33xFt5als7+g0X0aJvIxIGdubR/J5o3bnhtVCG3a60bpjNtjqslAPcPoaSZIKlLeONTdc1C709x7ZjJ493IZ03bhTcuY0y9ZwmAp9YTgP3bYPbP3QhSJ10MI5+ssLd6QZGPD1ZvZ/rizXy1cQ+x0cIFfTow6bQunNo1qW4+s18f7V4Pa7yagYwVbt0x/Q5VD7Y8rnbj2bMR5t8J6z50Ez1d+DB0PbN2YzDGNFiWAHhqNQFY8zbMuwWK8mHEVDdQS4BC/Kc9ucz4eguzlv7E7gMFHNuyMVcO6MLlqZ1onVh/nz2tF/b86Drapc11k4+A65vRa5R7FKh199BduzDPjW72xWNufIPBd7lBTRpgL2RjTPhYAuCplQQg/wB88Hs3v3WHFNfRr1xBUuxTPv7eDc/7yQ+7EGBIz3ZMHNiZc3q0ISrK7vZr3b4tXjPBXDcCIbjZyHqNgt6joc2JNXette/D/N/Bvs1w8uUw7IH600HRGFOvWALgCXkCsPUbN6Lfno1w1v/AuXeVGUxi5/6DvL7kJ2Z8vYVtWQdp27QR4wd0Zvypx3JMi/o1vGSDlrXV1eCkzXVzFKDQ5qRDzQRtex3Z/Ax7N8H8KfDDfHe+Cx+GbmfXdPTGGFPKEgBPyBIAX7Hrcfrxg5DYDi59pvQfu6ry3w2ZTF+8mQ9X76DIp5zdozUTB3ZmSM92xNa34Xkjzf4MNxFI2lw3Baj6oFX3Q8lA++Sqk4HCg/DfJ+Dzv4FEw7lT4LSbrbrfGBNylgB4QpIAZKXD7Jtg0+euQLj4MUhoyb7cAt70hufduDuHpIRYLk89lgkDOtOtdZOajcHUjgM7DyUDP34OWgxJ3Q4lA8f0OzwZWPcRvHeHG6So96Uw/EFo3jE88RtjIo4lAJ4aTwBWz4a3fw3FRXDhX9G+E/jmpyymL97MOyszKCjycUqXJCad1pkL+nQIzVS6JjxyMv2SgU/BVwTNO7sxBnpfCk3auEGfvn8HWvWAC/8Kxw8Od9TGmAhTZxMAERkBPA5EA8+p6tRy268F/gps9VY9parPeduuAe7x1v9JVV+q6no1mgDMn+Im8ul4CjkXP83szY2YvngLazL2k9gohkv7deTKgZ3p2cGmr23wcvfA2vkuGdiwCHyFbn1sAgz6nZvhsYFMLGKMqV/qZAIgItHAD8AwIB1YAkxQ1TS/fa4FUlX1V+WObQksBVIBBZYBp6jq3squWaMJwMo32P3jCh4vHsO/l+8gp6CYXh2aMem0LoxMOYbERjZ6W0TK2+fGHt/1PaReDy2ODXdExpgIVlkCEM5SagCwXlU3AojITGAUkFbpUc75wEequsc79iNgBDAjRLEe5nc/nMCspQk0itnOJX2PYeLAzqQc28IG7Il0jVtA3yvCHYUxxlQpnAlAR+Anv+V0INCMJ2NE5BxcbcFvVPWnCo4N2LNKRCYDkwE6d+5cA2E7g05oy4ntmzGmf0daJFj1rjHGmPolnAlAoFvl8u0RbwMzVDVfRG4CXgLOC/JYt1L1Wbqw2aQAACAASURBVOBZcE0ARx5uWRcl28Atxhhj6q9wPoSeDvg3kHYCtvnvoKqZqprvLf4TOCXYY40xxhhTsXAmAEuAHiLSTUTigPHAPP8dRMT/NnsksMZ7/QEwXESSRCQJGO6tM8YYY0wQwtYEoKpFIvIrXMEdDTyvqqtF5H5gqarOA24VkZFAEbAHuNY7do+IPIBLIgDuL+kQaIwxxpiq2UBAxhhjTANV2WOANhC9McYYE4EsATDGGGMikCUAxhhjTASyBMAYY4yJQJYAGGOMMRHIEgBjjDEmAlkCYIwxxkQgSwCMMcaYCGQJgDHGGBOBLAEwxhhjIpAlAMYYY0wEsgTAGGOMiUCWABhjjDERyBIAY4wxJgJZAmCMMcZEoLAmACIyQkTWish6EZkSYPttIpImIitFZKGIdPHbViwiy72vebUbuTHGGFO/xYTrwiISDfwdGAakA0tEZJ6qpvnt9i2Qqqq5InIz8BBwhbctT1VTajVoY4wxpoEIZw3AAGC9qm5U1QJgJjDKfwdV/VhVc73Fr4BOtRyjMcYY0yCFMwHoCPzkt5zuravIDcB8v+V4EVkqIl+JyOhQBGiMMcY0VGFrAgAkwDoNuKPIJCAVGOS3urOqbhOR44BFIrJKVTcEOHYyMBmgc+fORx+1McYY0wCEswYgHTjWb7kTsK38TiIyFLgbGKmq+SXrVXWb930j8AnQL9BFVPVZVU1V1dQ2bdrUXPTGGGNMPRbOBGAJ0ENEuolIHDAeKNObX0T6Ac/gCv+dfuuTRKSR97o1cCbg33nQGGOMMZUIWxOAqhaJyK+AD4Bo4HlVXS0i9wNLVXUe8FcgEXhDRAC2qOpIoCfwjIj4cEnM1HJPDxhjjDGmEqIasNn90A6ukJ6uqntrJ6TQSU1N1aVLl4Y7DGOMMaZWiMgyVU0NtC2YJoD2uGf0Z3kD9wTqvGeMMcaYeqTKBEBV7wF6AP8CrgXWicj/icjxIY7NGGOMMSESVCdAde0E272vIiAJeFNEHgphbMYYY4wJkSo7AYrIrcA1wG7gOeAOVS0UkShgHfC70IZojDHGmJoWzFMArYHLVHWz/0pV9YnIxaEJyxhjjDGhFEwTwHvAnpIFEWkqIgMBVHVNqAIzxhhjTOgEkwBMAw74Led464wxxhhTTwWTAIj6DRagqj7CO4eAMcYYY45SMAnARhG5VURiva9fAxtDHZgxxhhjQieYBOAm4AxgK24Cn4F4s+sZY4wxpn6qsirfm4RnfC3EYowxxphaEsw4APHADUBvIL5kvapeH8K4jDHGGBNCwTQBvIKbD+B84FOgE5AdyqCMMcYYE1rBJADdVfVeIEdVXwIuAk4ObVjGGGOMCaVgEoBC7/s+EekDNAe6hiwiY4wxxoRcMM/zPysiScA9wDwgEbg3pFEZY4wxJqQqrQHwJvzZr6p7VfUzVT1OVduq6jM1cXERGSEia0VkvYhMCbC9kYi87m1fLCJd/bb93lu/VkTOr4l4jDHGmEhRaQLgjfr3q1BcWESigb8DFwC9gAki0qvcbjcAe1W1O/Ao8Bfv2F64RxN7AyOAf3jnM8YYY0wQgukD8JGI3C4ix4pIy5KvGrj2AGC9qm5U1QJgJjCq3D6jgJe8128CQ0REvPUzVTVfVX8E1nvnM8YYY0wQgukDUPK8/y/91ilw3FFeuyPwk99yySiDAfdR1SIRyQJaeeu/Kndsx6OMxxhjjIkYwYwE2C1E15ZAlwtyn2COdScQmYw3dHHnzp2rE58xxhjTYAUzEuDVgdar6stHee104Fi/5U7Atgr2SReRGNwjiHuCPLYkzmeBZwFSU1MDJgnGGGNMpAmmD8Cpfl9nA38ERtbAtZcAPUSkm4jE4Tr1zSu3zzzgGu/1WGCRNzXxPGC895RAN6AH8HUNxGSMMcZEhGCaAG7xXxaR5rjhgY+K16b/K+ADIBp4XlVXi8j9wFJVnQf8C3hFRNbj7vzHe8euFpFZQBpQBPxSVYuPNiZjjDEmUoi7oa7GASKxwEpV7RmakEInNTVVly5dGu4wjDHGmFohIstUNTXQtmD6ALzNoQ52Ubhn9mfVXHjGGGOMqW3BPAb4sN/rImCzqqaHKB5jjDHG1IJgEoAtQIaqHgQQkcYi0lVVN4U0MmOMMcaETDBPAbwB+PyWi711xhhjjKmngkkAYryhegHwXseFLiRjjDHGhFowCcAuESl97l9ERgG7QxeSMcYYY0ItmD4ANwHTReQpbzkdCDg6oDHGGGPqh2AGAtoAnCYiibhxA7JDH5YxxhhjQqnKJgAR+T8RaaGqB1Q1W0SSRORPtRGcMcYYY0IjmD4AF6jqvpIFVd0LXBi6kIwxxhgTasEkANEi0qhkQUQaA40q2d8YY4wxdVwwnQBfBRaKyAve8nXAS6ELyRhjjDGhFkwnwIdEZCUwFBDgfaBLqAMzxhhjTOgE0wQAsB03GuAYYAiwJmQRGWOMMSbkKqwBEJETgPHABCATeB33GODgWorNGGOMMSFSWRPA98DnwCWquh5ARH5TK1EZY4wxJqQqawIYg6v6/1hE/ikiQ3B9AI6aiLQUkY9EZJ33PSnAPiki8qWIrBaRlSJyhd+2F0XkRxFZ7n2l1ERcxhhjTKSoMAFQ1dmqegVwEvAJ8BugnYhME5HhR3ndKcBCVe0BLPSWy8sFrlbV3sAI4DERaeG3/Q5VTfG+lh9lPMYYY0xEqbIToKrmqOp0Vb0Y6AQsJ3CBXR2jOPQo4UvA6ADX/UFV13mvtwE7gTZHeV1jjDHGEPxTAACo6h5VfUZVzzvK67ZT1QzvnBlA28p2FpEBuCmIN/itftBrGnjUf6CiAMdOFpGlIrJ0165dRxm2McYY0zBUKwGoDhFZICLfBfgaVc3zdABeAa5TVZ+3+ve4polTgZbAnRUdr6rPqmqqqqa2aWMVCMYYYwwENxLgEVHVoRVtE5EdItJBVTO8An5nBfs1A94F7lHVr/zOneG9zPdGKLy9BkM3xhhjGryQ1QBUYR5wjff6GmBu+R1EJA6YDbysqm+U29bB+y64/gPfhTRaY4wxpoEJVwIwFRgmIuuAYd4yIpIqIs95+4wDzgGuDfC433QRWQWsAloDNj2xMcYYUw2iquGOodakpqbq0qVLwx2GMcYYUytEZJmqpgbaFq4aAGOMMcaEkSUAxhhjTASyBMAYY4yJQJYAGGOMMRHIEgBjjDEmAlkCYIwxxkQgSwCMMcaYCGQJgDHGGBOBLAEwxhhjIpAlAMYYY0wEsgTAGGOMiUCWABhjjDERyBIAY4wxJgJZAmCMMcZEIEsAjDHGmAgUlgRARFqKyEciss77nlTBfsUistz7mue3vpuILPaOf11E4movemOMMab+C1cNwBRgoar2ABZ6y4HkqWqK9zXSb/1fgEe94/cCN4Q2XGOMMaZhCVcCMAp4yXv9EjA62ANFRIDzgDeP5HhjjDHGhC8BaKeqGQDe97YV7BcvIktF5CsRKSnkWwH7VLXIW04HOoY2XGOMMaZhiQnViUVkAdA+wKa7q3Gazqq6TUSOAxaJyCpgf4D9tJI4JgOTATp37lyNSxtjjDENV8gSAFUdWtE2EdkhIh1UNUNEOgA7KzjHNu/7RhH5BOgHvAW0EJEYrxagE7CtkjieBZ4FSE1NrTBRMMYYYyJJuJoA5gHXeK+vAeaW30FEkkSkkfe6NXAmkKaqCnwMjK3seGOMMcZULFwJwFRgmIisA4Z5y4hIqog85+3TE1gqIitwBf5UVU3ztt0J3CYi63F9Av5Vq9EbY4wx9Zy4G+rIkJqaqkuXLg13GMYYY0ytEJFlqpoaaJuNBGiMMcZEIEsAjDHGmAhkCYAxxhgTgSwBMMYYYyKQJQDGGGNMBLIEwBhjjIlAlgAYY4wxEShkQwEbY4ypeYWFhaSnp3Pw4MFwh2LqkPj4eDp16kRsbGzQx1gCYIwx9Uh6ejpNmzala9euuNnRTaRTVTIzM0lPT6dbt25BH2dNAMYYU48cPHiQVq1aWeFvSokIrVq1qnatkCUAxhhTz1jhb8o7kt8JSwCMMcYELTMzk5SUFFJSUmjfvj0dO3YsXS4oKAjqHNdddx1r166tdJ+///3vTJ8+vSZCBmDHjh3ExMTwr3/Z3HElbDIgY4ypR9asWUPPnj3DHQYAf/zjH0lMTOT2228vs15VUVWiourOPeYTTzzBG2+8QaNGjViwYEHIrlNUVERMTHi61wX63bDJgIwxxoTU+vXr6dOnDzfddBP9+/cnIyODyZMnk5qaSu/evbn//vtL9z3rrLNYvnw5RUVFtGjRgilTptC3b19OP/10du7cCcA999zDY489Vrr/lClTGDBgACeeeCL//e9/AcjJyWHMmDH07duXCRMmkJqayvLlywPGN2PGDB577DE2btzI9u3bS9e/++679O/fn759+zJ8+HAAsrOzueaaazj55JNJTk5mzpw5pbGWmDlzJjfeeCMAkyZN4re//S2DBw/mrrvu4quvvuL000+nX79+nHnmmaxbtw5wycFvfvMb+vTpQ3JyMv/4xz/44IMPuPzyy0vPO3/+fMaNG3fUP49g2FMAxhhTT/3v26tJ27a/Rs/Z65hm3HdJ7yM6Ni0tjRdeeIGnn34agKlTp9KyZUuKiooYPHgwY8eOpVevXmWOycrKYtCgQUydOpXbbruN559/nilTphx2blXl66+/Zt68edx///28//77PPnkk7Rv35633nqLFStW0L9//4Bxbdq0ib1793LKKacwduxYZs2axa233sr27du5+eab+fzzz+nSpQt79uwBXM1GmzZtWLVqFarKvn37qnzvGzZsYOHChURFRZGVlcUXX3xBdHQ077//Pvfccw+vv/4606ZNY9u2baxYsYLo6Gj27NlDixYtuPXWW8nMzKRVq1a88MILXHfdddX96I+I1QAYY4ypEccffzynnnpq6fKMGTPo378//fv3Z82aNaSlpR12TOPGjbngggsAOOWUU9i0aVPAc1922WWH7fPFF18wfvx4APr27Uvv3oETlxkzZnDFFVcAMH78eGbMmAHAl19+yeDBg+nSpQsALVu2BGDBggX88pe/BFznuqSkpCrf++WXX17a5LFv3z4uu+wy+vTpw+23387q1atLz3vTTTcRHR1der2oqCiuvPJKXnvtNfbs2cOyZctKayJCLSw1ACLSEngd6ApsAsap6t5y+wwGHvVbdRIwXlXniMiLwCAgy9t2raoGrvcxxpgG6kjv1EOlSZMmpa/XrVvH448/ztdff02LFi2YNGlSwMfU4uLiSl9HR0dTVFQU8NyNGjU6bJ9g+7DNmDGDzMxMXnrpJQC2bdvGjz/+iKoG7D0faH1UVFSZ65V/L/7v/e677+b888/nF7/4BevXr2fEiBEVnhfg+uuvZ8yYMQBcccUVpQlCqIWrBmAKsFBVewALveUyVPVjVU1R1RTgPCAX+NBvlztKtlvhb4wxdcv+/ftp2rQpzZo1IyMjgw8++KDGr3HWWWcxa9YsAFatWhWwhiEtLY3i4mK2bt3Kpk2b2LRpE3fccQczZ87kzDPPZNGiRWzevBmgtAlg+PDhPPXUU4ArtPfu3UtUVBRJSUmsW7cOn8/H7NmzK4wrKyuLjh07AvDiiy+Wrh8+fDjTpk2juLi4zPWOPfZYWrduzdSpU7n22muP7kOphnAlAKOAl7zXLwGjq9h/LDBfVXNDGpUxxpga0b9/f3r16kWfPn342c9+xplnnlnj17jlllvYunUrycnJ/O1vf6NPnz40b968zD6vvfYal156aZl1Y8aM4bXXXqNdu3ZMmzaNUaNG0bdvXyZOnAjAfffdx44dO+jTpw8pKSl8/vnnAPzlL39hxIgRDBkyhE6dOlUY15133skdd9xx2Hv++c9/Tvv27UlOTqZv376lyQvAlVdeSbdu3TjhhBOO6jOpjrA8Bigi+1S1hd/yXlWtsJFFRBYBj6jqO97yi8DpQD5eDYKq5ld1XXsM0BhT39WlxwDDraioiKKiIuLj41m3bh3Dhw9n3bp1YXsM72jcdNNNnH766VxzzTVHfI7qPgYYsk9JRBYA7QNsurua5+kAnAz41x/9HtgOxAHPAncC9x9+NIjIZGAyQOfOnatzaWOMMXXYgQMHGDJkCEVFRagqzzzzTL0s/FNSUkhKSuKJJ56o1euG7JNS1aEVbRORHSLSQVUzvAJ+ZyWnGgfMVtVCv3NneC/zReQF4PaAR7p9n8UlCaSmpkbOqEfGGNPAtWjRgmXLloU7jKNW0dgFoRauPgDzgJJ6jmuAuZXsOwGY4b/CSxoQ151yNPBdCGI0xhhjGqxwJQBTgWEisg4Y5i0jIqki8lzJTiLSFTgW+LTc8dNFZBWwCmgN/KkWYjbGGGMajLA0lqhqJjAkwPqlwI1+y5uAjgH2Oy+U8RljjDENnY0EaIwxxkQgSwCMMcYE7dxzzz1sUJ/HHnuMX/ziF5Uel5iYCLhR+MaOHVvhuat6VPuxxx4jN/fQkDAXXnhhUGP1B6tkYqFIYAmAMcaYoE2YMIGZM2eWWTdz5sygC81jjjmGN99884ivXz4BeO+998rM0nc01qxZg8/n47PPPiMnJ6dGzhlIRcMd1zZLAIwxxgRt7NixvPPOO+Tnu7HXNm3axLZt2zjrrLNKn8vv378/J598MnPnHv6A16ZNm+jTpw8AeXl5jB8/nuTkZK644gry8vJK97v55ptLpxK+7777AHjiiSfYtm0bgwcPZvDgwQB07dqV3bt3A/DII4/Qp08f+vTpUzqV8KZNm+jZsyc/+9nP6N27N8OHDy9zHX+vvfYaV111FcOHD2fevHml69evX8/QoUPp27cv/fv3Z8OGDQA89NBDnHzyyfTt27d0BkP/Wozdu3fTtWtXwA0JfPnll3PJJZcwfPjwSj+rl19+uXS0wKuuuors7Gy6detGYaF7Gn7//v107dq1dPlI1b8RE4wxxjjzp8D2VTV7zvYnwwVTK9zcqlUrBgwYwPvvv8+oUaOYOXMmV1xxBSJCfHw8s2fPplmzZuzevZvTTjuNkSNHBpwAB2DatGkkJCSwcuVKVq5cWWY63wcffJCWLVtSXFzMkCFDWLlyJbfeeiuPPPIIH3/8Ma1bty5zrmXLlvHCCy+wePFiVJWBAwcyaNCg0vH7Z8yYwT//+U/GjRvHW2+9xaRJkw6L5/XXX+ejjz5i7dq1PPXUU6W1GhMnTmTKlClceumlHDx4EJ/Px/z585kzZw6LFy8mISGhdFz/ynz55ZesXLmydIrkQJ9VWloaDz74IP/5z39o3bo1e/bsoWnTppx77rm8++67jB49mpkzZzJmzBhiY2OrvGZlrAbAGGNMtfg3A/hX/6sqd911F8nJyQwdOpStW7eyY8eOCs/z2WeflRbEycnJJCcnl26bNWsW/fv3p1+/fqxevTrgRD/+vvjiCy699FKaNGlCYmIil112WekY/t26dSMlJQWoeMrhJUuW0KZNG7p06cKQIUP45ptv2Lt3L9nZ2WzdurV0PoH4+HgSEhJYsGAB1113HQkJCcChqYQrM2zYsNL9KvqsFi1axNixY0sTnJL9b7zxRl544QUAXnjhBa677roqr1cVqwEwxpj6qpI79VAaPXo0t912G9988w15eXmld+7Tp09n165dLFu2jNjYWLp27RpwCmB/gWoHfvzxRx5++GGWLFlCUlIS1157bZXnqWxem5KphMFNJxyoCWDGjBl8//33pVX2+/fv56233mLcuHEVXi9Q7DExMfh8PqDyKYMr+qwqOu+ZZ57Jpk2b+PTTTykuLi5tRjkaVgNgjDGmWhITEzn33HO5/vrry3T+y8rKom3btsTGxvLxxx+XTrNbkXPOOYfp06cD8N1337Fy5UrAFb5NmjShefPm7Nixg/nz55ce07RpU7KzswOea86cOeTm5pKTk8Ps2bM5++yzg3o/Pp+PN954g5UrV5ZOGTx37lxmzJhBs2bN6NSpE3PmzAEgPz+f3Nxchg8fzvPPP1/aIbGkCaBr166lwxNX1tmxos9qyJAhzJo1i8zMzDLnBbj66quZMGFCjdz9gyUAxhhjjsCECRNYsWIF48ePL103ceJEli5dSmpqKtOnT+ekk06q9Bw333wzBw4cIDk5mYceeogBAwYA7lG8fv360bt3b66//voy0+pOnjyZCy64oLQTYIn+/ftz7bXXMmDAAAYOHMiNN95Iv379gnovn332GR07dqRjx0Pjzp1zzjmkpaWRkZHBK6+8whNPPEFycjJnnHEG27dvZ8SIEYwcOZLU1FRSUlJ4+OGHAbj99tuZNm0aZ5xxRmnnxEAq+qx69+7N3XffzaBBg+jbty+33XZbmWP27t1bY48phmU64HCx6YCNMfWdTQccud58803mzp3LK6+8EnB7nZkO2BhjjDE145ZbbmH+/Pm89957NXZOSwCMMcaYOu7JJ5+s8XNaHwBjjDEmAlkCYIwx9Uwk9d0ywTmS3wlLAIwxph6Jj48nMzPTkgBTSlXJzMwkPj6+WsdZHwBjjKlHOnXqRHp6Ort27Qp3KKYOiY+Pp1OnTtU6JiwJgIhcDvwR6AkMUNWAz+aJyAjgcSAaeE5Vp3rruwEzgZbAN8BVqlpQC6EbY0xYxcbG0q1bt3CHYRqAcDUBfAdcBnxW0Q4iEg38HbgA6AVMEJFe3ua/AI+qag9gL3BDaMM1xhhjGpawJACqukZV11ax2wBgvapu9O7uZwKjxA2SfB5QMsbiS8Do0EVrjDHGNDx1uRNgR+Anv+V0b10rYJ+qFpVbb4wxxpgghawPgIgsANoH2HS3qs4N5hQB1mkl6yuKYzIw2Vs8ICJV1TxUR2ug4sGeTU2xz7l22Odce+yzrh32OUOXijaELAFQ1aFHeYp04Fi/5U7ANtwPs4WIxHi1ACXrK4rjWeDZo4wlIBFZWtEYy6bm2OdcO+xzrj32WdcO+5wrV5ebAJYAPUSkm4jEAeOBeeoefv0YGOvtdw0QTI2CMcYYYzxhSQBE5FIRSQdOB94VkQ+89ceIyHsA3t39r4APgDXALFVd7Z3iTuA2EVmP6xPwr9p+D8YYY0x9FpZxAFR1NjA7wPptwIV+y+8Bh019pKobcU8JhFtImhbMYexzrh32Odce+6xrh33OlRAbTtIYY4yJPHW5D4AxxhhjQsQSgCMkIiNEZK2IrBeRKeGOpyESkWNF5GMRWSMiq0Xk1+GOqSETkWgR+VZE3gl3LA2ViLQQkTdF5Hvv9/r0cMfUEInIb7z/Gd+JyAwRqd4sORHCEoAjUMUwxabmFAG/VdWewGnAL+1zDqlf4zrcmtB5HHhfVU8C+mKfd40TkY7ArUCqqvbBzSUzPrxR1U2WAByZgMMUhzmmBkdVM1T1G+91Nu6fpY36GAIi0gm4CHgu3LE0VCLSDDgH76klVS1Q1X3hjarBigEai0gMkEAlY8VEMksAjkxFwxSbEBGRrkA/YHF4I2mwHgN+B/jCHUgDdhywC3jBa2p5TkSahDuohkZVtwIPA1uADCBLVT8Mb1R1kyUAR6ZawxGboyMiicBbwP+o6v5wx9PQiMjFwE5VXRbuWBq4GKA/ME1V+wE5gPUfqmEikoSrke0GHAM0EZFJ4Y2qbrIE4MhUNEyxqWEiEosr/Ker6r/DHU8DdSYwUkQ24ZqzzhORV8MbUoOUDqSrakkt1pu4hMDUrKHAj6q6S1ULgX8DZ4Q5pjrJEoAjE3CY4jDH1OB4Uz//C1ijqo+EO56GSlV/r6qdVLUr7nd5karaHVMNU9XtwE8icqK3agiQFsaQGqotwGkikuD9DxmCdbYMKCwjAdZ3qlokIiXDFEcDz/sNU2xqzpnAVcAqEVnurbvLGyHSmProFmC6d+OwEbguzPE0OKq6WETeBL7BPUn0LTYiYEA2EqAxxhgTgawJwBhjjIlAlgAYY4wxEcgSAGOMMSYCWQJgjDHGRCBLAIwxxpgIZAmAMcYYE4EsATDGGGMikCUAxoSQiESLyAER6VyT+4aTiHQXkZAMIFL+3CLyoYhMDEUcInKviDx9pMcbU99ZAmCMH68ALvnyiUie33LAgqgyqlqsqomquqUm962rRGShiPwhwPoxIrJVRKr1P0dVh6vq9BqIa6g314H/uR9Q1ZuO9twBrnWjiHxS0+c1pqZZAmCMH68ATlTVRNyY4pf4rTusIPLmGzeHvIgbvrm8q4BXVdWmGzamjrAEwJhqEJE/icjrIjJDRLKBSSJyuoh8JSL7RCRDRJ7wZjFERGJEREWkq7f8qrd9vohki8iXItKtuvt62y8QkR9EJEtEnhSR/4jItRXEHUyMPxeR9SKyV0Se8Ds2WkQeFZFMEdkAjKjkI/o30F5ESmdfE5FWwIXAy97ySBFZ7r2nLSJybyWf9xcl76mqOLw77zXeeTeIyI3e+ubA20Bnv9qctt7P8kW/40eLyGrvM1rkN2kPIpIuIreJyCrv854hIo0q+Rwqej+dROQdEdkjIutE5Hq/baeJyDcisl9EdojIX731CSLymve+94nI1yLSurrXNqY8SwCMqb5LgdeA5sDruAlHfg20xk1gNAL4eSXHXwncC7TE1TI8UN19RaQtMAu4w7vuj8CASs4TTIwXAqcA/XCJzVBv/c3AcKCvd41xFV1EVXNw09xe7bd6PLDSb8KsA8Ak3Od3CfBrEbm4kthLVBXHDuAioBnwM+BJEUlW1SzvOlv8anN2+h8oIj2BV3GT9bQBFgBvlyRJnnHAMOA43OcUqKajKq/jflbHAFcAD4nIIG/bk8BfVbUZ0B33OYKbMCgBN+14K+AXwMEjuLYxZVgCYEz1faGqb6uqT1XzVHWJqi5W1SJV3YibeWxQJce/qapLvbnKpwMpR7DvxcByVZ3rbXsU2F3RSYKM8c+qmqWqm4BP/K41DnhUVdNVNROYWkm8AC8B4/zukK/21pXEskhVv/M+vxXAzACxBFJpHN7PjrLz6wAAIABJREFUZKM6i4CFwNlBnBe8Kb292Aq9czcDBvrt85iqbveu/Q6V/9wO49XeDACmqOpBVf0GeIFDiUQhbprxVqqaraqL/da3Brp7/USWquqB6lzbmEAsATCm+n7yXxCRk0TkXRHZLiL7gftx/7Arst3vdS6QeAT7HuMfh7ppPdMrOkmQMQZ1LWBzJfECfApkAZeIyAm4GoUZfrGcLiKfiMguEckCbgwQSyCVxiEiF4vIYq96fR+utiDYqvJj/M/n9VVIBzr67VOdn1tF19jt1ZKU2Ox3jeuAXsBar5r//9u78/iqqnP/458n88QQSJAxkgAqg4iYMigg1qpoVay1KlWLVkrrbWttf3rF3ra3tbbX21qnDlbrUGsR6sVSabWlCrRorUBQZFJkxpAIQeYQCAnP74+9iQdMQiA5OUnO9/165XXO3nvtvZ9zAlnPXnvttS4J1/+WoEXiOQs6Ut5r6nsiTUAJgMjxO/rRs0eB5QRXaO2B7wEW5RhKCZqEATAz48jK6miNibEU6BWxXO9jimEy8gzBlf8NwEvuHtk6MR14Hujl7h2AxxsYS51xmFk6QZP5/wAnuXtH4O8Rxz3W44IlwMkRx0sg+H43NyCuhioBcswsM2Jd3uFzuPsqd78W6AL8DHjezNLcvdLdv+/u/YFRBLegjvuJFJGjKQEQabx2BFe85eG95Pru/zeVvwBDzeyy8GrwGwT3rqMR43PAbWbWI+zQd2cD9nmaoJ/BF4lo/o+IZbu77zezEQTN742NIxVIAcqA6rBPwfkR27cQVL7t6jn25WY2NrzvfwewB1hQR/ljSTCztMgfd18PFAE/NrNUMxtCcNU/FcDMbjCznLD1YRdB0nLIzD5pZoPCpGQ3wS2B6hOMS6SGEgCRxvt/wESCCuNRgo5eUeXuWwg6kd0PfAj0Ad4CDkQhxkcI7qcvAxbxUee0+uJbCywE0oAXj9p8C/A/FjxF8W2CyrdRcbj7TuCbwExgO3AVQZJ0ePtyglaHDWFP+i5HxbuC4Pt5hCCJGAdcHvYHOBGjgYqjfiD4nfUjuJ0wA/i2u88Lt10CvBN+L/cB17h7JcGtgz8SVP4rCG4H1NxSETlRFrTWiUhrZmaJBE3MV7n7q7GOR0RaPrUAiLRSZjbOzDqEve2/S/Co38IYhyUirURMEwAze9LMtprZ8jq2mwUDlqwxs6VmNjRi28RwII3VZjax+aIWaTFGAesIHv8bB1zh7nXdAhAROUJMbwGY2RiCQUF+5+6Datl+CcHAHJcQPI/7kLsPN7NOBJ1pCgk6yiwGznL3Hc0WvIiISCsW0xYAd59P0GGnLuMJkgN39zeAjmbWDbgIeNndt4eV/svUPzypiIiIRGjpfQB6cOTAH4cH5qhrvYiIiDRASx9NqrbBQbye9R8/gNlkYDJAZmbmWaeddlrTRSciItKCLV68eJu71zpGSEtPAIo5cuSvngSPOhUDY49a/4/aDuDujxGMe05hYaEXFRVFI04REZEWx8zqHLq7pd8CmAV8IXwaYASwy91LgdnAhWaWbWbZBGN+z45loCIiIq1JTFsAzGwawZV8jpkVA/8NJAO4+6+BlwieAFhDMPnGTeG27Wb2Q4LRwADudvf6OhOKiIhIhJgmAO4+4RjbHfhqHdueBJ6MRlwiIiJtXUvvAyAiIs3o4MGDFBcXs3///liHIschLS2Nnj17kpyc3OB9lACIiEiN4uJi2rVrR+/evQlmmZaWzt358MMPKS4uJj8/v8H7tfROgCIi0oz2799P586dVfm3ImZG586dj7vVRgmAiIgcQZV/63MivzMlACIi0mJ8+OGHDBkyhCFDhtC1a1d69OhRs1xZWdmgY9x0002sWrWq3jK//OUvmTp1alOEzKhRo1iyZEmTHKs5qQ+AiIi0GJ07d66pTL///e+TlZXF7bfffkQZd8fdSUio/Rr2qaeeOuZ5vvrVWh8wiytqARARkRZvzZo1DBo0iK985SsMHTqU0tJSJk+eTGFhIQMHDuTuu++uKXv4iryqqoqOHTsyZcoUzjjjDEaOHMnWrVsB+M53vsODDz5YU37KlCkMGzaMU089lddffx2A8vJyPvvZz3LGGWcwYcIECgsLG3ylX1FRwcSJEzn99NMZOnQo8+fPB2DZsmV84hOfYMiQIQwePJh169axZ88eLr74Ys444wwGDRrEjBkzmvKrq5MSABERaRVWrlzJzTffzFtvvUWPHj249957KSoq4u233+bll19m5cqVH9tn165dnHvuubz99tuMHDmSJ5+sffgYd2fhwoX89Kc/rUkmfv7zn9O1a1fefvttpkyZwltvvdXgWB9++GFSUlJYtmwZzzzzDDfccAOVlZX86le/4vbbb2fJkiUsWrSI7t2789JLL9G7d2/efvttli9fzgUXXHBiX9Bx0i2AEzT33S1s21vJZYO7k56SGOtwRESa3A/+vIKVJbub9JgDurfnvy8beEL79unTh0984hM1y9OmTeOJJ56gqqqKkpISVq5cyYABA47YJz09nYsvvhiAs846i1dffbXWY1955ZU1ZTZs2ADAa6+9xp133gnAGWecwcCBDY/7tdde44477gBg4MCBdO/enTVr1nD22Wdzzz33sHHjRq688kr69u3L4MGDmTJlClOmTOGyyy7jnHPOafB5GkMtACfoT2+V8J8zljL8x6/wgz+vYM3WPbEOSUSkTcvMzKx5v3r1ah566CHmzp3L0qVLGTduXK2PwaWkpNS8T0xMpKqqqtZjp6amfqxMMBjtialr3xtuuIGZM2eSmprKBRdcwPz58+nfvz9FRUUMHDiQO+64gx//+McnfN7joRaAE/TQtUO4bngeUxds4vdvbOSpf21geH4nPj88j3GDupKapFYBEWndTvRKvTns3r2bdu3a0b59e0pLS5k9ezbjxo1r0nOMGjWK5557jtGjR7Ns2bJabzHUZcyYMUydOpUxY8bwzjvvUFpaSt++fVm3bh19+/blG9/4BqtXr2bp0qX06dOHnJwcbrjhBtLT05k+fXqTfo66KAE4QWbG8ILODC/ozLa9A5ixuJhnF2ziG9OX0Ckzhc8V9uTzw/I4uXPmsQ8mIiLHZejQoQwYMIBBgwZRUFAQlWbzr3/963zhC19g8ODBDB06lEGDBtGhQ4day1500UU1w/COHj2aJ598ki9/+cucfvrpJCcn87vf/Y6UlBSeffZZpk2bRnJyMt27d+eee+7h9ddfZ8qUKSQkJJCSksKvf/3rJv8stbHGNHG0NoWFhV5UVBS14x865Ly2ZhtTF2zklXe2Un3IGd0vh+uGn8yn+nchKVF3XESkZXvnnXfo379/rMNoEaqqqqiqqiItLY3Vq1dz4YUXsnr1apKSWua1c22/OzNb7O6FtZVvmZ+ilUpIMMacksuYU3L5YNd+/rDofaYv2sRXfr+Yk9qncs0n8pgwrBfdOqTHOlQRETmGvXv3cv7551NVVYW78+ijj7bYyv9EqAUgyqqqDzFvVRlTF2zkn++VYcAnTzuJ60bkMaZfLokJGnJTRFoOtQC0XmoBaGGSEhO4YMBJXDDgJN7fvo9pCzfxXNH7vPLOFnpmpzNhWB5XF/Yit11qrEMVEZE4EtOb0mY2zsxWmdkaM5tSy/YHzGxJ+POeme2M2FYdsW1W80Z+Ynp1yuA/x53G61PO5xefP5Ne2Rn8dPYqzr53Dl999k1eX7utUY+diIiINFTMWgDMLBH4JXABUAwsMrNZ7l7znIW7fzOi/NeBMyMOUeHuQ5or3qaUkpTApYO7c+ng7qzZupdpCzcxY3ExLy4tpSA3k88Py+Oqs3rSMSPl2AeTFmdfZRV791fRpX1arEMREalTLG8BDAPWuPs6ADObDowH6nrQcgLw380UW7Pp2yWL7146gDsuOpUXl5YydcFG7nnxHX4yexWXDu7GdcNPZmheR03P2cIcOuSU7t7PurK9rN26l3XbyllXVs66sr2U7AoGI7lwwEl868JTOK1r+xhHKyLycbFMAHoA70csFwPDaytoZicD+cDciNVpZlYEVAH3uvufohVoc0hLTuSzZ/Xks2f1ZGXJbp5duJGZb27mj29u5rSu7bhuxMlcMaQ77dKSYx1qXCk/UMX6beWsLdvL2rCCX1dWzrpte9l/8FBNuXapSRTkZjK8oDMFOZkcqDrE069v4OKHXuWywd257VP9KMjNiuEnEWkdxo4dy1133cVFF11Us+7BBx/kvffe41e/+lWd+2VlZbF3715KSkq49dZba51QZ+zYsdx3330UFtbaJ67mXJMnTyYjIwOASy65hGeffZaOHTs24lPVPbNhLMUyAajtkrauG+DXAjPcvTpiXZ67l5hZATDXzJa5+9qPncRsMjAZIC8vr7ExN4sB3dtzzxWnM+Xi/sxaUsLUBRv57p+W8z8vvcP4IT24bngeg3rUPhiFHL9Dh5zNOyvCq/iggl8bvn6w+6OhRRMMemZnUJCbyYiCzvTpkklBThZ9cjPJbZf6sVaaSaPzeXT+On77rw28uKyUzw7twa3n96NndkZzf0SRVmPChAlMnz79iARg+vTp/PSnP23Q/t27d2/UbHoPPvgg119/fU0C8NJLL53wsVq6WCYAxUCviOWeQEkdZa8Fjpi82d1Lwtd1ZvYPgv4BH0sA3P0x4DEIHgNsdNTNKCs1ic8PD8YOeLt4F1Pf2MjMt4qZtnATZ/TqyHXD8zQZ0XHYe6Dqoyv48Ip+bdleNnxYfuTVfFoSBblZnN23M31ysyjIyaQgN4uTO2eQltzw77pjRgp3jjuNL56TzyP/WMvvF2xk5lubmTAsj6+e15eT1EdA5GOuuuoqvvOd73DgwAFSU1PZsGEDJSUljBo1ir179zJ+/Hh27NjBwYMHueeeexg/fvwR+2/YsIFLL72U5cuXU1FRwU033cTKlSvp378/FRUVNeVuueUWFi1aREVFBVdddRU/+MEPePjhhykpKeG8884jJyeHefPm0bt3b4qKisjJyeH++++vmU1w0qRJ3HbbbWzYsIGLL76YUaNG8frrr9OjRw9eeOEF0tMbNt5LbccsLy/n6quvpri4mOrqar773e9yzTXXMGXKFGbNmkVSUhIXXngh9913X6O+61gmAIuAfmaWD2wmqOQ/f3QhMzsVyAb+HbEuG9jn7gfMLAc4B/hJs0QdA2bGkF4dGdKrI9/59AD++FYxUxds4j9nLOWHf1nJZ4f25LrhefQ7qV2sQ4256kNOyc6KWpvst+w+UFMuwYKnMvrkZjGqbw4FucGVfEFuFjlZKU3a5yK3XSrfu2wAk0bn84t5a3h2wSb+sOh9Jp7dm6+c24dOmersKXJY586dGTZsGH/7298YP34806dP55prrsHMSEtLY+bMmbRv355t27YxYsQILr/88jr/vz7yyCNkZGSwdOlSli5dytChQ2u2/ehHP6JTp05UV1dz/vnns3TpUm699Vbuv/9+5s2bR05OzhHHWrx4MU899RQLFizA3Rk+fDjnnnsu2dnZrF69mmnTpvGb3/yGq6++mueff57rr7/+mJ+1rmOuW7eO7t278+KLLwLBlMbbt29n5syZvPvuu5gZO3fuPMbRjy1mCYC7V5nZ14DZQCLwpLuvMLO7gSJ3P/xo3wRguh/5fFx/4FEzO0TwKOO9kU8PtGUdMpK56Zx8bjy7NwvXb2fqgk1MXbCR376+gWH5nbguTiYj2rP/4BFN9eu27WXt1nLWf1hOZdVHV/Pt05Lo0yWLUX1zKcjNpE9Y0ed1zmj276h7x3R+/JnT+fKYAh6as5rHX13H1Dc2cvOofG4eXUCHdPXvkBbmr1Pgg2VNe8yup8PF99Zb5PBtgMMJwOErZHfn29/+NvPnzychIYHNmzezZcsWunbtWutx5s+fz6233grA4MGDGTx4cM225557jscee4yqqipKS0tZuXLlEduP9tprr/GZz3ymZkbCK6+8kldffZXLL7+c/Px8hgwJHkqLnE74WOo65rhx47j99tu58847ufTSSxk9enTNkMSTJk3i05/+NJdeemmDzlGfmA4E5O4vAS8dte57Ry1/v5b9XgdOj2pwLVxtkxFNW9i6JyNydyqrD1FRWU15ZTX7DlRRXlnNjvLKoKLfVl7T475sz0dX84kJRl6nDApyMjn31NyaJvuC3Ew6Zzbt1XxTOLlzJvdfPYT/GNuHB15ezcNz1/D0vzcyeUwBN57dm8xUjc8l8e2KK67gW9/6Fm+++SYVFRU1V+5Tp06lrKyMxYsXk5ycTO/evWudAjhSbf//169fz3333ceiRYvIzs7mxhtvPOZx6huj5fBUwhBMJxx5q+FEjnnKKaewePFiXnrpJe666y4uvPBCvve977Fw4ULmzJnD9OnT+cUvfsHcuXNr3b+h9JemDcjJSuUr5/Zh8ugC/rV2G1Pf2MTjr67n0X+uq5mM6Pz+XUhuwsmIqg85+yqr2FdZzb7KasoPBO/LK6vYd+DwaxX7DlZHLIevYfmKgxH7ha9Vh+r+T9YxI5k+uVmMPSW3poLvk5tFXqcMUpJa30RLfbu045fXDeWWzbt44OX3+OnsVTz1r/XcMrYv1w3PO67+BiJRcYwr9WjJyspi7NixfPGLX2TChAk163ft2kWXLl1ITk5m3rx5bNy4sd7jHJ6S97zzzmP58uUsXboUCKYSzszMpEOHDmzZsoW//vWvjB07FoB27dqxZ8+ej90CGDNmDDfeeCNTpkzB3Zk5cybPPPNMoz5nXccsKSmhU6dOXH/99WRlZfHb3/6WvXv3sm/fPi655BJGjBhB3759G3VuUALQpiQkGKP75TK6Xy5bdgeTEU1beORkRBf0P4mDhw5FVMZVlB+orr0yj6jUK46q3CM7zR0zLoPMlCQyUhNrXjNSkuicmUKvThlkpgTLGSmJZKaGrxHl26cnkZ+T1WbvlQ/q0YEnbvwEizfu4P6XV/HDv6zkN/PX8fXz+/K5s3q1yuRGpLEmTJjAlVdeyfTp02vWXXfddVx22WUUFhYyZMgQTjvttHqPccstt3DTTTcxePBghgwZwrBhwwA444wzOPPMMxk4cODHphKePHkyF198Md26dWPevHk164cOHcqNN95Yc4xJkyZx5plnNri5H+Cee+7hwQcfrFkuLi6u9ZizZ8/mjjvuICEhgeTkZB555BH27NnD+PHj2b9/P+7OAw880ODz1kWTAbVxR09GdKxfd1pywpGVdUSlXF8lXbO9lv1SkxJaXDN8S/b62m3cN3sVb27aSa9O6dx2/ilccWYPTRwlzUKTAbVemgxIjnD0ZETLNu8iPeXISjozJZGM1CTSkxNVybQAZ/fJ4flbOvOPVWXc9/dV/L//e5tf/WMN37zgFC4Z1I0E/Y5EpAkoAYgjvTpl0KuTBqFpDcyM807rwthTc5m94gN+9vf3+Nqzb9G/21puv/AUPnlaF7WqiEij6OaiSAtmZowb1I2/3TaGB68Zwr7KKm5+uogrH3mdf63R7JEicuKUAIi0AokJxhVn9uCVb53LvVeezpZd+7nu8QVM+M0bFG3YHuvwpI1RYtn6nMjvTAmASCuSnJjAtcPymHv7WL5/2QDWbC3nql//mxufWsiy4l2xDk/agLS0ND788EMlAa2Iu/Phhx+SlnZ8w4vrKQCRVmxfZRW/+/dGfv3Ptezcd5BxA7vyrQtP4RQNCy0n6ODBgxQXFx9zYBxpWdLS0ujZsyfJyUeOKFrfUwBKAETagN37D/Lka+t5/NX1lFdWMf6M7tz2qVPondN6RoIUkaanBCCkBEDauh3llcEUxK+v52C187mzevL18/vRo2PDZiYTkbZFCUBICYDEi6179vOreWt5dsEmAD4/PI//OK8PXdppCmKReKIEIKQEQOJNyc4Kfj53Nc8VFZOcaMEUxGP6kN1Gh1UWkSMpAQgpAZB4tWFbOQ/NWc2flmwmMyUpnII4n/ZpmoJYpC1TAhBSAiDx7r0te3jg5ff46/IP6JCezJfPDaYgzkjRoKAibZESgJASAJHA8s27+NnfVzFvVRk5Wal89bw+TBimKYhF2pr6EoCYDgRkZuPMbJWZrTGzKbVsv9HMysxsSfgzKWLbRDNbHf5MbN7IRVq3QT068NRNw3j+lpH065LFD/68kvPu+wfPvLGRA1XVsQ5PRJpBzFoAzCwReA+4ACgGFgET3H1lRJkbgUJ3/9pR+3YCioBCwIHFwFnuvqO+c6oFQKR2r6/Zxs9efo/FG3fQrUMat4ztw9WFvdQiINLKtdQWgGHAGndf5+6VwHRgfAP3vQh42d23h5X+y8C4KMUp0uad3TeHGV8Zye9vHk7P7HS+98IKzv3pPH77r/XsP6gWAZG2KJYJQA/g/Yjl4nDd0T5rZkvNbIaZ9TrOfUWkgcyMUf1yeO7LI3l20nBO7pzJ9/+8kjE/mceTrykREGlrYpkA1DaZ+dH3I/4M9Hb3wcArwNPHsW9Q0GyymRWZWVFZWdkJBysSL8yMs/sGicC0L40gPyeTu/+yktE/mcfjr66jolKJgEhbEMsEoBjoFbHcEyiJLODuH7r7gXDxN8BZDd034hiPuXuhuxfm5uY2SeAi8WJkn8784csjmT55BH1zs7jnxXeUCIi0EbFMABYB/cws38xSgGuBWZEFzKxbxOLlwDvh+9nAhWaWbWbZwIXhOhGJghEFnZk2eQTPfXkkp3Y9nAjM5bH5a9lXWRXr8ETkBMRs9A93rzKzrxFU3InAk+6+wszuBorcfRZwq5ldDlQB24Ebw323m9kPCZIIgLvdfXuzfwiRODMsvxNTJ42gaMN2Hpqzmh+/9C6P/nMdXxpTwA0jTiYzVQMKibQWGghIRE7Y4o07eGjOaua/V0anzBQmjc7nCyN7k6VEQKRF0EiAISUAItHx5qYdPPTKav75XhnZGclMGl3AF0aeTDvNNSASU0oAQkoARKJryfs7eeiV95i3qoyOGclMGpXPxLN7KxEQiRElACElACLN4+33d/LwnNXMeXcrHdKTuXlUPjee01uzD4o0MyUAISUAIs1rWfEuHpqzmlfe2UL7tCS+OCqfm87Jp0O6EgGR5qAEIKQEQCQ2lm/excNzVvP3lVtol5bETefkc/M5+XTIUCIgEk1KAEJKAERia0VJkAjMXrGFdqlJ3HROb744Kp+OGSmxDk2kTVICEFICINIyvFO6m4fnrOavyz8gKzWJG8/uzc2j8snOVCIg0pSUAISUAIi0LO9+sJufz1nDS8tLyUhOZOLZvZk0uoBOSgREmoQSgJASAJGW6b0te3h4zmpeXFZKenIiXxjZmy+NzqdzVmqsQxNp1ZQAhJQAiLRsq7fs4edz1/DnpSWkJydyw4iT+dKYAnKUCIickEYlAGaW6O5tYtovJQAircOarWEi8HYJqUmJ3DDyZL40uoDcdkoERI5HYxOA9cAM4Cl3XxmF+JqNEgCR1mVt2V5+MXcNLyzZTEpSAtcPP5nJ5xbQpV1arEMTaRUamwC0I5iq9yaC6YOfBKa7++6mDjTalACItE7ryvbyi3lr+NNbm0lOTOC64SfzlXML6NJeiYBIfZqsD4CZjQGmAR0JWgV+6O5rmiTKZqAEQKR127CtnF/MW8PMtzaTlGBc+4lenN03hz65meR1yiQlKSHWIYq0KI3uAwB8mqAFoDfwDDAVGA382N1PadJoo0gJgEjbsPHDcn45bw1/fHMzVYeCv2GJCUav7HQKcrMoyMkMXnMzKcjNJDcrFTOLcdQiza+xCcA6YB7whLu/ftS2h9391iaLNMqUAIi0LXv2H2RtWTnrt+1lXVk568rKWVu2l/XbyjlQdaimXLvUJPJzM49MDHKyyM/JJD0lMYafQCS6GpsAZLn73igFNg54CEgEHnf3e4/a/i1gElAFlAFfdPeN4bZqYFlYdJO7X36s8ykBEIkPhw45JbsqwqRgL+u2lde8L9m1/4iyPTqmU5CbSX7OkQlC9w7pJCSo1UBat/oSgKQG7N/FzKYBI4FDwL+Bb7r7ukYGlQj8ErgAKAYWmdmso540eAsodPd9ZnYL8BPgmnBbhbsPaUwMItI2JSQYPbMz6JmdwZhTco/YVlFZzfpt5ayraTUIEoQ/vrmZvQeqasqlJSfQu3MmfSJuJeTnBO/bwrTGB6qq2bnvIDv3HWTHvsrwfSU7Kw5+9D7ctnt/FV3bpzKge3sGdu/AgG7tyeuUoQSplWtIAvAsQUX9mXD5WoKOgMMbee5hwJrDiYSZTQfGAzUJgLvPiyj/BnB9I88pInEuPSWRAd3bM6B7+yPWuztlew6EtxQ+SgxWlOzibys+oPrQR62lOVmpFORm0ie8lRAkCFn0yk4nKbF5OyJWVh1iZ0Ulu/YdZEdExb2zojJcPsiuikp2lB8MK/dge8XBuod3SU40OmakkJ2RTMf0FLp3SGPzzgrmr95W8z1kpSbRv1s7BnQLk4Lu7el3UhapSbql0lo0JAEwd38mYvn3Zva1Jjh3D+D9iOVi6k8qbgb+GrGcZmZFBLcH7nX3P9W2k5lNBiYD5OXlNSpgEWm7zIwu7dPo0j6NkX06H7GtsuoQm7aXs7as/IhWg9krtrC9/KM/Y8mJRl6nDPJzsoLkIEwMCnIy6ZSZUm9HxIPVh9gVUUEfvvreVRF5hR5U7DXv91VSXll3RZ6UYHTMSKZjRgod05Pp0TGdgd3b0zE9mezMFDqkJ5OdkRKW+ahcRkpirbHuP1jN6i17WVm6ixUlu1lZspsZi4t5+t8ba87Xt0tWTUIwoFuQaHVIb/0tJtFWVX2I0l37OeTOyZ0zm+WcDUkA5pnZFGA64ARN8C+aWScAd99+gueu7X9CrR0SzOx6oBA4N2J1nruXmFkBMNfMlrn72o8d0P0x4DEI+gCcYKwiEsdSkhLo26Udfbu0+9i2nfsqw8TgcF+D4NbC/PfKqKz+qCNih/RkCnIz6d05M6KyDysVb9vWAAAT40lEQVT5fQfZE3H74WgJRlA5ZyTTMT2Zru3TOLVrOzqmh1fphyvvjKBC75AerMtKTWrSpx/SkhM5vWcHTu/ZoWbdoUPOxu37WFGyi5Ulu1lZupv5q8t4/s3imjI9s4PEY0C3DsFr9/Z065AWV09muDvbyyvZtH0f7++o4P3t+4KfHft4f3sFJTsrqDrkXDTwJB69odZb9k2uoSMB1sXdveCETmw2Evi+u18ULt8VHvB/jir3KeDnwLnuvrWOY/0W+Iu7z6jvnOoEKCLNpfqQU7xjX9BiEJEYbNq+j5SkhPDqO6i4a7sSP9z83iEjmXapSa3ufvvWPftrEoIVJbt5p2Q36z8s53CVk52RXNNKcLjFoCAns9lvoTSlfZVVvL89qNw3RVTuhyv6fUe11uRkpdAzO4NenTLolZ1OXqcMTuvWniG9OjZZTC1yMiAzSwLeA84HNgOLgM+7+4qIMmcSDDg0zt1XR6zPBva5+wEzyyHomDj+WEMVKwEQEYmd8gNVvPtBcOtgRZgcvPvBHirDRzZTkxI4rWs7BkTcQujfrR0ZKQ1prI6+g9WHKN25P6zY933sav7D8sojymekJJLXKSOs5NPplZ1BXqegwu+ZnU5mavQ/V2MfA0wGbgHGhKv+ATzq7gebILBLgAcJHgN80t1/ZGZ3A0XuPsvMXgFOB0rDXTa5++VmdjbwKMFTCQnAg+7+xLHOpwRARKRlOVh9iHVl5UG/gs0ftRjsqgiqGDPIz8msefogeBKhfVRmiHR3tu0NmumLw0r+/e0VNVfzpbv2H9EZNCnB6N4xnV6d0iMq+rCSz04/Zr+P5tDYBOBxIBl4Olx1A1Dt7pOaNMpmoARARKTlc3dKdu1nxeZdrCz9qMVg886KmjJd2qXW9Cc4nkcT9x6oqrliDyr6iiPeH/10RG67VHplp0dU7Bn0DCv8ru3TWvwti8aOA/AJdz8jYnmumb3dNKGJiIgcyczo0TGdHh3TuXBg15r1u/YdZEVp2NmwpsNh7Y8mDujenupD8P6OsHIPm+u3H9VMn5WaRK9OGeTnZDLmlNywiT5oru+ZndGmR4psSAJQbWZ9DvewD3vd1/3ciYiISBR0yEjm7D45nN0np2ZdXY8mlv87qKaSE4NkolenDMb16BBxHz6o5DtmJMe8mT5WGpIA3EHwKOA6gkf3TiaYGEhERCSm6no08fDTFie1TyOxlT1B0VzqTQDMLAGoAPoBpxIkAO+6+4FmiE1EROS4JSQYvXOaZzCd1qzeBMDdD5nZz9x9JLC0mWISERGRKGtI98W/m9lnLV5vkoiIiLRBDekD8C0gE6gys/0EtwHc3dvXv5uIiIi0VMdMANz944Nfi4iISKt2zFsAZjanIetERESk9aizBcDM0oAMICcce/9wH4D2QPdmiE1ERESipL5bAF8GbiOo7BfzUQKwG/hllOMSERGRKKozAXD3h4CHzOzr7v7zZoxJREREoqwhnQB/Hs6+1zuyvLv/LopxiYiISBQdMwEws2eAPsASPpoDwAElACIiIq1UQ8YBKAQG+LHmDRYREZFWoyEjAS4Huh6z1Akws3FmtsrM1pjZlFq2p5rZH8LtC8ysd8S2u8L1q8zsomjEJyIi0lY1pAUgB1hpZguBmkmA3P3yxpzYzBIJnia4ACgGFpnZLHdfGVHsZmCHu/c1s2uB/wWuMbMBwLXAQIKnFF4xs1PcXdMUi4iINEBDEoDvR+ncw4A17r4OwMymA+OByARgfMT5ZwC/COckGA9MD2clXG9ma8Lj/TtKsYqIiLQp9Q0EdJq7v+vu/zSz1MgpgM1sRBOcuwfwfsRyMTC8rjLuXmVmu4DO4fo3jtq3RxPEJCIiEhfq6wPwbMT7o6+sf9UE565tdsGjOxrWVaYh+wYHMJtsZkVmVlRWVnacIYqIiLRN9SUAVsf72pZPRDHQK2K5J1BSVxkzSwI6ANsbuC8A7v6Yuxe6e2Fubm4ThC0iItL61ZcAeB3va1s+EYuAfmaWb2YpBJ36Zh1VZhYwMXx/FTA3fBxxFnBt+JRAPtAPWNgEMYmIiMSF+joB9jSzhwmu9g+/J1xu9P328J7+14DZQCLwpLuvMLO7gSJ3nwU8ATwTdvLbTpAkEJZ7jqDDYBXwVT0BICIi0nBW1/g+Zjax1g0hd386KhFFUWFhoRcVFcU6DBERkWZhZovdvbC2bfVNBtTqKngRERFpmIaMBCgiIiJtjBIAERGROKQEQEREJA4dMwEws5+YWXszSzazOWa2zcyub47gREREJDoa0gJwobvvBi4lGIDnFOCOqEYlIiIiUdWQBCA5fL0EmObu26MYj4iIiDSDhswG+GczexeoAP7DzHKB/dENS0RERKLpmC0A7j4FGAkUuvtBoJxgOl4RERFppRrSCfBzQJW7V5vZd4DfA92jHpmIiIhETUP6AHzX3feY2SjgIuBp4JHohiUiIiLR1JAE4PAkO58GHnH3F4CU6IUkIiIi0daQBGCzmT0KXA28ZGapDdxPREREWqiGVORXE0zZO87ddwKd0DgAIiIirVpDngLYB6wFLjKzrwFd3P3vUY9MREREoqYhTwF8A5gKdAl/fm9mX492YCIiIhI9DbkFcDMw3N2/5+7fA0YAX2rMSc2sk5m9bGarw9fsWsoMMbN/m9kKM1tqZtdEbPutma03syXhz5DGxCMiIhJvGpIAGB89CUD43hp53inAHHfvB8wJl4+2D/iCuw8ExgEPmlnHiO13uPuQ8GdJI+MRERGJKw0ZCvgpYIGZzQyXrwCeaOR5xwNjw/dPA/8A7ows4O7vRbwvMbOtQC6ws5HnFhERiXsN6QR4P3ATsB3YAdzk7g828rwnuXtpePxSgr4FdTKzYQRjD6yNWP2j8NbAA+GjiSIiItJA9bYAmFkCsNTdBwFvHs+BzewVoGstm/7rOI/TDXgGmOjuh8LVdwEfECQFjxG0Htxdx/6TgckAeXl5x3NqERGRNqveBMDdD5nZ22aW5+6bjufA7v6puraZ2RYz6+bupWEFv7WOcu2BF4HvuPsbEccuDd8eMLOngNvrieMxgiSBwsJCP57PICIi0lY1pA9AN2CFmS0kmAkQAHe/vBHnnQVMBO4NX184uoCZpQAzgd+5+/8dte1w8mAEfRKWNyIWERGRuNOQBOAHUTjvvcBzZnYzsAn4HICZFQJfcfdJBCMQjgE6m9mN4X43hj3+p5pZLsHTCEuAr0QhRhERkTbL3GtvFTezvgSd9f511PoxwGZ3X1vrji1YYWGhFxUVxToMERGRZmFmi929sLZt9T0F8CCwp5b1+8JtIiIi0krVlwD0dvelR6909yKgd9QiEhERkairLwFIq2dbelMHIiIiIs2nvgRgkZl9bMz/sOPe4uiFJCIiItFW31MAtwEzzew6PqrwCwkG3/lMtAMTERGR6KkzAXD3LcDZZnYeMChc/aK7z22WyERERCRqjjkOgLvPA+Y1QywiIiLSTBoyHbCIiIi0MUoARERE4pASABERkTikBEBERCQOKQEQERGJQ0oARERE4pASABERkTikBEBERCQOKQEQERGJQzFJAMysk5m9bGarw9fsOspVm9mS8GdWxPp8M1sQ7v8HM0tpvuhFRERav1i1AEwB5rh7P2BOuFybCncfEv5cHrH+f4EHwv13ADdHN1wREZG2JVYJwHjg6fD908AVDd3RzAz4JDDjRPYXERGR2CUAJ7l7KUD42qWOcmlmVmRmb5jZ4Uq+M7DT3avC5WKgR10nMrPJ4TGKysrKmip+ERGRVu2YswGeKDN7Behay6b/Oo7D5Ll7iZkVAHPNbBmwu5ZyXtcB3P0x4DGAwsLCOsuJiIjEk6glAO7+qbq2mdkWM+vm7qVm1g3YWscxSsLXdWb2D+BM4Hmgo5klha0APYGSJv8AIiIibVisbgHMAiaG7ycCLxxdwMyyzSw1fJ8DnAOsdHcH5gFX1be/iIiI1C1WCcC9wAVmthq4IFzGzArN7PGwTH+gyMzeJqjw73X3leG2O4Fvmdkagj4BTzRr9CIiIq2cBRfU8aGwsNCLiopiHYaIiEizMLPF7l5Y2zaNBCgiIhKHlACIiIjEISUAIiIicUgJgIiISBxSAiAiIhKHlACIiIjEISUAIiIicUgJgIiISBxSAiAiIhKHlACIiIjEISUAIiIicUgJgIiISBxSAiAiIhKHlACIiIjEISUAIiIicSgmCYCZdTKzl81sdfiaXUuZ88xsScTPfjO7Itz2WzNbH7FtSPN/ChERkdYrVi0AU4A57t4PmBMuH8Hd57n7EHcfAnwS2Af8PaLIHYe3u/uSZolaRESkjYhVAjAeeDp8/zRwxTHKXwX81d33RTUqERGROBGrBOAkdy8FCF+7HKP8tcC0o9b9yMyWmtkDZpYajSBFRETaqqRoHdjMXgG61rLpv47zON2A04HZEavvAj4AUoDHgDuBu+vYfzIwGSAvL+94Ti0iItJmRS0BcPdP1bXNzLaYWTd3Lw0r+K31HOpqYKa7H4w4dmn49oCZPQXcXk8cjxEkCRQWFvrxfAYREZG2Kla3AGYBE8P3E4EX6ik7gaOa/8OkATMzgv4Dy6MQo4iISJsVqwTgXuACM1sNXBAuY2aFZvb44UJm1hvoBfzzqP2nmtkyYBmQA9zTDDGLiIi0GVG7BVAfd/8QOL+W9UXApIjlDUCPWsp9MprxiYiItHUaCVBERCQOKQEQERGJQ0oARERE4pASABERkTikBEBERCQOKQEQERGJQ0oARERE4pASABERkTikBEBERCQOKQEQERGJQ0oARERE4pASABERkTikBEBERCQOKQEQERGJQ0oARERE4pASABERkTgUkwTAzD5nZivM7JCZFdZTbpyZrTKzNWY2JWJ9vpktMLPVZvYHM0tpnshFRETahli1ACwHrgTm11XAzBKBXwIXAwOACWY2INz8v8AD7t4P2AHcHN1wRURE2paYJADu/o67rzpGsWHAGndf5+6VwHRgvJkZ8ElgRljuaeCK6EUrIiLS9rTkPgA9gPcjlovDdZ2Bne5eddR6ERERaaCkaB3YzF4Butay6b/c/YWGHKKWdV7P+rrimAxMDhf3mtmxWh6ORw6wrQmPJ7XT99w89D03H33XzUPfM5xc14aoJQDu/qlGHqIY6BWx3BMoIfhldjSzpLAV4PD6uuJ4DHiskbHUysyK3L3OTozSNPQ9Nw99z81H33Xz0Pdcv5Z8C2AR0C/s8Z8CXAvMcncH5gFXheUmAg1pURAREZFQrB4D/IyZFQMjgRfNbHa4vruZvQQQXt1/DZgNvAM85+4rwkPcCXzLzNYQ9Al4ork/g4iISGsWtVsA9XH3mcDMWtaXAJdELL8EvFRLuXUETwnEWlRuLcjH6HtuHvqem4++6+ah77keFrSoi4iISDxpyX0AREREJEqUAJyguoYplqZjZr3MbJ6ZvRMOHf2NWMfUlplZopm9ZWZ/iXUsbZWZdTSzGWb2bvjvemSsY2qLzOyb4d+M5WY2zczSYh1TS6QE4AQcY5hiaTpVwP9z9/7ACOCr+p6j6hsEHW4leh4C/ubupwFnoO+7yZlZD+BWoNDdBwGJBE+RyVGUAJyYWocpjnFMbY67l7r7m+H7PQR/LDXqYxSYWU/g08DjsY6lrTKz9sAYwqeW3L3S3XfGNqo2KwlIN7MkIIN6xoqJZ0oATkxdwxRLlJhZb+BMYEFsI2mzHgT+EzgU60DasAKgDHgqvNXyuJllxjqotsbdNwP3AZuAUmCXu/89tlG1TEoATsxxDUcsjWNmWcDzwG3uvjvW8bQ1ZnYpsNXdF8c6ljYuCRgKPOLuZwLlgPoPNTEzyyZokc0HugOZZnZ9bKNqmZQAnJi6himWJmZmyQSV/1R3/2Os42mjzgEuN7MNBLezPmlmv49tSG1SMVDs7odbsWYQJATStD4FrHf3Mnc/CPwRODvGMbVISgBOTK3DFMc4pjYnnPr5CeAdd78/1vG0Ve5+l7v3dPfeBP+W57q7rpiamLt/ALxvZqeGq84HVsYwpLZqEzDCzDLCvyHno86WtYrJSICtnbtXmdnhYYoTgScjhimWpnMOcAOwzMyWhOu+HY4QKdIafR2YGl44rANuinE8bY67LzCzGcCbBE8SvYVGBKyVRgIUERGJQ7oFICIiEoeUAIiIiMQhJQAiIiJxSAmAiIhIHFICICIiEoeUAIhIzJnZWM1CKNK8lACIiIjEISUAItJgZna9mS00syVm9qiZJZrZXjP7mZm9aWZzzCw3LDvEzN4ws6VmNjMcox0z62tmr5jZ2+E+fcLDZ5nZDDN718ymhqO4iUiUKAEQkQYxs/7ANcA57j4EqAauAzKBN919KPBP4L/DXX4H3Onug4FlEeunAr909zMIxmgvDdefCdwGDCCYOe+cqH8okTimoYBFpKHOB84CFoUX5+nAVoIphP8Qlvk98Ecz6wB0dPd/huufBv7PzNoBPdx9JoC77wcIj7fQ3YvD5SVAb+C16H8skfikBEBEGsqAp939riNWmn33qHL1jS9eX7P+gYj31ejvk0hU6RaAiDTUHOAqM+sCYGadzOxkgr8jV4VlPg+85u67gB1mNjpcfwPwT3ffDRSb2RXhMVLNLKNZP4WIAMqwRaSB3H2lmX0H+LuZJQAHga8C5cBAM1sM7CLoJwAwEfh1WMFHznx3A/Comd0dHuNzzfgxRCSk2QBFpFHMbK+7Z8U6DhE5ProFICIiEofUAiAiIhKH1AIgIiISh5QAiIiIxCElACIiInFICYCIiEgcUgIgIiISh5QAiIiIxKH/D8Bw43zMLqUzAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([-1,1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([-1,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.50555557,\n",
       " 0.69166666,\n",
       " 0.725,\n",
       " 0.73055553,\n",
       " 0.8138889,\n",
       " 0.88611114,\n",
       " 0.90555555,\n",
       " 0.9,\n",
       " 0.91944444,\n",
       " 0.8972222]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import InputLayer, Input\n",
    "from keras.layers import Conv2D, Dense, Flatten, Dropout, Activation\n",
    "from keras.layers import BatchNormalization, Reshape, MaxPooling2D, GlobalAveragePooling2D\n",
    "def build_model4():\n",
    "    dropout_dense_layer = 0.1\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), input_shape=(64, 64, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(dropout_dense_layer))\n",
    "\n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(dropout_dense_layer))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    model.compile(loss=keras.losses.binary_crossentropy,\n",
    "              optimizer=optimizers.Adam(lr=0.001),\n",
    "              metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "t = build_model4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_56\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_175 (Conv2D)          (None, 62, 62, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 62, 62, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 62, 62, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_176 (Conv2D)          (None, 60, 60, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 60, 60, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 60, 60, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_177 (Conv2D)          (None, 58, 58, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 58, 58, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 58, 58, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_109 (MaxPoolin (None, 29, 29, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_178 (Conv2D)          (None, 27, 27, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 27, 27, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 27, 27, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_179 (Conv2D)          (None, 25, 25, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 25, 25, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 25, 25, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_180 (Conv2D)          (None, 23, 23, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 23, 23, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 23, 23, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_110 (MaxPoolin (None, 11, 11, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_181 (Conv2D)          (None, 9, 9, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 9, 9, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_55 (Flatten)         (None, 10368)             0         \n",
      "_________________________________________________________________\n",
      "dense_109 (Dense)            (None, 1024)              10617856  \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_110 (Dense)            (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_111 (Dense)            (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 11,067,777\n",
      "Trainable params: 11,066,945\n",
      "Non-trainable params: 832\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "t.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model3():\n",
    "    \n",
    "    model = models.Sequential()\n",
    "\n",
    "    model.add(layers.Conv2D(32, (5,5),padding = 'Same', activation ='relu', input_shape=(64, 64, 3)))\n",
    "    model.add(layers.Conv2D(32, (5,5),padding = 'Same', activation ='relu'))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(layers.Dropout(0.25))\n",
    "\n",
    "\n",
    "    model.add(layers.Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n",
    "    model.add(layers.Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "    model.add(layers.Dropout(0.25))\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0),\n",
    "              metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model2():\n",
    "    \n",
    "    model = models.Sequential()\n",
    "\n",
    "    model.add(layers.Conv2D(32, (5,5),padding = 'Same', activation ='relu', input_shape=(64, 64, 3)))\n",
    "    model.add(layers.Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', activation ='relu'))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(layers.Dropout(0.25))\n",
    "\n",
    "    model.add(layers.Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n",
    "    model.add(layers.Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "    model.add(layers.Dropout(0.25))\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0),\n",
    "              metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "    # Feed to a densily connected layer for prediction\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0),\n",
    "              metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                  rotation_range=10,\n",
    "                                  width_shift_range=0.1,\n",
    "                                  shear_range=0.1,\n",
    "                                  zoom_range=0.1,\n",
    "                                  horizontal_flip=True,\n",
    "                                  fill_mode='nearest')\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "12/11 [================================] - 2s 161ms/step - loss: 0.0776 - acc: 0.9694 - val_loss: 9.2421 - val_acc: 0.9500\n",
      "Epoch 2/20\n",
      "12/11 [================================] - 2s 156ms/step - loss: 0.0808 - acc: 0.9722 - val_loss: 9.1715 - val_acc: 0.9500\n",
      "Epoch 3/20\n",
      "12/11 [================================] - 2s 154ms/step - loss: 0.0772 - acc: 0.9750 - val_loss: 9.1436 - val_acc: 0.9500\n",
      "Epoch 4/20\n",
      "12/11 [================================] - 2s 159ms/step - loss: 0.0770 - acc: 0.9778 - val_loss: 9.0988 - val_acc: 0.9500\n",
      "Epoch 5/20\n",
      "12/11 [================================] - 2s 162ms/step - loss: 0.0764 - acc: 0.9750 - val_loss: 9.0718 - val_acc: 0.9500\n",
      "Epoch 6/20\n",
      "12/11 [================================] - 2s 163ms/step - loss: 0.1075 - acc: 0.9722 - val_loss: 8.9662 - val_acc: 0.9500\n",
      "Epoch 7/20\n",
      "12/11 [================================] - 2s 162ms/step - loss: 0.0776 - acc: 0.9722 - val_loss: 8.8984 - val_acc: 0.9500\n",
      "Epoch 8/20\n",
      "12/11 [================================] - 2s 154ms/step - loss: 0.0761 - acc: 0.9806 - val_loss: 8.8261 - val_acc: 0.9500\n",
      "Epoch 9/20\n",
      "12/11 [================================] - 2s 163ms/step - loss: 0.0776 - acc: 0.9750 - val_loss: 8.7707 - val_acc: 0.9500\n",
      "Epoch 10/20\n",
      "12/11 [================================] - 2s 152ms/step - loss: 0.0753 - acc: 0.9778 - val_loss: 8.6617 - val_acc: 0.9500\n",
      "Epoch 11/20\n",
      "12/11 [================================] - 2s 153ms/step - loss: 0.0827 - acc: 0.9778 - val_loss: 8.5858 - val_acc: 0.9500\n",
      "Epoch 12/20\n",
      "12/11 [================================] - 2s 153ms/step - loss: 0.0794 - acc: 0.9750 - val_loss: 8.5058 - val_acc: 0.9500\n",
      "Epoch 13/20\n",
      "12/11 [================================] - 2s 161ms/step - loss: 0.0774 - acc: 0.9750 - val_loss: 8.5162 - val_acc: 0.9500\n",
      "Epoch 14/20\n",
      "12/11 [================================] - 2s 160ms/step - loss: 0.0751 - acc: 0.9750 - val_loss: 8.4742 - val_acc: 0.9500\n",
      "Epoch 15/20\n",
      "12/11 [================================] - 2s 159ms/step - loss: 0.0792 - acc: 0.9750 - val_loss: 8.3512 - val_acc: 0.9500\n",
      "Epoch 16/20\n",
      "12/11 [================================] - 2s 156ms/step - loss: 0.0790 - acc: 0.9778 - val_loss: 8.3204 - val_acc: 0.9500\n",
      "Epoch 17/20\n",
      "12/11 [================================] - 2s 155ms/step - loss: 0.0732 - acc: 0.9778 - val_loss: 8.2958 - val_acc: 0.9500\n",
      "Epoch 18/20\n",
      "12/11 [================================] - 2s 158ms/step - loss: 0.0744 - acc: 0.9778 - val_loss: 8.9515 - val_acc: 0.9750\n",
      "Epoch 19/20\n",
      "12/11 [================================] - 2s 162ms/step - loss: 0.0771 - acc: 0.9778 - val_loss: 8.1797 - val_acc: 0.9500\n",
      "Epoch 20/20\n",
      "12/11 [================================] - 2s 158ms/step - loss: 0.0730 - acc: 0.9778 - val_loss: 8.1719 - val_acc: 0.9500\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(datagen.flow(X_train, y_train, batch_size = 32), epochs = 50, \n",
    "                              validation_data = (X_val,y_val), steps_per_epoch=len(X_train) / 32,\n",
    "                              callbacks=[learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "?classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95        19\n",
      "           1       0.95      0.95      0.95        21\n",
      "\n",
      "    accuracy                           0.95        40\n",
      "   macro avg       0.95      0.95      0.95        40\n",
      "weighted avg       0.95      0.95      0.95        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K FOLD!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kfold iteration 1/10\n",
      "Total images: 400 ---- Train images: 360 ---- Test images: 40\n",
      "Test index start:0 and finish index: 46\n",
      "Distribution of labels: Counter({0: 20, 1: 20})\n",
      "Epoch 1/30\n",
      "12/11 [================================] - 10s 860ms/step - loss: 1.5807 - acc: 0.5250 - val_loss: 6.2337 - val_acc: 0.5000\n",
      "Epoch 2/30\n",
      "12/11 [================================] - 9s 781ms/step - loss: 0.6816 - acc: 0.6000 - val_loss: 60.3931 - val_acc: 0.5000\n",
      "Epoch 3/30\n",
      "12/11 [================================] - 10s 823ms/step - loss: 0.6665 - acc: 0.6167 - val_loss: 3.2481 - val_acc: 0.8250\n",
      "Epoch 4/30\n",
      "12/11 [================================] - 9s 786ms/step - loss: 0.6897 - acc: 0.7556 - val_loss: 22.2777 - val_acc: 0.6500\n",
      "Epoch 5/30\n",
      "12/11 [================================] - 9s 784ms/step - loss: 0.4665 - acc: 0.7667 - val_loss: 355.9328 - val_acc: 0.5000\n",
      "Epoch 6/30\n",
      "12/11 [================================] - 10s 794ms/step - loss: 0.7325 - acc: 0.6528 - val_loss: 31.6437 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 7/30\n",
      "12/11 [================================] - 9s 788ms/step - loss: 0.3923 - acc: 0.8528 - val_loss: 0.3427 - val_acc: 0.9750\n",
      "Epoch 8/30\n",
      "12/11 [================================] - 10s 794ms/step - loss: 0.2672 - acc: 0.8917 - val_loss: 31.3877 - val_acc: 0.7750\n",
      "Epoch 9/30\n",
      "12/11 [================================] - 9s 792ms/step - loss: 0.3368 - acc: 0.8639 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 10/30\n",
      "12/11 [================================] - 9s 781ms/step - loss: 0.2928 - acc: 0.8750 - val_loss: 20.2166 - val_acc: 0.9250\n",
      "Epoch 11/30\n",
      "12/11 [================================] - 10s 801ms/step - loss: 0.2495 - acc: 0.9056 - val_loss: 2.0830e-09 - val_acc: 1.0000\n",
      "Epoch 12/30\n",
      "12/11 [================================] - 10s 809ms/step - loss: 0.2626 - acc: 0.9000 - val_loss: 3.1597 - val_acc: 0.9750\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 13/30\n",
      "12/11 [================================] - 9s 786ms/step - loss: 0.1666 - acc: 0.9306 - val_loss: 7.3785e-07 - val_acc: 1.0000\n",
      "Epoch 14/30\n",
      "12/11 [================================] - 10s 798ms/step - loss: 0.2143 - acc: 0.9083 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 15/30\n",
      "12/11 [================================] - 10s 804ms/step - loss: 0.1797 - acc: 0.9250 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 16/30\n",
      "12/11 [================================] - 10s 796ms/step - loss: 0.1675 - acc: 0.9361 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 17/30\n",
      "12/11 [================================] - 9s 787ms/step - loss: 0.1711 - acc: 0.9278 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 18/30\n",
      "12/11 [================================] - 9s 789ms/step - loss: 0.1746 - acc: 0.9361 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 19/30\n",
      "12/11 [================================] - 9s 789ms/step - loss: 0.1670 - acc: 0.9278 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 20/30\n",
      "12/11 [================================] - 9s 790ms/step - loss: 0.1853 - acc: 0.9389 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 21/30\n",
      "12/11 [================================] - 9s 790ms/step - loss: 0.1701 - acc: 0.9278 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 22/30\n",
      "12/11 [================================] - 10s 794ms/step - loss: 0.1877 - acc: 0.9306 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 23/30\n",
      "12/11 [================================] - 10s 833ms/step - loss: 0.1655 - acc: 0.9389 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 24/30\n",
      "12/11 [================================] - 10s 851ms/step - loss: 0.1827 - acc: 0.9389 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 25/30\n",
      "12/11 [================================] - 9s 791ms/step - loss: 0.1355 - acc: 0.9306 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 26/30\n",
      "12/11 [================================] - 9s 784ms/step - loss: 0.1511 - acc: 0.9417 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 27/30\n",
      "12/11 [================================] - 10s 795ms/step - loss: 0.1443 - acc: 0.9417 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "Epoch 28/30\n",
      "12/11 [================================] - 10s 796ms/step - loss: 0.1508 - acc: 0.9444 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 29/30\n",
      "12/11 [================================] - 10s 817ms/step - loss: 0.1612 - acc: 0.9417 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 30/30\n",
      "12/11 [================================] - 9s 786ms/step - loss: 0.1624 - acc: 0.9278 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        20\n",
      "           1       1.00      1.00      1.00        20\n",
      "\n",
      "    accuracy                           1.00        40\n",
      "   macro avg       1.00      1.00      1.00        40\n",
      "weighted avg       1.00      1.00      1.00        40\n",
      "\n",
      "Kfold iteration 2/10\n",
      "Total images: 400 ---- Train images: 360 ---- Test images: 40\n",
      "Test index start:37 and finish index: 85\n",
      "Distribution of labels: Counter({0: 20, 1: 20})\n",
      "Epoch 1/30\n",
      "12/11 [================================] - 10s 871ms/step - loss: 1.0136 - acc: 0.5222 - val_loss: 2.2052 - val_acc: 0.5000\n",
      "Epoch 2/30\n",
      "12/11 [================================] - 10s 793ms/step - loss: 0.6818 - acc: 0.5722 - val_loss: 1.9589 - val_acc: 0.9000\n",
      "Epoch 3/30\n",
      "12/11 [================================] - 9s 790ms/step - loss: 0.8062 - acc: 0.6472 - val_loss: 29.5083 - val_acc: 0.6000\n",
      "Epoch 4/30\n",
      "12/11 [================================] - 10s 796ms/step - loss: 0.7028 - acc: 0.6250 - val_loss: 0.2533 - val_acc: 0.9250\n",
      "Epoch 5/30\n",
      "12/11 [================================] - 9s 791ms/step - loss: 0.5459 - acc: 0.7556 - val_loss: 11.2340 - val_acc: 0.9250\n",
      "Epoch 6/30\n",
      "12/11 [================================] - 10s 815ms/step - loss: 0.3852 - acc: 0.8417 - val_loss: 96.5886 - val_acc: 0.6500\n",
      "Epoch 7/30\n",
      "12/11 [================================] - 9s 791ms/step - loss: 0.3251 - acc: 0.8611 - val_loss: 26.0528 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 8/30\n",
      "12/11 [================================] - 10s 794ms/step - loss: 0.2148 - acc: 0.9194 - val_loss: 11.4778 - val_acc: 0.9250\n",
      "Epoch 9/30\n",
      "12/11 [================================] - 10s 809ms/step - loss: 0.2308 - acc: 0.9083 - val_loss: 9.4882 - val_acc: 0.9500\n",
      "Epoch 10/30\n",
      "12/11 [================================] - 10s 804ms/step - loss: 0.2125 - acc: 0.9167 - val_loss: 4.1108 - val_acc: 0.9500\n",
      "Epoch 11/30\n",
      "12/11 [================================] - 10s 797ms/step - loss: 0.1749 - acc: 0.9194 - val_loss: 16.0871 - val_acc: 0.9500\n",
      "Epoch 12/30\n",
      "12/11 [================================] - 10s 793ms/step - loss: 0.2281 - acc: 0.9000 - val_loss: 0.5898 - val_acc: 0.9750\n",
      "Epoch 13/30\n",
      "12/11 [================================] - 10s 797ms/step - loss: 0.2174 - acc: 0.9250 - val_loss: 17.7752 - val_acc: 0.9250\n",
      "Epoch 14/30\n",
      "12/11 [================================] - 10s 833ms/step - loss: 0.1421 - acc: 0.9444 - val_loss: 45.0634 - val_acc: 0.8250\n",
      "Epoch 15/30\n",
      "12/11 [================================] - 10s 854ms/step - loss: 0.2153 - acc: 0.9139 - val_loss: 0.3445 - val_acc: 0.9750\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 16/30\n",
      "12/11 [================================] - 10s 798ms/step - loss: 0.1579 - acc: 0.9333 - val_loss: 1.1135 - val_acc: 0.9750\n",
      "Epoch 17/30\n",
      "12/11 [================================] - 10s 793ms/step - loss: 0.1475 - acc: 0.9444 - val_loss: 2.0041 - val_acc: 0.9750\n",
      "Epoch 18/30\n",
      "12/11 [================================] - 10s 833ms/step - loss: 0.1456 - acc: 0.9444 - val_loss: 0.3461 - val_acc: 0.9750\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 19/30\n",
      "12/11 [================================] - 10s 797ms/step - loss: 0.1493 - acc: 0.9444 - val_loss: 2.6243e-25 - val_acc: 1.0000\n",
      "Epoch 20/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/11 [================================] - 10s 797ms/step - loss: 0.1567 - acc: 0.9528 - val_loss: 0.5418 - val_acc: 0.9750\n",
      "Epoch 21/30\n",
      "12/11 [================================] - 10s 825ms/step - loss: 0.1308 - acc: 0.9556 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 22/30\n",
      "12/11 [================================] - 10s 794ms/step - loss: 0.1536 - acc: 0.9417 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 23/30\n",
      "12/11 [================================] - 10s 799ms/step - loss: 0.1165 - acc: 0.9556 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 24/30\n",
      "12/11 [================================] - 10s 801ms/step - loss: 0.1039 - acc: 0.9583 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 25/30\n",
      "12/11 [================================] - 10s 793ms/step - loss: 0.0843 - acc: 0.9639 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 26/30\n",
      "12/11 [================================] - 10s 793ms/step - loss: 0.1243 - acc: 0.9611 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 27/30\n",
      "12/11 [================================] - 9s 791ms/step - loss: 0.1474 - acc: 0.9639 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 28/30\n",
      "12/11 [================================] - 9s 791ms/step - loss: 0.1066 - acc: 0.9583 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 29/30\n",
      "12/11 [================================] - 10s 796ms/step - loss: 0.1377 - acc: 0.9389 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 30/30\n",
      "12/11 [================================] - 10s 795ms/step - loss: 0.1432 - acc: 0.9528 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        20\n",
      "           1       1.00      1.00      1.00        20\n",
      "\n",
      "    accuracy                           1.00        40\n",
      "   macro avg       1.00      1.00      1.00        40\n",
      "weighted avg       1.00      1.00      1.00        40\n",
      "\n",
      "Kfold iteration 3/10\n",
      "Total images: 400 ---- Train images: 360 ---- Test images: 40\n",
      "Test index start:73 and finish index: 119\n",
      "Distribution of labels: Counter({0: 20, 1: 20})\n",
      "Epoch 1/30\n",
      "12/11 [================================] - 14s 1s/step - loss: 1.0622 - acc: 0.5250 - val_loss: 13.0092 - val_acc: 0.5000\n",
      "Epoch 2/30\n",
      "12/11 [================================] - 11s 907ms/step - loss: 0.6838 - acc: 0.5000 - val_loss: 48.1438 - val_acc: 0.5000\n",
      "Epoch 3/30\n",
      "12/11 [================================] - 10s 873ms/step - loss: 0.6885 - acc: 0.6556 - val_loss: 87.3909 - val_acc: 0.5000\n",
      "Epoch 4/30\n",
      "12/11 [================================] - 10s 864ms/step - loss: 0.6842 - acc: 0.6167 - val_loss: 25.6554 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 5/30\n",
      "12/11 [================================] - 12s 984ms/step - loss: 0.6034 - acc: 0.7139 - val_loss: 16.6823 - val_acc: 0.9000\n",
      "Epoch 6/30\n",
      "12/11 [================================] - 10s 872ms/step - loss: 0.3051 - acc: 0.8889 - val_loss: 28.5704 - val_acc: 0.9000\n",
      "Epoch 7/30\n",
      "12/11 [================================] - 10s 816ms/step - loss: 0.3361 - acc: 0.8750 - val_loss: 53.1248 - val_acc: 0.8750\n",
      "Epoch 8/30\n",
      "12/11 [================================] - 10s 865ms/step - loss: 0.2708 - acc: 0.8889 - val_loss: 48.2177 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 9/30\n",
      "12/11 [================================] - 10s 856ms/step - loss: 0.1750 - acc: 0.9417 - val_loss: 50.3494 - val_acc: 0.9000\n",
      "Epoch 10/30\n",
      "12/11 [================================] - 10s 857ms/step - loss: 0.1489 - acc: 0.9444 - val_loss: 60.0370 - val_acc: 0.9000\n",
      "Epoch 11/30\n",
      "12/11 [================================] - 11s 886ms/step - loss: 0.1475 - acc: 0.9472 - val_loss: 63.9515 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 12/30\n",
      "12/11 [================================] - 10s 859ms/step - loss: 0.1255 - acc: 0.9500 - val_loss: 62.6008 - val_acc: 0.9000\n",
      "Epoch 13/30\n",
      "12/11 [================================] - 10s 810ms/step - loss: 0.1394 - acc: 0.9389 - val_loss: 60.1309 - val_acc: 0.9000\n",
      "Epoch 14/30\n",
      "12/11 [================================] - 11s 945ms/step - loss: 0.1469 - acc: 0.9444 - val_loss: 63.6286 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 15/30\n",
      "12/11 [================================] - 11s 924ms/step - loss: 0.1420 - acc: 0.9472 - val_loss: 54.5002 - val_acc: 0.9000\n",
      "Epoch 16/30\n",
      "12/11 [================================] - 12s 1s/step - loss: 0.1168 - acc: 0.9694 - val_loss: 63.2381 - val_acc: 0.9000\n",
      "Epoch 17/30\n",
      "12/11 [================================] - 11s 933ms/step - loss: 0.1221 - acc: 0.9528 - val_loss: 50.9737 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 18/30\n",
      "12/11 [================================] - 10s 836ms/step - loss: 0.1224 - acc: 0.9528 - val_loss: 53.7544 - val_acc: 0.9000\n",
      "Epoch 19/30\n",
      "12/11 [================================] - 9s 763ms/step - loss: 0.1064 - acc: 0.9583 - val_loss: 57.5620 - val_acc: 0.9000\n",
      "Epoch 20/30\n",
      "12/11 [================================] - 9s 769ms/step - loss: 0.1109 - acc: 0.9667 - val_loss: 55.8290 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 21/30\n",
      "12/11 [================================] - 9s 765ms/step - loss: 0.1815 - acc: 0.9528 - val_loss: 54.3978 - val_acc: 0.9000\n",
      "Epoch 22/30\n",
      "12/11 [================================] - 9s 786ms/step - loss: 0.1228 - acc: 0.9556 - val_loss: 52.8765 - val_acc: 0.9000\n",
      "Epoch 23/30\n",
      "12/11 [================================] - 9s 773ms/step - loss: 0.1035 - acc: 0.9667 - val_loss: 50.9022 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "Epoch 24/30\n",
      "12/11 [================================] - 9s 765ms/step - loss: 0.1373 - acc: 0.9500 - val_loss: 52.0167 - val_acc: 0.9000\n",
      "Epoch 25/30\n",
      "12/11 [================================] - 9s 765ms/step - loss: 0.1296 - acc: 0.9528 - val_loss: 51.9420 - val_acc: 0.9000\n",
      "Epoch 26/30\n",
      "12/11 [================================] - 9s 762ms/step - loss: 0.1172 - acc: 0.9556 - val_loss: 52.1183 - val_acc: 0.9000\n",
      "Epoch 27/30\n",
      "12/11 [================================] - 9s 792ms/step - loss: 0.1097 - acc: 0.9667 - val_loss: 52.6287 - val_acc: 0.9000\n",
      "Epoch 28/30\n",
      "12/11 [================================] - 9s 769ms/step - loss: 0.1079 - acc: 0.9583 - val_loss: 52.4488 - val_acc: 0.9000\n",
      "Epoch 29/30\n",
      "12/11 [================================] - 9s 785ms/step - loss: 0.1328 - acc: 0.9556 - val_loss: 51.6274 - val_acc: 0.9000\n",
      "Epoch 30/30\n",
      "12/11 [================================] - 11s 945ms/step - loss: 0.1048 - acc: 0.9500 - val_loss: 49.7137 - val_acc: 0.9000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.85      0.89        20\n",
      "           1       0.86      0.95      0.90        20\n",
      "\n",
      "    accuracy                           0.90        40\n",
      "   macro avg       0.90      0.90      0.90        40\n",
      "weighted avg       0.90      0.90      0.90        40\n",
      "\n",
      "Kfold iteration 4/10\n",
      "Total images: 400 ---- Train images: 360 ---- Test images: 40\n",
      "Test index start:120 and finish index: 160\n",
      "Distribution of labels: Counter({0: 20, 1: 20})\n",
      "Epoch 1/30\n",
      "12/11 [================================] - 11s 881ms/step - loss: 1.0150 - acc: 0.4750 - val_loss: 4.8710 - val_acc: 0.5000\n",
      "Epoch 2/30\n",
      "12/11 [================================] - 10s 796ms/step - loss: 0.6910 - acc: 0.5500 - val_loss: 4.3343 - val_acc: 0.7000\n",
      "Epoch 3/30\n",
      "12/11 [================================] - 10s 799ms/step - loss: 0.7209 - acc: 0.6389 - val_loss: 9.7680 - val_acc: 0.6000\n",
      "Epoch 4/30\n",
      "12/11 [================================] - 10s 815ms/step - loss: 0.5810 - acc: 0.7472 - val_loss: 20.3604 - val_acc: 0.8000\n",
      "Epoch 5/30\n",
      "12/11 [================================] - 10s 794ms/step - loss: 0.4422 - acc: 0.7861 - val_loss: 94.7888 - val_acc: 0.8750\n",
      "Epoch 6/30\n",
      "12/11 [================================] - 10s 803ms/step - loss: 0.6293 - acc: 0.7306 - val_loss: 28.1505 - val_acc: 0.8500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30\n",
      "12/11 [================================] - 10s 796ms/step - loss: 0.3195 - acc: 0.8611 - val_loss: 43.2913 - val_acc: 0.8250\n",
      "Epoch 8/30\n",
      "12/11 [================================] - 10s 863ms/step - loss: 0.3894 - acc: 0.8389 - val_loss: 36.0827 - val_acc: 0.8500\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 9/30\n",
      "12/11 [================================] - 9s 784ms/step - loss: 0.2107 - acc: 0.9167 - val_loss: 40.1518 - val_acc: 0.8500\n",
      "Epoch 10/30\n",
      "12/11 [================================] - 12s 1s/step - loss: 0.1766 - acc: 0.9306 - val_loss: 63.6850 - val_acc: 0.9000\n",
      "Epoch 11/30\n",
      "12/11 [================================] - 12s 1s/step - loss: 0.2729 - acc: 0.9056 - val_loss: 35.9849 - val_acc: 0.8750\n",
      "Epoch 12/30\n",
      "12/11 [================================] - 13s 1s/step - loss: 0.1740 - acc: 0.9194 - val_loss: 77.8870 - val_acc: 0.9000\n",
      "Epoch 13/30\n",
      "12/11 [================================] - 13s 1s/step - loss: 0.1583 - acc: 0.9361 - val_loss: 35.9498 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 14/30\n",
      "12/11 [================================] - 13s 1s/step - loss: 0.1598 - acc: 0.9250 - val_loss: 28.8302 - val_acc: 0.8750\n",
      "Epoch 15/30\n",
      "12/11 [================================] - 12s 997ms/step - loss: 0.1569 - acc: 0.9361 - val_loss: 33.9775 - val_acc: 0.8750\n",
      "Epoch 16/30\n",
      "12/11 [================================] - 13s 1s/step - loss: 0.1316 - acc: 0.9444 - val_loss: 37.2568 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 17/30\n",
      "12/11 [================================] - 12s 994ms/step - loss: 0.1079 - acc: 0.9556 - val_loss: 34.8512 - val_acc: 0.9000\n",
      "Epoch 18/30\n",
      "12/11 [================================] - 12s 1s/step - loss: 0.1600 - acc: 0.9333 - val_loss: 30.5061 - val_acc: 0.9000\n",
      "Epoch 19/30\n",
      "12/11 [================================] - 12s 977ms/step - loss: 0.1278 - acc: 0.9556 - val_loss: 33.5821 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 20/30\n",
      "12/11 [================================] - 12s 1s/step - loss: 0.1010 - acc: 0.9528 - val_loss: 35.9439 - val_acc: 0.8750\n",
      "Epoch 21/30\n",
      "12/11 [================================] - 12s 995ms/step - loss: 0.1187 - acc: 0.9444 - val_loss: 30.2279 - val_acc: 0.8750\n",
      "Epoch 22/30\n",
      "12/11 [================================] - 12s 959ms/step - loss: 0.1157 - acc: 0.9500 - val_loss: 27.5099 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 23/30\n",
      "12/11 [================================] - 11s 925ms/step - loss: 0.1243 - acc: 0.9556 - val_loss: 29.1585 - val_acc: 0.8750\n",
      "Epoch 24/30\n",
      "12/11 [================================] - 11s 958ms/step - loss: 0.1309 - acc: 0.9639 - val_loss: 28.9747 - val_acc: 0.9000\n",
      "Epoch 25/30\n",
      "12/11 [================================] - 12s 1s/step - loss: 0.1103 - acc: 0.9500 - val_loss: 26.7362 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 26/30\n",
      "12/11 [================================] - 12s 965ms/step - loss: 0.0981 - acc: 0.9528 - val_loss: 26.3632 - val_acc: 0.8750\n",
      "Epoch 27/30\n",
      "12/11 [================================] - 12s 976ms/step - loss: 0.1114 - acc: 0.9556 - val_loss: 27.0054 - val_acc: 0.8750\n",
      "Epoch 28/30\n",
      "12/11 [================================] - 12s 972ms/step - loss: 0.1058 - acc: 0.9556 - val_loss: 27.3302 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "Epoch 29/30\n",
      "12/11 [================================] - 11s 953ms/step - loss: 0.1145 - acc: 0.9639 - val_loss: 27.3501 - val_acc: 0.8750\n",
      "Epoch 30/30\n",
      "12/11 [================================] - 10s 861ms/step - loss: 0.1073 - acc: 0.9556 - val_loss: 27.1858 - val_acc: 0.8750\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.85      0.87        20\n",
      "           1       0.86      0.90      0.88        20\n",
      "\n",
      "    accuracy                           0.88        40\n",
      "   macro avg       0.88      0.88      0.87        40\n",
      "weighted avg       0.88      0.88      0.87        40\n",
      "\n",
      "Kfold iteration 5/10\n",
      "Total images: 400 ---- Train images: 360 ---- Test images: 40\n",
      "Test index start:159 and finish index: 201\n",
      "Distribution of labels: Counter({0: 20, 1: 20})\n",
      "Epoch 1/30\n",
      "12/11 [================================] - 12s 993ms/step - loss: 0.7658 - acc: 0.4667 - val_loss: 5.7088 - val_acc: 0.5000\n",
      "Epoch 2/30\n",
      "12/11 [================================] - 10s 875ms/step - loss: 0.7014 - acc: 0.4917 - val_loss: 0.8262 - val_acc: 0.7750\n",
      "Epoch 3/30\n",
      "12/11 [================================] - 12s 1s/step - loss: 0.6663 - acc: 0.6111 - val_loss: 18.6814 - val_acc: 0.5000\n",
      "Epoch 4/30\n",
      "12/11 [================================] - 12s 981ms/step - loss: 0.7584 - acc: 0.5611 - val_loss: 0.4184 - val_acc: 0.9250\n",
      "Epoch 5/30\n",
      "12/11 [================================] - 12s 960ms/step - loss: 0.6527 - acc: 0.7111 - val_loss: 48.2241 - val_acc: 0.5000\n",
      "Epoch 6/30\n",
      "12/11 [================================] - 11s 891ms/step - loss: 0.6627 - acc: 0.6278 - val_loss: 6.7448 - val_acc: 0.8000\n",
      "Epoch 7/30\n",
      "12/11 [================================] - 11s 948ms/step - loss: 0.5253 - acc: 0.7667 - val_loss: 8.0150 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 8/30\n",
      "12/11 [================================] - 12s 969ms/step - loss: 0.2979 - acc: 0.8889 - val_loss: 28.3523 - val_acc: 0.9250\n",
      "Epoch 9/30\n",
      "12/11 [================================] - 12s 1s/step - loss: 0.3235 - acc: 0.8389 - val_loss: 5.0722 - val_acc: 0.9500\n",
      "Epoch 10/30\n",
      "12/11 [================================] - 11s 922ms/step - loss: 0.2079 - acc: 0.9111 - val_loss: 9.1817 - val_acc: 0.9250\n",
      "Epoch 11/30\n",
      "12/11 [================================] - 11s 951ms/step - loss: 0.1924 - acc: 0.9333 - val_loss: 22.2263 - val_acc: 0.9500\n",
      "Epoch 12/30\n",
      "12/11 [================================] - 13s 1s/step - loss: 0.3210 - acc: 0.8778 - val_loss: 8.3513 - val_acc: 0.9750\n",
      "Epoch 13/30\n",
      "12/11 [================================] - 11s 900ms/step - loss: 0.2116 - acc: 0.9083 - val_loss: 9.2130 - val_acc: 0.9500\n",
      "Epoch 14/30\n",
      "12/11 [================================] - 11s 938ms/step - loss: 0.2163 - acc: 0.9222 - val_loss: 20.1951 - val_acc: 0.8500\n",
      "Epoch 15/30\n",
      "12/11 [================================] - 13s 1s/step - loss: 0.2182 - acc: 0.9250 - val_loss: 9.5715 - val_acc: 0.9250\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 16/30\n",
      "12/11 [================================] - 12s 984ms/step - loss: 0.1938 - acc: 0.9361 - val_loss: 9.4523 - val_acc: 0.9250\n",
      "Epoch 17/30\n",
      "12/11 [================================] - 10s 863ms/step - loss: 0.1901 - acc: 0.9333 - val_loss: 2.1319 - val_acc: 0.9500\n",
      "Epoch 18/30\n",
      "12/11 [================================] - 11s 893ms/step - loss: 0.1969 - acc: 0.9250 - val_loss: 1.4979 - val_acc: 0.9750\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 19/30\n",
      "12/11 [================================] - 11s 933ms/step - loss: 0.1426 - acc: 0.9417 - val_loss: 3.0699 - val_acc: 0.9750\n",
      "Epoch 20/30\n",
      "12/11 [================================] - 12s 999ms/step - loss: 0.1536 - acc: 0.9389 - val_loss: 8.7418 - val_acc: 0.9750\n",
      "Epoch 21/30\n",
      "12/11 [================================] - 13s 1s/step - loss: 0.1566 - acc: 0.9333 - val_loss: 3.8159 - val_acc: 0.9750\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 22/30\n",
      "12/11 [================================] - 11s 956ms/step - loss: 0.1540 - acc: 0.9417 - val_loss: 6.3273 - val_acc: 0.9750\n",
      "Epoch 23/30\n",
      "12/11 [================================] - 12s 976ms/step - loss: 0.1455 - acc: 0.9389 - val_loss: 4.7727 - val_acc: 0.9750\n",
      "Epoch 24/30\n",
      "12/11 [================================] - 12s 966ms/step - loss: 0.1350 - acc: 0.9417 - val_loss: 6.7318 - val_acc: 0.9750\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 25/30\n",
      "12/11 [================================] - 13s 1s/step - loss: 0.1093 - acc: 0.9500 - val_loss: 6.3943 - val_acc: 0.9750\n",
      "Epoch 26/30\n",
      "12/11 [================================] - 12s 979ms/step - loss: 0.1297 - acc: 0.9389 - val_loss: 6.9546 - val_acc: 0.9750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30\n",
      "12/11 [================================] - 12s 1s/step - loss: 0.1367 - acc: 0.9528 - val_loss: 4.6254 - val_acc: 0.9750\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 28/30\n",
      "12/11 [================================] - 12s 1s/step - loss: 0.1593 - acc: 0.9417 - val_loss: 6.1350 - val_acc: 0.9750\n",
      "Epoch 29/30\n",
      "12/11 [================================] - 12s 997ms/step - loss: 0.1449 - acc: 0.9333 - val_loss: 5.6006 - val_acc: 0.9750\n",
      "Epoch 30/30\n",
      "12/11 [================================] - 12s 995ms/step - loss: 0.1212 - acc: 0.9528 - val_loss: 6.5260 - val_acc: 0.9750\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98        20\n",
      "           1       1.00      0.95      0.97        20\n",
      "\n",
      "    accuracy                           0.97        40\n",
      "   macro avg       0.98      0.97      0.97        40\n",
      "weighted avg       0.98      0.97      0.97        40\n",
      "\n",
      "Kfold iteration 6/10\n",
      "Total images: 400 ---- Train images: 360 ---- Test images: 40\n",
      "Test index start:199 and finish index: 244\n",
      "Distribution of labels: Counter({1: 20, 0: 20})\n",
      "Epoch 1/30\n",
      "12/11 [================================] - 14s 1s/step - loss: 0.9140 - acc: 0.5222 - val_loss: 45.0264 - val_acc: 0.5000\n",
      "Epoch 2/30\n",
      "12/11 [================================] - 12s 1s/step - loss: 0.6875 - acc: 0.5861 - val_loss: 17.6922 - val_acc: 0.5000\n",
      "Epoch 3/30\n",
      "12/11 [================================] - 14s 1s/step - loss: 0.7183 - acc: 0.5083 - val_loss: 0.7272 - val_acc: 0.8500\n",
      "Epoch 4/30\n",
      "12/11 [================================] - 12s 994ms/step - loss: 0.8004 - acc: 0.7444 - val_loss: 3.4961 - val_acc: 0.9000\n",
      "Epoch 5/30\n",
      "12/11 [================================] - 13s 1s/step - loss: 0.4812 - acc: 0.7917 - val_loss: 195.3807 - val_acc: 0.5250\n",
      "Epoch 6/30\n",
      "12/11 [================================] - 12s 1s/step - loss: 0.3928 - acc: 0.8639 - val_loss: 4.8792 - val_acc: 0.9750\n",
      "Epoch 7/30\n",
      "12/11 [================================] - 11s 951ms/step - loss: 0.3827 - acc: 0.8306 - val_loss: 4.5226 - val_acc: 0.9750\n",
      "Epoch 8/30\n",
      "12/11 [================================] - 12s 968ms/step - loss: 0.3586 - acc: 0.8556 - val_loss: 0.9345 - val_acc: 0.9750\n",
      "Epoch 9/30\n",
      "12/11 [================================] - 12s 974ms/step - loss: 0.4244 - acc: 0.8056 - val_loss: 19.8482 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 10/30\n",
      "12/11 [================================] - 11s 938ms/step - loss: 0.2083 - acc: 0.9222 - val_loss: 16.9692 - val_acc: 0.9500\n",
      "Epoch 11/30\n",
      "12/11 [================================] - 11s 909ms/step - loss: 0.2171 - acc: 0.9028 - val_loss: 6.7164 - val_acc: 0.9750\n",
      "Epoch 12/30\n",
      "12/11 [================================] - 11s 947ms/step - loss: 0.2560 - acc: 0.9028 - val_loss: 12.1322 - val_acc: 0.9250\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 13/30\n",
      "12/11 [================================] - 12s 1s/step - loss: 0.1770 - acc: 0.9389 - val_loss: 6.4001 - val_acc: 0.9500\n",
      "Epoch 14/30\n",
      "12/11 [================================] - 12s 977ms/step - loss: 0.1496 - acc: 0.9389 - val_loss: 19.4141 - val_acc: 0.9250\n",
      "Epoch 15/30\n",
      "12/11 [================================] - 12s 1s/step - loss: 0.1615 - acc: 0.9389 - val_loss: 8.1856 - val_acc: 0.9750\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 16/30\n",
      "12/11 [================================] - 12s 961ms/step - loss: 0.1664 - acc: 0.9472 - val_loss: 3.2715e-13 - val_acc: 1.0000\n",
      "Epoch 17/30\n",
      "12/11 [================================] - 11s 922ms/step - loss: 0.1813 - acc: 0.9250 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 18/30\n",
      "12/11 [================================] - 11s 922ms/step - loss: 0.1878 - acc: 0.9306 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 19/30\n",
      "12/11 [================================] - 11s 934ms/step - loss: 0.1440 - acc: 0.9444 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 20/30\n",
      "12/11 [================================] - 11s 923ms/step - loss: 0.1190 - acc: 0.9500 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 21/30\n",
      "12/11 [================================] - 11s 944ms/step - loss: 0.1688 - acc: 0.9417 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 22/30\n",
      "12/11 [================================] - 11s 939ms/step - loss: 0.1137 - acc: 0.9556 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 23/30\n",
      "12/11 [================================] - 11s 929ms/step - loss: 0.1054 - acc: 0.9611 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 24/30\n",
      "12/11 [================================] - 12s 969ms/step - loss: 0.1280 - acc: 0.9528 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 25/30\n",
      "12/11 [================================] - 12s 1s/step - loss: 0.1267 - acc: 0.9500 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 26/30\n",
      "12/11 [================================] - 12s 987ms/step - loss: 0.1486 - acc: 0.9528 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 27/30\n",
      "12/11 [================================] - 12s 977ms/step - loss: 0.1419 - acc: 0.9361 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 28/30\n",
      "12/11 [================================] - 12s 974ms/step - loss: 0.1519 - acc: 0.9361 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "Epoch 29/30\n",
      "12/11 [================================] - 11s 952ms/step - loss: 0.0917 - acc: 0.9667 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 30/30\n",
      "12/11 [================================] - 12s 1s/step - loss: 0.1234 - acc: 0.9500 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        20\n",
      "           1       1.00      1.00      1.00        20\n",
      "\n",
      "    accuracy                           1.00        40\n",
      "   macro avg       1.00      1.00      1.00        40\n",
      "weighted avg       1.00      1.00      1.00        40\n",
      "\n",
      "Kfold iteration 7/10\n",
      "Total images: 400 ---- Train images: 360 ---- Test images: 40\n",
      "Test index start:235 and finish index: 280\n",
      "Distribution of labels: Counter({0: 20, 1: 20})\n",
      "Epoch 1/30\n",
      "12/11 [================================] - 13s 1s/step - loss: 0.9982 - acc: 0.5667 - val_loss: 19.4940 - val_acc: 0.5000\n",
      "Epoch 2/30\n",
      "12/11 [================================] - 13s 1s/step - loss: 0.6900 - acc: 0.6194 - val_loss: 34.0379 - val_acc: 0.5000\n",
      "Epoch 3/30\n",
      "12/11 [================================] - 12s 999ms/step - loss: 0.6336 - acc: 0.6528 - val_loss: 249.0449 - val_acc: 0.5000\n",
      "Epoch 4/30\n",
      "12/11 [================================] - 11s 906ms/step - loss: 0.7491 - acc: 0.6972 - val_loss: 7.3111 - val_acc: 0.8750\n",
      "Epoch 5/30\n",
      "12/11 [================================] - 11s 902ms/step - loss: 0.6118 - acc: 0.7250 - val_loss: 27.9532 - val_acc: 0.5250\n",
      "Epoch 6/30\n",
      "12/11 [================================] - 10s 808ms/step - loss: 0.3846 - acc: 0.8139 - val_loss: 696.6302 - val_acc: 0.5000\n",
      "Epoch 7/30\n",
      "12/11 [================================] - 11s 886ms/step - loss: 0.6446 - acc: 0.8250 - val_loss: 33.2636 - val_acc: 0.9000\n",
      "Epoch 8/30\n",
      "12/11 [================================] - 11s 899ms/step - loss: 0.3441 - acc: 0.8611 - val_loss: 38.7943 - val_acc: 0.8750\n",
      "Epoch 9/30\n",
      "12/11 [================================] - 11s 914ms/step - loss: 0.3233 - acc: 0.8694 - val_loss: 40.0565 - val_acc: 0.9000\n",
      "Epoch 10/30\n",
      "12/11 [================================] - 10s 825ms/step - loss: 0.2901 - acc: 0.8944 - val_loss: 16.2903 - val_acc: 0.9250\n",
      "Epoch 11/30\n",
      "12/11 [================================] - 10s 830ms/step - loss: 0.3701 - acc: 0.8444 - val_loss: 8.8444 - val_acc: 0.9250\n",
      "Epoch 12/30\n",
      "12/11 [================================] - 10s 824ms/step - loss: 0.1990 - acc: 0.9250 - val_loss: 24.2164 - val_acc: 0.9500\n",
      "Epoch 13/30\n",
      "12/11 [================================] - 11s 955ms/step - loss: 0.4516 - acc: 0.8333 - val_loss: 4.2350 - val_acc: 0.9250\n",
      "Epoch 14/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/11 [================================] - 10s 843ms/step - loss: 0.2001 - acc: 0.9167 - val_loss: 5.7407 - val_acc: 0.9500\n",
      "Epoch 15/30\n",
      "12/11 [================================] - 10s 854ms/step - loss: 0.1760 - acc: 0.9333 - val_loss: 48.3524 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 16/30\n",
      "12/11 [================================] - 11s 903ms/step - loss: 0.1830 - acc: 0.9167 - val_loss: 13.7999 - val_acc: 0.9250\n",
      "Epoch 17/30\n",
      "12/11 [================================] - 10s 810ms/step - loss: 0.1218 - acc: 0.9389 - val_loss: 12.7019 - val_acc: 0.9750\n",
      "Epoch 18/30\n",
      "12/11 [================================] - 10s 855ms/step - loss: 0.1544 - acc: 0.9417 - val_loss: 26.0915 - val_acc: 0.9250\n",
      "Epoch 19/30\n",
      "12/11 [================================] - 13s 1s/step - loss: 0.0819 - acc: 0.9722 - val_loss: 32.1612 - val_acc: 0.9000\n",
      "Epoch 20/30\n",
      "12/11 [================================] - 12s 1s/step - loss: 0.1735 - acc: 0.9389 - val_loss: 5.3006 - val_acc: 0.9500\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 21/30\n",
      "12/11 [================================] - 10s 859ms/step - loss: 0.1075 - acc: 0.9583 - val_loss: 3.9812 - val_acc: 0.9750\n",
      "Epoch 22/30\n",
      "12/11 [================================] - 12s 986ms/step - loss: 0.0981 - acc: 0.9694 - val_loss: 5.2837 - val_acc: 0.9750\n",
      "Epoch 23/30\n",
      "12/11 [================================] - 10s 847ms/step - loss: 0.1296 - acc: 0.9472 - val_loss: 6.8125 - val_acc: 0.9250\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 24/30\n",
      "12/11 [================================] - 12s 996ms/step - loss: 0.1226 - acc: 0.9500 - val_loss: 6.3557 - val_acc: 0.9500\n",
      "Epoch 25/30\n",
      "12/11 [================================] - 10s 799ms/step - loss: 0.1221 - acc: 0.9611 - val_loss: 5.9689 - val_acc: 0.9500\n",
      "Epoch 26/30\n",
      "12/11 [================================] - 10s 840ms/step - loss: 0.0985 - acc: 0.9556 - val_loss: 4.4759 - val_acc: 0.9500\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 27/30\n",
      "12/11 [================================] - 10s 800ms/step - loss: 0.0989 - acc: 0.9583 - val_loss: 3.5940 - val_acc: 0.9750\n",
      "Epoch 28/30\n",
      "12/11 [================================] - 10s 810ms/step - loss: 0.1098 - acc: 0.9611 - val_loss: 3.3228 - val_acc: 0.9750\n",
      "Epoch 29/30\n",
      "12/11 [================================] - 10s 831ms/step - loss: 0.1024 - acc: 0.9639 - val_loss: 4.7970 - val_acc: 0.9750\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 30/30\n",
      "12/11 [================================] - 10s 867ms/step - loss: 0.1334 - acc: 0.9667 - val_loss: 5.3054 - val_acc: 0.9750\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97        20\n",
      "           1       0.95      1.00      0.98        20\n",
      "\n",
      "    accuracy                           0.97        40\n",
      "   macro avg       0.98      0.97      0.97        40\n",
      "weighted avg       0.98      0.97      0.97        40\n",
      "\n",
      "Kfold iteration 8/10\n",
      "Total images: 400 ---- Train images: 360 ---- Test images: 40\n",
      "Test index start:279 and finish index: 322\n",
      "Distribution of labels: Counter({1: 20, 0: 20})\n",
      "Epoch 1/30\n",
      "12/11 [================================] - 12s 1s/step - loss: 0.9656 - acc: 0.4972 - val_loss: 10.0499 - val_acc: 0.5000\n",
      "Epoch 2/30\n",
      "12/11 [================================] - 11s 952ms/step - loss: 0.6890 - acc: 0.6000 - val_loss: 27.6265 - val_acc: 0.5000\n",
      "Epoch 3/30\n",
      "12/11 [================================] - 12s 1s/step - loss: 0.6984 - acc: 0.5472 - val_loss: 101.4332 - val_acc: 0.5000\n",
      "Epoch 4/30\n",
      "12/11 [================================] - 13s 1s/step - loss: 0.7639 - acc: 0.5778 - val_loss: 6.3929 - val_acc: 0.5250\n",
      "Epoch 5/30\n",
      "12/11 [================================] - 13s 1s/step - loss: 0.7439 - acc: 0.5667 - val_loss: 15.8365 - val_acc: 0.5000\n",
      "Epoch 6/30\n",
      "12/11 [================================] - 13s 1s/step - loss: 0.7791 - acc: 0.7000 - val_loss: 4.4394 - val_acc: 0.8500\n",
      "Epoch 7/30\n",
      "12/11 [================================] - 12s 1s/step - loss: 0.5880 - acc: 0.7667 - val_loss: 2.8911 - val_acc: 0.9750\n",
      "Epoch 8/30\n",
      "12/11 [================================] - 13s 1s/step - loss: 0.4478 - acc: 0.7750 - val_loss: 582.2790 - val_acc: 0.5000\n",
      "Epoch 9/30\n",
      "12/11 [================================] - 12s 1s/step - loss: 0.6812 - acc: 0.7806 - val_loss: 56.9510 - val_acc: 0.6250\n",
      "Epoch 10/30\n",
      "12/11 [================================] - 12s 975ms/step - loss: 0.4214 - acc: 0.7667 - val_loss: 41.2960 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 11/30\n",
      "12/11 [================================] - 11s 881ms/step - loss: 0.4834 - acc: 0.7083 - val_loss: 8.2611 - val_acc: 0.9750\n",
      "Epoch 12/30\n",
      "12/11 [================================] - 10s 835ms/step - loss: 0.2804 - acc: 0.8694 - val_loss: 2.6999e-08 - val_acc: 1.0000\n",
      "Epoch 13/30\n",
      "12/11 [================================] - 10s 823ms/step - loss: 0.2433 - acc: 0.8861 - val_loss: 4.6396 - val_acc: 0.9750\n",
      "Epoch 14/30\n",
      "12/11 [================================] - 10s 844ms/step - loss: 0.2436 - acc: 0.9028 - val_loss: 1.9624 - val_acc: 0.9750\n",
      "Epoch 15/30\n",
      "12/11 [================================] - 10s 851ms/step - loss: 0.2370 - acc: 0.9194 - val_loss: 9.8055 - val_acc: 0.9500\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 16/30\n",
      "12/11 [================================] - 10s 811ms/step - loss: 0.1561 - acc: 0.9444 - val_loss: 2.5054 - val_acc: 0.9750\n",
      "Epoch 17/30\n",
      "12/11 [================================] - 10s 821ms/step - loss: 0.2150 - acc: 0.9194 - val_loss: 7.2949e-38 - val_acc: 1.0000\n",
      "Epoch 18/30\n",
      "12/11 [================================] - 11s 878ms/step - loss: 0.1723 - acc: 0.9250 - val_loss: 3.1332 - val_acc: 0.9750\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 19/30\n",
      "12/11 [================================] - 10s 865ms/step - loss: 0.1245 - acc: 0.9472 - val_loss: 5.0926 - val_acc: 0.9750\n",
      "Epoch 20/30\n",
      "12/11 [================================] - 10s 865ms/step - loss: 0.1437 - acc: 0.9250 - val_loss: 8.9640 - val_acc: 0.9500\n",
      "Epoch 21/30\n",
      "12/11 [================================] - 11s 926ms/step - loss: 0.1642 - acc: 0.9444 - val_loss: 8.0225 - val_acc: 0.9750\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 22/30\n",
      "12/11 [================================] - 11s 914ms/step - loss: 0.1166 - acc: 0.9667 - val_loss: 4.8883 - val_acc: 0.9750\n",
      "Epoch 23/30\n",
      "12/11 [================================] - 11s 887ms/step - loss: 0.1545 - acc: 0.9472 - val_loss: 0.9797 - val_acc: 0.9750\n",
      "Epoch 24/30\n",
      "12/11 [================================] - 10s 813ms/step - loss: 0.1350 - acc: 0.9444 - val_loss: 6.9687 - val_acc: 0.9750\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 25/30\n",
      "12/11 [================================] - 10s 853ms/step - loss: 0.1352 - acc: 0.9472 - val_loss: 3.5792 - val_acc: 0.9750\n",
      "Epoch 26/30\n",
      "12/11 [================================] - 10s 842ms/step - loss: 0.1349 - acc: 0.9500 - val_loss: 4.3288 - val_acc: 0.9750\n",
      "Epoch 27/30\n",
      "12/11 [================================] - 9s 788ms/step - loss: 0.1352 - acc: 0.9389 - val_loss: 5.4697 - val_acc: 0.9750\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 28/30\n",
      "12/11 [================================] - 10s 848ms/step - loss: 0.1371 - acc: 0.9444 - val_loss: 6.5974 - val_acc: 0.9750\n",
      "Epoch 29/30\n",
      "12/11 [================================] - 11s 879ms/step - loss: 0.1234 - acc: 0.9528 - val_loss: 6.2193 - val_acc: 0.9750\n",
      "Epoch 30/30\n",
      "12/11 [================================] - 10s 831ms/step - loss: 0.1224 - acc: 0.9500 - val_loss: 6.6941 - val_acc: 0.9750\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98        20\n",
      "           1       1.00      0.95      0.97        20\n",
      "\n",
      "    accuracy                           0.97        40\n",
      "   macro avg       0.98      0.97      0.97        40\n",
      "weighted avg       0.98      0.97      0.97        40\n",
      "\n",
      "Kfold iteration 9/10\n",
      "Total images: 400 ---- Train images: 360 ---- Test images: 40\n",
      "Test index start:317 and finish index: 365\n",
      "Distribution of labels: Counter({1: 20, 0: 20})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "12/11 [================================] - 11s 951ms/step - loss: 0.8930 - acc: 0.5417 - val_loss: 6.0720 - val_acc: 0.5000\n",
      "Epoch 2/30\n",
      "12/11 [================================] - 10s 822ms/step - loss: 0.9832 - acc: 0.6222 - val_loss: 0.5835 - val_acc: 0.9000\n",
      "Epoch 3/30\n",
      "12/11 [================================] - 10s 796ms/step - loss: 0.5935 - acc: 0.6917 - val_loss: 1.8656 - val_acc: 0.9000\n",
      "Epoch 4/30\n",
      "12/11 [================================] - 10s 834ms/step - loss: 0.5373 - acc: 0.7361 - val_loss: 11.2966 - val_acc: 0.8750\n",
      "Epoch 5/30\n",
      "12/11 [================================] - 10s 792ms/step - loss: 0.3254 - acc: 0.8639 - val_loss: 666.6355 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 6/30\n",
      "12/11 [================================] - 10s 849ms/step - loss: 0.4360 - acc: 0.8667 - val_loss: 10.9997 - val_acc: 0.9500\n",
      "Epoch 7/30\n",
      "12/11 [================================] - 11s 888ms/step - loss: 0.2769 - acc: 0.9028 - val_loss: 14.1753 - val_acc: 0.9500\n",
      "Epoch 8/30\n",
      "12/11 [================================] - 10s 835ms/step - loss: 0.1910 - acc: 0.9139 - val_loss: 28.9311 - val_acc: 0.9500\n",
      "Epoch 9/30\n",
      "12/11 [================================] - 10s 840ms/step - loss: 0.1913 - acc: 0.9222 - val_loss: 15.7665 - val_acc: 0.9500\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 10/30\n",
      "12/11 [================================] - 9s 783ms/step - loss: 0.1777 - acc: 0.9083 - val_loss: 15.6634 - val_acc: 0.9500\n",
      "Epoch 11/30\n",
      "12/11 [================================] - 10s 820ms/step - loss: 0.1542 - acc: 0.9361 - val_loss: 20.3645 - val_acc: 0.9500\n",
      "Epoch 12/30\n",
      "12/11 [================================] - 10s 819ms/step - loss: 0.1551 - acc: 0.9472 - val_loss: 56.7425 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 13/30\n",
      "12/11 [================================] - 10s 849ms/step - loss: 0.1746 - acc: 0.9361 - val_loss: 21.0369 - val_acc: 0.9500\n",
      "Epoch 14/30\n",
      "12/11 [================================] - 9s 779ms/step - loss: 0.1328 - acc: 0.9500 - val_loss: 24.5622 - val_acc: 0.9500\n",
      "Epoch 15/30\n",
      "12/11 [================================] - 9s 749ms/step - loss: 0.1468 - acc: 0.9417 - val_loss: 22.8558 - val_acc: 0.9500\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 16/30\n",
      "12/11 [================================] - 9s 751ms/step - loss: 0.1387 - acc: 0.9528 - val_loss: 23.1535 - val_acc: 0.9500\n",
      "Epoch 17/30\n",
      "12/11 [================================] - 10s 820ms/step - loss: 0.1231 - acc: 0.9639 - val_loss: 28.8553 - val_acc: 0.9500\n",
      "Epoch 18/30\n",
      "12/11 [================================] - 11s 898ms/step - loss: 0.1190 - acc: 0.9639 - val_loss: 22.2401 - val_acc: 0.9500\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 19/30\n",
      "12/11 [================================] - 11s 944ms/step - loss: 0.0968 - acc: 0.9583 - val_loss: 26.2269 - val_acc: 0.9500\n",
      "Epoch 20/30\n",
      "12/11 [================================] - 11s 916ms/step - loss: 0.1162 - acc: 0.9500 - val_loss: 24.7248 - val_acc: 0.9500\n",
      "Epoch 21/30\n",
      "12/11 [================================] - 12s 980ms/step - loss: 0.1291 - acc: 0.9444 - val_loss: 26.1557 - val_acc: 0.9500\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 22/30\n",
      "12/11 [================================] - 11s 895ms/step - loss: 0.1059 - acc: 0.9667 - val_loss: 26.3767 - val_acc: 0.9500\n",
      "Epoch 23/30\n",
      "12/11 [================================] - 10s 821ms/step - loss: 0.1454 - acc: 0.9500 - val_loss: 27.5540 - val_acc: 0.9500\n",
      "Epoch 24/30\n",
      "12/11 [================================] - 10s 857ms/step - loss: 0.0944 - acc: 0.9528 - val_loss: 28.4324 - val_acc: 0.9500\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "Epoch 25/30\n",
      "12/11 [================================] - 10s 865ms/step - loss: 0.1164 - acc: 0.9528 - val_loss: 27.2666 - val_acc: 0.9500\n",
      "Epoch 26/30\n",
      "12/11 [================================] - 10s 812ms/step - loss: 0.1272 - acc: 0.9528 - val_loss: 26.3987 - val_acc: 0.9500\n",
      "Epoch 27/30\n",
      "12/11 [================================] - 10s 822ms/step - loss: 0.1111 - acc: 0.9583 - val_loss: 26.3257 - val_acc: 0.9500\n",
      "Epoch 28/30\n",
      "12/11 [================================] - 10s 855ms/step - loss: 0.1494 - acc: 0.9583 - val_loss: 26.6974 - val_acc: 0.9500\n",
      "Epoch 29/30\n",
      "12/11 [================================] - 10s 793ms/step - loss: 0.1387 - acc: 0.9389 - val_loss: 26.6124 - val_acc: 0.9500\n",
      "Epoch 30/30\n",
      "12/11 [================================] - 9s 735ms/step - loss: 0.1413 - acc: 0.9583 - val_loss: 25.3856 - val_acc: 0.9500\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.95        20\n",
      "           1       1.00      0.90      0.95        20\n",
      "\n",
      "    accuracy                           0.95        40\n",
      "   macro avg       0.95      0.95      0.95        40\n",
      "weighted avg       0.95      0.95      0.95        40\n",
      "\n",
      "Kfold iteration 10/10\n",
      "Total images: 400 ---- Train images: 360 ---- Test images: 40\n",
      "Test index start:357 and finish index: 399\n",
      "Distribution of labels: Counter({1: 20, 0: 20})\n",
      "Epoch 1/30\n",
      "12/11 [================================] - 10s 837ms/step - loss: 1.2884 - acc: 0.5278 - val_loss: 10.7843 - val_acc: 0.5000\n",
      "Epoch 2/30\n",
      "12/11 [================================] - 9s 738ms/step - loss: 0.6782 - acc: 0.5917 - val_loss: 1.5404 - val_acc: 0.9000\n",
      "Epoch 3/30\n",
      "12/11 [================================] - 9s 739ms/step - loss: 0.5634 - acc: 0.6833 - val_loss: 8.0915 - val_acc: 0.9250\n",
      "Epoch 4/30\n",
      "12/11 [================================] - 9s 731ms/step - loss: 0.5218 - acc: 0.8194 - val_loss: 94.7643 - val_acc: 0.7500\n",
      "Epoch 5/30\n",
      "12/11 [================================] - 9s 730ms/step - loss: 0.5036 - acc: 0.7833 - val_loss: 16.1537 - val_acc: 0.9500\n",
      "Epoch 6/30\n",
      "12/11 [================================] - 9s 737ms/step - loss: 0.2613 - acc: 0.8861 - val_loss: 21.3387 - val_acc: 0.9500\n",
      "Epoch 7/30\n",
      "12/11 [================================] - 9s 754ms/step - loss: 0.2927 - acc: 0.8694 - val_loss: 18.4050 - val_acc: 0.9500\n",
      "Epoch 8/30\n",
      "12/11 [================================] - 9s 734ms/step - loss: 0.2289 - acc: 0.9250 - val_loss: 72.8554 - val_acc: 0.8500\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 9/30\n",
      "12/11 [================================] - 9s 752ms/step - loss: 0.2512 - acc: 0.9111 - val_loss: 41.7197 - val_acc: 0.9500\n",
      "Epoch 10/30\n",
      "12/11 [================================] - 9s 785ms/step - loss: 0.1560 - acc: 0.9278 - val_loss: 67.1244 - val_acc: 0.8750\n",
      "Epoch 11/30\n",
      "12/11 [================================] - 10s 849ms/step - loss: 0.1821 - acc: 0.9194 - val_loss: 34.3783 - val_acc: 0.9500\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 12/30\n",
      "12/11 [================================] - 9s 773ms/step - loss: 0.1163 - acc: 0.9500 - val_loss: 42.0579 - val_acc: 0.9500\n",
      "Epoch 13/30\n",
      "12/11 [================================] - 9s 775ms/step - loss: 0.1278 - acc: 0.9472 - val_loss: 44.8928 - val_acc: 0.9500\n",
      "Epoch 14/30\n",
      "12/11 [================================] - 10s 807ms/step - loss: 0.1424 - acc: 0.9389 - val_loss: 39.7422 - val_acc: 0.9500\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 15/30\n",
      "12/11 [================================] - 11s 896ms/step - loss: 0.1149 - acc: 0.9583 - val_loss: 42.9441 - val_acc: 0.9500\n",
      "Epoch 16/30\n",
      "12/11 [================================] - 10s 816ms/step - loss: 0.1390 - acc: 0.9528 - val_loss: 36.4829 - val_acc: 0.9500\n",
      "Epoch 17/30\n",
      "12/11 [================================] - 10s 828ms/step - loss: 0.1156 - acc: 0.9444 - val_loss: 38.8860 - val_acc: 0.9500\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 18/30\n",
      "12/11 [================================] - 10s 818ms/step - loss: 0.1401 - acc: 0.9556 - val_loss: 36.7148 - val_acc: 0.9500\n",
      "Epoch 19/30\n",
      "12/11 [================================] - 11s 892ms/step - loss: 0.0926 - acc: 0.9556 - val_loss: 40.4825 - val_acc: 0.9500\n",
      "Epoch 20/30\n",
      "12/11 [================================] - 12s 981ms/step - loss: 0.1213 - acc: 0.9528 - val_loss: 38.6856 - val_acc: 0.9500\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 21/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/11 [================================] - 10s 799ms/step - loss: 0.1079 - acc: 0.9583 - val_loss: 39.2923 - val_acc: 0.9500\n",
      "Epoch 22/30\n",
      "12/11 [================================] - 10s 874ms/step - loss: 0.1016 - acc: 0.9694 - val_loss: 39.7879 - val_acc: 0.9500\n",
      "Epoch 23/30\n",
      "12/11 [================================] - 10s 823ms/step - loss: 0.1213 - acc: 0.9472 - val_loss: 40.9790 - val_acc: 0.9500\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 24/30\n",
      "12/11 [================================] - 10s 800ms/step - loss: 0.1068 - acc: 0.9583 - val_loss: 40.7214 - val_acc: 0.9500\n",
      "Epoch 25/30\n",
      "12/11 [================================] - 10s 797ms/step - loss: 0.1132 - acc: 0.9528 - val_loss: 40.7899 - val_acc: 0.9500\n",
      "Epoch 26/30\n",
      "12/11 [================================] - 10s 797ms/step - loss: 0.0960 - acc: 0.9694 - val_loss: 40.8127 - val_acc: 0.9500\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "Epoch 27/30\n",
      "12/11 [================================] - 10s 814ms/step - loss: 0.1122 - acc: 0.9528 - val_loss: 40.4805 - val_acc: 0.9500\n",
      "Epoch 28/30\n",
      "12/11 [================================] - 10s 804ms/step - loss: 0.0945 - acc: 0.9556 - val_loss: 40.4340 - val_acc: 0.9500\n",
      "Epoch 29/30\n",
      "12/11 [================================] - 11s 897ms/step - loss: 0.1200 - acc: 0.9556 - val_loss: 40.3620 - val_acc: 0.9500\n",
      "Epoch 30/30\n",
      "12/11 [================================] - 10s 805ms/step - loss: 0.0929 - acc: 0.9694 - val_loss: 40.4017 - val_acc: 0.9500\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95        20\n",
      "           1       0.95      0.95      0.95        20\n",
      "\n",
      "    accuracy                           0.95        40\n",
      "   macro avg       0.95      0.95      0.95        40\n",
      "weighted avg       0.95      0.95      0.95        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reports = []\n",
    "\n",
    "kf = StratifiedKFold(n_splits=10)\n",
    "\n",
    "# shuffle data\n",
    "id = 1\n",
    "for train_index, test_index in kf.split(x,y):\n",
    "    print('Kfold iteration {}/10'.format(id))\n",
    "    id += 1\n",
    "    print('Total images: {} ---- Train images: {} ---- Test images: {}'.format(len(x),len(train_index),len(test_index)))\n",
    "    #print(\"Train index start:{} and finish index: {}\".format(train_index[0],train_index[-1]))\n",
    "    print(\"Test index start:{} and finish index: {}\".format(test_index[0],test_index[-1]))\n",
    "    \n",
    "    X_train, X_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    print('Distribution of labels: {}'.format(collections.Counter(y_test)))\n",
    "    \n",
    "    model = build_model2()\n",
    "\n",
    "    datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                 rotation_range=10, # randomly rotate images in the range (degrees, 0 to 180)\n",
    "                                 width_shift_range=0.1, # randomly shift images horizontally (fraction of total width)\n",
    "                                 height_shift_range=0.1, \n",
    "                                 shear_range=0.1,\n",
    "                                 zoom_range=0.1)   \n",
    "    \n",
    "    datagen.fit(X_train)\n",
    "\n",
    "    learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00001)\n",
    "    \n",
    "    history = model.fit_generator(datagen.flow(X_train, y_train, batch_size = 32), epochs = 30, \n",
    "                              validation_data = (X_test,y_test), steps_per_epoch=len(X_train) / 32,\n",
    "                              callbacks=[learning_rate_reduction])\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = [np.round(p[0]) for p in y_pred]\n",
    "    \n",
    "    print(classification_report(y_test, y_pred))\n",
    "    reports.append(classification_report(y_test, y_pred,output_dict=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1s3 = []\n",
    "for rep in reports:\n",
    "    f1s3.append(rep['0']['f1-score'])\n",
    "    f1s3.append(rep['1']['f1-score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9410731116866572"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9361187755781935"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(f1s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9599498981976516"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(f1s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9545454545454545,\n",
       " 0.9444444444444444,\n",
       " 0.9523809523809523,\n",
       " 0.9473684210526315,\n",
       " 0.9032258064516129,\n",
       " 0.9387755102040817,\n",
       " 0.8292682926829269,\n",
       " 0.8205128205128205,\n",
       " 0.9444444444444444,\n",
       " 0.9545454545454546,\n",
       " 0.9583333333333334,\n",
       " 0.9375,\n",
       " 0.896551724137931,\n",
       " 0.9411764705882353,\n",
       " 0.975609756097561,\n",
       " 0.9743589743589743,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9787234042553191,\n",
       " 0.9696969696969697]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9777777777777777,\n",
       " 0.9714285714285714,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.9032258064516129,\n",
       " 0.9387755102040817,\n",
       " 0.8205128205128205,\n",
       " 0.8292682926829269,\n",
       " 0.972972972972973,\n",
       " 0.9767441860465117,\n",
       " 0.9795918367346939,\n",
       " 0.967741935483871,\n",
       " 0.8387096774193549,\n",
       " 0.8979591836734694,\n",
       " 0.975609756097561,\n",
       " 0.9743589743589743,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9565217391304348,\n",
       " 0.9411764705882353]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.8947368421052632,\n",
       " 0.9047619047619048,\n",
       " 0.8717948717948718,\n",
       " 0.8780487804878048,\n",
       " 0.975609756097561,\n",
       " 0.9743589743589743,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9743589743589743,\n",
       " 0.975609756097561,\n",
       " 0.975609756097561,\n",
       " 0.9743589743589743,\n",
       " 0.9523809523809523,\n",
       " 0.9473684210526316,\n",
       " 0.9500000000000001,\n",
       " 0.9500000000000001]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
