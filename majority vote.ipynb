{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(2)\n",
    "\n",
    "import collections\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing import image\n",
    "\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import labels and images to arrays**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv('./SMILE_Dataset/annotations.csv', header=None, names=['fname','label'])\n",
    "# Shuffle data\n",
    "labels = labels.sample(frac=1).reset_index()\n",
    "\n",
    "x = np.array([image.img_to_array(image.load_img('./SMILE_Dataset/all/'+fname, target_size=(128, 128))) for fname in labels['fname']])\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(labels['label'])\n",
    "y = integer_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create train and validation set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(x, y, test_size=0.10, random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>algo1</th>\n",
       "      <th>algo2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>355</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>356</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>357</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>358</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>359</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>360 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     algo1  algo2\n",
       "0      1.0    1.0\n",
       "1      1.0    1.0\n",
       "2      0.0    0.0\n",
       "3      1.0    1.0\n",
       "4      0.0    0.0\n",
       "..     ...    ...\n",
       "355    1.0    1.0\n",
       "356    1.0    1.0\n",
       "357    1.0    1.0\n",
       "358    1.0    1.0\n",
       "359    1.0    1.0\n",
       "\n",
       "[360 rows x 2 columns]"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'algo1':y_pred,\n",
    "              'algo2':y_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_train)\n",
    "#y_pred = [np.round(p[0]) for p in y_pred]\n",
    "y_pred2_test = y_pred2_test.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred = [np.round(p[0]) for p in y\n",
    "temp_train = pd.DataFrame({'algo1':y_pred_test,\n",
    "              'algo2':y_pred2_test,\n",
    "                          'algo3':y_pred_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_train['new'] = temp_train.mode(axis=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>algo1</th>\n",
       "      <th>algo2</th>\n",
       "      <th>algo3</th>\n",
       "      <th>new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    algo1  algo2  algo3  new\n",
       "0     1.0      0    1.0  1.0\n",
       "1     1.0      0    1.0  1.0\n",
       "2     1.0      0    1.0  1.0\n",
       "3     1.0      0    1.0  1.0\n",
       "4     1.0      0    1.0  1.0\n",
       "5     1.0      0    1.0  1.0\n",
       "6     1.0      0    1.0  1.0\n",
       "7     1.0      0    1.0  1.0\n",
       "8     1.0      0    1.0  1.0\n",
       "9     1.0      0    1.0  1.0\n",
       "10    1.0      0    1.0  1.0\n",
       "11    1.0      0    1.0  1.0\n",
       "12    1.0      0    1.0  1.0\n",
       "13    1.0      0    1.0  1.0\n",
       "14    1.0      0    1.0  1.0\n",
       "15    1.0      0    1.0  1.0\n",
       "16    1.0      0    1.0  1.0\n",
       "17    1.0      0    1.0  1.0\n",
       "18    1.0      0    1.0  1.0\n",
       "19    1.0      0    1.0  1.0\n",
       "20    1.0      0    1.0  1.0\n",
       "21    1.0      0    1.0  1.0\n",
       "22    1.0      0    1.0  1.0\n",
       "23    1.0      0    1.0  1.0\n",
       "24    1.0      0    1.0  1.0\n",
       "25    1.0      0    1.0  1.0\n",
       "26    1.0      0    1.0  1.0\n",
       "27    1.0      0    1.0  1.0\n",
       "28    1.0      0    1.0  1.0\n",
       "29    1.0      0    1.0  1.0\n",
       "30    1.0      0    1.0  1.0\n",
       "31    1.0      0    1.0  1.0\n",
       "32    1.0      0    1.0  1.0\n",
       "33    1.0      0    1.0  1.0\n",
       "34    1.0      0    1.0  1.0\n",
       "35    1.0      0    1.0  1.0\n",
       "36    1.0      0    1.0  1.0\n",
       "37    1.0      0    1.0  1.0\n",
       "38    1.0      0    1.0  1.0\n",
       "39    1.0      0    1.0  1.0"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My models are having time using this split of data. Therefore, part of my error analysis will be investigating why!\n",
    "# And not use kfold because it takes too long\n",
    "\n",
    "X_train = np.concatenate((x[0:120], x[160::]), axis = 0)\n",
    "y_train = np.concatenate((y[0:120], y[160::]), axis = 0)\n",
    "\n",
    "X_test = x[120:160]\n",
    "y_test = y[120:160]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "12/11 [================================] - 9s 717ms/step - loss: 0.6914 - acc: 0.5000 - val_loss: 25.8741 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "12/11 [================================] - 6s 518ms/step - loss: 0.6735 - acc: 0.5792 - val_loss: 49.8936 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "12/11 [================================] - 6s 502ms/step - loss: 0.6583 - acc: 0.6750 - val_loss: 35.7561 - val_acc: 0.6000\n",
      "Epoch 4/10\n",
      "12/11 [================================] - 7s 542ms/step - loss: 0.5701 - acc: 0.7042 - val_loss: 3.1082 - val_acc: 0.9250\n",
      "Epoch 5/10\n",
      "12/11 [================================] - 6s 518ms/step - loss: 0.4760 - acc: 0.7833 - val_loss: 94.2393 - val_acc: 0.6500\n",
      "Epoch 6/10\n",
      "12/11 [================================] - 6s 504ms/step - loss: 0.3984 - acc: 0.8417 - val_loss: 2.2915 - val_acc: 0.9500\n",
      "Epoch 7/10\n",
      "12/11 [================================] - 6s 541ms/step - loss: 0.3272 - acc: 0.8750 - val_loss: 52.5875 - val_acc: 0.8500\n",
      "Epoch 8/10\n",
      "12/11 [================================] - 6s 537ms/step - loss: 0.3018 - acc: 0.9000 - val_loss: 16.7268 - val_acc: 0.9250\n",
      "Epoch 9/10\n",
      "12/11 [================================] - 6s 508ms/step - loss: 0.2766 - acc: 0.9042 - val_loss: 41.4109 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 10/10\n",
      "12/11 [================================] - 6s 502ms/step - loss: 0.2069 - acc: 0.9083 - val_loss: 31.3323 - val_acc: 0.9250\n",
      "Epoch 1/10\n",
      "12/11 [================================] - 9s 735ms/step - loss: 1.2257 - acc: 0.5167 - val_loss: 20.3758 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "12/11 [================================] - 6s 509ms/step - loss: 0.6853 - acc: 0.5583 - val_loss: 1.3123 - val_acc: 0.8750\n",
      "Epoch 3/10\n",
      "12/11 [================================] - 6s 498ms/step - loss: 0.6468 - acc: 0.6333 - val_loss: 21.5897 - val_acc: 0.6500\n",
      "Epoch 4/10\n",
      "12/11 [================================] - 6s 533ms/step - loss: 0.5262 - acc: 0.7542 - val_loss: 33.0407 - val_acc: 0.8500\n",
      "Epoch 5/10\n",
      "12/11 [================================] - 6s 512ms/step - loss: 0.4119 - acc: 0.8292 - val_loss: 10.3445 - val_acc: 0.9250\n",
      "Epoch 6/10\n",
      "12/11 [================================] - 6s 512ms/step - loss: 0.2652 - acc: 0.9125 - val_loss: 12.7668 - val_acc: 0.9500\n",
      "Epoch 7/10\n",
      "12/11 [================================] - 6s 538ms/step - loss: 0.2382 - acc: 0.9250 - val_loss: 20.2404 - val_acc: 0.9250\n",
      "Epoch 8/10\n",
      "12/11 [================================] - 6s 500ms/step - loss: 0.3521 - acc: 0.8750 - val_loss: 23.6711 - val_acc: 0.9250\n",
      "Epoch 9/10\n",
      "12/11 [================================] - 6s 508ms/step - loss: 0.2152 - acc: 0.9167 - val_loss: 25.4340 - val_acc: 0.9250\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 10/10\n",
      "12/11 [================================] - 6s 507ms/step - loss: 0.1635 - acc: 0.9500 - val_loss: 22.4663 - val_acc: 0.9250\n",
      "Epoch 1/10\n",
      "12/11 [================================] - 32s 3s/step - loss: 3.6314 - acc: 0.5625 - val_loss: 0.3662 - val_acc: 0.8750\n",
      "Epoch 2/10\n",
      "12/11 [================================] - 25s 2s/step - loss: 0.6610 - acc: 0.6458 - val_loss: 46.3092 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "12/11 [================================] - 25s 2s/step - loss: 0.6961 - acc: 0.5833 - val_loss: 98.5335 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "12/11 [================================] - 25s 2s/step - loss: 0.5970 - acc: 0.6833 - val_loss: 1.9112 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 5/10\n",
      "12/11 [================================] - 25s 2s/step - loss: 0.6681 - acc: 0.6542 - val_loss: 6.0766 - val_acc: 0.8500\n",
      "Epoch 6/10\n",
      "12/11 [================================] - 27s 2s/step - loss: 0.5931 - acc: 0.7458 - val_loss: 21.8177 - val_acc: 0.8250\n",
      "Epoch 7/10\n",
      "12/11 [================================] - 25s 2s/step - loss: 0.3529 - acc: 0.8375 - val_loss: 48.9803 - val_acc: 0.8500\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 8/10\n",
      "12/11 [================================] - 27s 2s/step - loss: 0.2902 - acc: 0.8875 - val_loss: 36.3844 - val_acc: 0.8250\n",
      "Epoch 9/10\n",
      "12/11 [================================] - 26s 2s/step - loss: 0.2405 - acc: 0.9125 - val_loss: 61.7499 - val_acc: 0.8750\n",
      "Epoch 10/10\n",
      "12/11 [================================] - 25s 2s/step - loss: 0.2301 - acc: 0.9167 - val_loss: 92.0567 - val_acc: 0.9000\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y_pred3_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-257-30982734ccae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[0my_pred_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_pred_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[0my_pred2_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_pred2_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m \u001b[0my_pred3_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_pred3_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_pred3_test' is not defined"
     ]
    }
   ],
   "source": [
    "model = build_model1()\n",
    "model2 = build_model()\n",
    "model3 = build_model2()\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255,\n",
    "                             rotation_range=10, # randomly rotate images in the range (degrees, 0 to 180)\n",
    "                             width_shift_range=0.1, # randomly shift images horizontally (fraction of total width)\n",
    "                             height_shift_range=0.1, \n",
    "                             shear_range=0.1,\n",
    "                             zoom_range=0.1)   \n",
    "\n",
    "datagen.fit(X_train)\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "                                        patience=3, \n",
    "                                        verbose=1, \n",
    "                                        factor=0.5, \n",
    "                                        min_lr=0.00001)\n",
    "\n",
    "history = model.fit_generator(datagen.flow(X_train, y_train, batch_size = 20), epochs = 10, \n",
    "                          validation_data = (X_test,y_test), steps_per_epoch=len(X_train) / 32,\n",
    "                          callbacks=[learning_rate_reduction])\n",
    "\n",
    "history2 = model2.fit_generator(datagen.flow(X_train, y_train, batch_size = 20), epochs = 10, \n",
    "                          validation_data = (X_test,y_test), steps_per_epoch=len(X_train) / 32,\n",
    "                          callbacks=[learning_rate_reduction])\n",
    "\n",
    "history3 = model3.fit_generator(datagen.flow(X_train, y_train, batch_size = 20), epochs = 10, \n",
    "                          validation_data = (X_test,y_test), steps_per_epoch=len(X_train) / 32,\n",
    "                          callbacks=[learning_rate_reduction])\n",
    "\n",
    "# # Retrieve training predictions to train stacking aglortihm, in this case majority vote\n",
    "# y_pred = model.predict(X_train)\n",
    "# y_pred = y_pred.astype(int) # Convert float to int\n",
    "\n",
    "# y_pred2 = model2.predict(X_train)\n",
    "# y_pred2 = y_pred2.astype(int) # Convert float to int\n",
    "\n",
    "# y_pred3 = model3.predict(X_train)\n",
    "# y_pred3 = y_pred3.astype(int) # Convert float to int\n",
    "\n",
    "# # Reshape the data (360, 1) -> (360)\n",
    "\n",
    "# y_pred = y_pred.reshape(360)\n",
    "# y_pred2 = y_pred2.reshape(360)\n",
    "# y_pred3 = y_pred3.reshape(360)\n",
    "\n",
    "\n",
    "# temp_train = pd.DataFrame({'algo1':y_pred,\n",
    "#                            'algo2':y_pred2,\n",
    "#                            'algo3':y_pred3})\n",
    "\n",
    "# # Majority vote using the mode\n",
    "# temp_train['new'] = temp_train.mode(axis=1)[0]\n",
    "\n",
    "# clf = SVC(gamma='auto')\n",
    "# clf.fit(temp_train, y_train)\n",
    "\n",
    "# now we get test\n",
    "\n",
    "y_pred_test = model.predict(X_test)\n",
    "y_pred_test = y_pred_test.astype(int) # Convert float to int\n",
    "y_pred2_test = model2.predict(X_test)\n",
    "y_pred2_test = y_pred2_test.astype(int) # Convert float to int\n",
    "y_pred3_test = model3.predict(X_test)\n",
    "y_pred3_test = y_pred3.astype(int) # Convert float to int\n",
    "\n",
    "y_pred_test = y_pred_test.reshape(40)\n",
    "y_pred2_test = y_pred2_test.reshape(40)\n",
    "y_pred3_test = y_pred3_test.reshape(40)\n",
    "\n",
    "\n",
    "temp_test = pd.DataFrame({'algo1':y_pred_test,\n",
    "                           'algo2':y_pred2_test,\n",
    "                           'algo3':y_pred3_test})\n",
    "\n",
    "temp_test['prediction'] = temp_test.mode(axis=1)[0]\n",
    "\n",
    "#preds = clf.predict(temp_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred_test = model.predict(X_test)\n",
    "y_pred_test = y_pred_test.astype(int) # Convert float to int\n",
    "y_pred2_test = model2.predict(X_test)\n",
    "y_pred2_test = y_pred2_test.astype(int) # Convert float to int\n",
    "y_pred3_test = model3.predict(X_test)\n",
    "y_pred3_test = y_pred3_test.astype(int) # Convert float to int\n",
    "\n",
    "y_pred_test = y_pred_test.reshape(40)\n",
    "y_pred2_test = y_pred2_test.reshape(40)\n",
    "y_pred3_test = y_pred3_test.reshape(40)\n",
    "\n",
    "\n",
    "temp_test = pd.DataFrame({'algo1':y_pred_test,\n",
    "                           'algo2':y_pred2_test,\n",
    "                           'algo3':y_pred3_test})\n",
    "\n",
    "temp_test['prediction'] = temp_test.mode(axis=1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.85      0.92        20\n",
      "           1       0.87      1.00      0.93        20\n",
      "\n",
      "    accuracy                           0.93        40\n",
      "   macro avg       0.93      0.93      0.92        40\n",
      "weighted avg       0.93      0.93      0.92        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, temp_test['prediction']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.80      0.89        20\n",
      "           1       0.83      1.00      0.91        20\n",
      "\n",
      "    accuracy                           0.90        40\n",
      "   macro avg       0.92      0.90      0.90        40\n",
      "weighted avg       0.92      0.90      0.90        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred3_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = y_pred_test.reshape(40)\n",
    "y_pred2_test = y_pred2_test.reshape(40)\n",
    "\n",
    "temp_train = pd.DataFrame({'algo1':y_pred_test,\n",
    "              'algo2':y_pred2_test})\n",
    "\n",
    "preds = clf.predict(temp_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0000000e+00, 0.0000000e+00, 5.2452087e-06, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 9.4473362e-05, 3.7446320e-03,\n",
       "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 1.5380979e-04,\n",
       "       1.4901161e-06, 5.6624413e-07, 0.0000000e+00, 6.3776970e-06,\n",
       "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 1.4603138e-05,\n",
       "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 1.4901161e-06,\n",
       "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 1.4901161e-07, 6.8545341e-07, 6.4432621e-05],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred2_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67        20\n",
      "           1       0.00      0.00      0.00        20\n",
      "\n",
      "    accuracy                           0.50        40\n",
      "   macro avg       0.25      0.50      0.33        40\n",
      "weighted avg       0.25      0.50      0.33        40\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\justin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([-1,1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([-1,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.50555557,\n",
       " 0.69166666,\n",
       " 0.725,\n",
       " 0.73055553,\n",
       " 0.8138889,\n",
       " 0.88611114,\n",
       " 0.90555555,\n",
       " 0.9,\n",
       " 0.91944444,\n",
       " 0.8972222]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import InputLayer, Input\n",
    "from keras.layers import Conv2D, Dense, Flatten, Dropout, Activation\n",
    "from keras.layers import BatchNormalization, Reshape, MaxPooling2D, GlobalAveragePooling2D\n",
    "def build_model4():\n",
    "    dropout_dense_layer = 0.1\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), input_shape=(64, 64, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(dropout_dense_layer))\n",
    "\n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(dropout_dense_layer))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    model.compile(loss=keras.losses.binary_crossentropy,\n",
    "              optimizer=optimizers.Adam(lr=0.001),\n",
    "              metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "t = build_model4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_56\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_175 (Conv2D)          (None, 62, 62, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 62, 62, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 62, 62, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_176 (Conv2D)          (None, 60, 60, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 60, 60, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 60, 60, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_177 (Conv2D)          (None, 58, 58, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 58, 58, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 58, 58, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_109 (MaxPoolin (None, 29, 29, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_178 (Conv2D)          (None, 27, 27, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 27, 27, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 27, 27, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_179 (Conv2D)          (None, 25, 25, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 25, 25, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 25, 25, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_180 (Conv2D)          (None, 23, 23, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 23, 23, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 23, 23, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_110 (MaxPoolin (None, 11, 11, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_181 (Conv2D)          (None, 9, 9, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 9, 9, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_55 (Flatten)         (None, 10368)             0         \n",
      "_________________________________________________________________\n",
      "dense_109 (Dense)            (None, 1024)              10617856  \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_110 (Dense)            (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_111 (Dense)            (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 11,067,777\n",
      "Trainable params: 11,066,945\n",
      "Non-trainable params: 832\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "t.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model3():\n",
    "    \n",
    "    model = models.Sequential()\n",
    "\n",
    "    model.add(layers.Conv2D(32, (5,5),padding = 'Same', activation ='relu', input_shape=(64, 64, 3)))\n",
    "    model.add(layers.Conv2D(32, (5,5),padding = 'Same', activation ='relu'))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(layers.Dropout(0.25))\n",
    "\n",
    "\n",
    "    model.add(layers.Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n",
    "    model.add(layers.Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "    model.add(layers.Dropout(0.25))\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0),\n",
    "              metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model2():\n",
    "    \n",
    "    model = models.Sequential()\n",
    "\n",
    "    model.add(layers.Conv2D(32, (5,5),padding = 'Same', activation ='relu', input_shape=(128, 128, 3)))\n",
    "    model.add(layers.Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', activation ='relu'))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(layers.Dropout(0.25))\n",
    "\n",
    "    model.add(layers.Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n",
    "    model.add(layers.Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "    model.add(layers.Dropout(0.25))\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0),\n",
    "              metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model1():\n",
    "\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    # Feed to a densily connected layer for prediction\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128))\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.Dropout(.2))\n",
    "\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=0.0002, rho=0.9, epsilon=1e-08, decay=0.0),\n",
    "              metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "    # Feed to a densily connected layer for prediction\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0),\n",
    "              metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                  rotation_range=10,\n",
    "                                  width_shift_range=0.1,\n",
    "                                  shear_range=0.1,\n",
    "                                  zoom_range=0.1,\n",
    "                                  horizontal_flip=True,\n",
    "                                  fill_mode='nearest')\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "12/11 [================================] - 2s 161ms/step - loss: 0.0776 - acc: 0.9694 - val_loss: 9.2421 - val_acc: 0.9500\n",
      "Epoch 2/20\n",
      "12/11 [================================] - 2s 156ms/step - loss: 0.0808 - acc: 0.9722 - val_loss: 9.1715 - val_acc: 0.9500\n",
      "Epoch 3/20\n",
      "12/11 [================================] - 2s 154ms/step - loss: 0.0772 - acc: 0.9750 - val_loss: 9.1436 - val_acc: 0.9500\n",
      "Epoch 4/20\n",
      "12/11 [================================] - 2s 159ms/step - loss: 0.0770 - acc: 0.9778 - val_loss: 9.0988 - val_acc: 0.9500\n",
      "Epoch 5/20\n",
      "12/11 [================================] - 2s 162ms/step - loss: 0.0764 - acc: 0.9750 - val_loss: 9.0718 - val_acc: 0.9500\n",
      "Epoch 6/20\n",
      "12/11 [================================] - 2s 163ms/step - loss: 0.1075 - acc: 0.9722 - val_loss: 8.9662 - val_acc: 0.9500\n",
      "Epoch 7/20\n",
      "12/11 [================================] - 2s 162ms/step - loss: 0.0776 - acc: 0.9722 - val_loss: 8.8984 - val_acc: 0.9500\n",
      "Epoch 8/20\n",
      "12/11 [================================] - 2s 154ms/step - loss: 0.0761 - acc: 0.9806 - val_loss: 8.8261 - val_acc: 0.9500\n",
      "Epoch 9/20\n",
      "12/11 [================================] - 2s 163ms/step - loss: 0.0776 - acc: 0.9750 - val_loss: 8.7707 - val_acc: 0.9500\n",
      "Epoch 10/20\n",
      "12/11 [================================] - 2s 152ms/step - loss: 0.0753 - acc: 0.9778 - val_loss: 8.6617 - val_acc: 0.9500\n",
      "Epoch 11/20\n",
      "12/11 [================================] - 2s 153ms/step - loss: 0.0827 - acc: 0.9778 - val_loss: 8.5858 - val_acc: 0.9500\n",
      "Epoch 12/20\n",
      "12/11 [================================] - 2s 153ms/step - loss: 0.0794 - acc: 0.9750 - val_loss: 8.5058 - val_acc: 0.9500\n",
      "Epoch 13/20\n",
      "12/11 [================================] - 2s 161ms/step - loss: 0.0774 - acc: 0.9750 - val_loss: 8.5162 - val_acc: 0.9500\n",
      "Epoch 14/20\n",
      "12/11 [================================] - 2s 160ms/step - loss: 0.0751 - acc: 0.9750 - val_loss: 8.4742 - val_acc: 0.9500\n",
      "Epoch 15/20\n",
      "12/11 [================================] - 2s 159ms/step - loss: 0.0792 - acc: 0.9750 - val_loss: 8.3512 - val_acc: 0.9500\n",
      "Epoch 16/20\n",
      "12/11 [================================] - 2s 156ms/step - loss: 0.0790 - acc: 0.9778 - val_loss: 8.3204 - val_acc: 0.9500\n",
      "Epoch 17/20\n",
      "12/11 [================================] - 2s 155ms/step - loss: 0.0732 - acc: 0.9778 - val_loss: 8.2958 - val_acc: 0.9500\n",
      "Epoch 18/20\n",
      "12/11 [================================] - 2s 158ms/step - loss: 0.0744 - acc: 0.9778 - val_loss: 8.9515 - val_acc: 0.9750\n",
      "Epoch 19/20\n",
      "12/11 [================================] - 2s 162ms/step - loss: 0.0771 - acc: 0.9778 - val_loss: 8.1797 - val_acc: 0.9500\n",
      "Epoch 20/20\n",
      "12/11 [================================] - 2s 158ms/step - loss: 0.0730 - acc: 0.9778 - val_loss: 8.1719 - val_acc: 0.9500\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(datagen.flow(X_train, y_train, batch_size = 32), epochs = 50, \n",
    "                              validation_data = (X_val,y_val), steps_per_epoch=len(X_train) / 32,\n",
    "                              callbacks=[learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "?classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95        19\n",
      "           1       0.95      0.95      0.95        21\n",
      "\n",
      "    accuracy                           0.95        40\n",
      "   macro avg       0.95      0.95      0.95        40\n",
      "weighted avg       0.95      0.95      0.95        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K FOLD!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kfold iteration 1/10\n",
      "Total images: 400 ---- Train images: 360 ---- Test images: 40\n",
      "Test index start:0 and finish index: 41\n",
      "Distribution of labels: Counter({1: 20, 0: 20})\n",
      "Epoch 1/20\n",
      "12/11 [================================] - 8s 683ms/step - loss: 0.6930 - acc: 0.5000 - val_loss: 28.3470 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      "12/11 [================================] - 6s 529ms/step - loss: 0.6612 - acc: 0.6083 - val_loss: 3.0695 - val_acc: 0.8750\n",
      "Epoch 3/20\n",
      "12/11 [================================] - 6s 505ms/step - loss: 0.6150 - acc: 0.6875 - val_loss: 32.1166 - val_acc: 0.7250\n",
      "Epoch 4/20\n",
      "12/11 [================================] - 6s 530ms/step - loss: 0.5055 - acc: 0.7833 - val_loss: 21.2925 - val_acc: 0.8750\n",
      "Epoch 5/20\n",
      "12/11 [================================] - 6s 540ms/step - loss: 0.4121 - acc: 0.8708 - val_loss: 13.8512 - val_acc: 0.9000\n",
      "Epoch 6/20\n",
      "12/11 [================================] - 6s 504ms/step - loss: 0.3619 - acc: 0.8542 - val_loss: 11.0450 - val_acc: 0.9250\n",
      "Epoch 7/20\n",
      "12/11 [================================] - 7s 547ms/step - loss: 0.3439 - acc: 0.8542 - val_loss: 18.3536 - val_acc: 0.9000\n",
      "Epoch 8/20\n",
      "12/11 [================================] - 6s 517ms/step - loss: 0.2752 - acc: 0.8708 - val_loss: 10.1963 - val_acc: 0.9000\n",
      "Epoch 9/20\n",
      "12/11 [================================] - 6s 509ms/step - loss: 0.3198 - acc: 0.8542 - val_loss: 11.4823 - val_acc: 0.9250\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 10/20\n",
      "12/11 [================================] - 6s 513ms/step - loss: 0.1922 - acc: 0.9375 - val_loss: 15.6864 - val_acc: 0.9250\n",
      "Epoch 11/20\n",
      "12/11 [================================] - 6s 517ms/step - loss: 0.2280 - acc: 0.9083 - val_loss: 12.9034 - val_acc: 0.9250\n",
      "Epoch 12/20\n",
      "12/11 [================================] - 6s 509ms/step - loss: 0.1991 - acc: 0.9250 - val_loss: 10.6535 - val_acc: 0.9250\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Epoch 13/20\n",
      "12/11 [================================] - 7s 556ms/step - loss: 0.1903 - acc: 0.9292 - val_loss: 17.7175 - val_acc: 0.9500\n",
      "Epoch 14/20\n",
      "12/11 [================================] - 6s 523ms/step - loss: 0.1906 - acc: 0.9292 - val_loss: 14.5294 - val_acc: 0.9250\n",
      "Epoch 15/20\n",
      "12/11 [================================] - 6s 508ms/step - loss: 0.1478 - acc: 0.9583 - val_loss: 13.0769 - val_acc: 0.9250\n",
      "Epoch 16/20\n",
      "12/11 [================================] - 7s 566ms/step - loss: 0.1799 - acc: 0.9292 - val_loss: 19.9105 - val_acc: 0.9500\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 17/20\n",
      "12/11 [================================] - 6s 538ms/step - loss: 0.1208 - acc: 0.9667 - val_loss: 20.3853 - val_acc: 0.9500\n",
      "Epoch 18/20\n",
      "12/11 [================================] - 6s 503ms/step - loss: 0.2016 - acc: 0.9292 - val_loss: 17.4825 - val_acc: 0.9500\n",
      "Epoch 19/20\n",
      "12/11 [================================] - 7s 543ms/step - loss: 0.2046 - acc: 0.9208 - val_loss: 19.4668 - val_acc: 0.9500\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 20/20\n",
      "12/11 [================================] - 6s 529ms/step - loss: 0.1508 - acc: 0.9333 - val_loss: 15.4487 - val_acc: 0.9500\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.90      0.95        20\n",
      "           1       0.91      1.00      0.95        20\n",
      "\n",
      "    accuracy                           0.95        40\n",
      "   macro avg       0.95      0.95      0.95        40\n",
      "weighted avg       0.95      0.95      0.95        40\n",
      "\n",
      "Kfold iteration 2/10\n",
      "Total images: 400 ---- Train images: 360 ---- Test images: 40\n",
      "Test index start:37 and finish index: 81\n",
      "Distribution of labels: Counter({1: 20, 0: 20})\n",
      "Epoch 1/20\n",
      "12/11 [================================] - 9s 780ms/step - loss: 0.6966 - acc: 0.5417 - val_loss: 5.9637 - val_acc: 0.5500\n",
      "Epoch 2/20\n",
      "12/11 [================================] - 6s 509ms/step - loss: 0.6844 - acc: 0.5875 - val_loss: 0.3430 - val_acc: 0.9500\n",
      "Epoch 3/20\n",
      "12/11 [================================] - 6s 510ms/step - loss: 0.6325 - acc: 0.6792 - val_loss: 4.4489 - val_acc: 0.8000\n",
      "Epoch 4/20\n",
      "12/11 [================================] - 6s 535ms/step - loss: 0.5541 - acc: 0.7750 - val_loss: 33.1761 - val_acc: 0.7250\n",
      "Epoch 5/20\n",
      "12/11 [================================] - 6s 513ms/step - loss: 0.4732 - acc: 0.7708 - val_loss: 6.1822 - val_acc: 0.9250\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 6/20\n",
      "12/11 [================================] - 6s 489ms/step - loss: 0.3544 - acc: 0.8667 - val_loss: 5.5124 - val_acc: 0.9250\n",
      "Epoch 7/20\n",
      "12/11 [================================] - 6s 535ms/step - loss: 0.2985 - acc: 0.8708 - val_loss: 8.4130 - val_acc: 0.9750\n",
      "Epoch 8/20\n",
      "12/11 [================================] - 6s 538ms/step - loss: 0.3219 - acc: 0.8667 - val_loss: 1.7770 - val_acc: 0.9750\n",
      "Epoch 9/20\n",
      "12/11 [================================] - 7s 554ms/step - loss: 0.2738 - acc: 0.9125 - val_loss: 0.8293 - val_acc: 0.9750\n",
      "Epoch 10/20\n",
      "12/11 [================================] - 7s 548ms/step - loss: 0.2497 - acc: 0.9125 - val_loss: 5.0598 - val_acc: 0.9500\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Epoch 11/20\n",
      "12/11 [================================] - 6s 512ms/step - loss: 0.2617 - acc: 0.9000 - val_loss: 0.8640 - val_acc: 0.9750\n",
      "Epoch 12/20\n",
      "12/11 [================================] - 6s 509ms/step - loss: 0.2482 - acc: 0.8917 - val_loss: 4.8207e-04 - val_acc: 1.0000\n",
      "Epoch 13/20\n",
      "12/11 [================================] - 7s 542ms/step - loss: 0.2302 - acc: 0.9042 - val_loss: 0.5777 - val_acc: 0.9750\n",
      "Epoch 14/20\n",
      "12/11 [================================] - 6s 518ms/step - loss: 0.2204 - acc: 0.9083 - val_loss: 4.5518 - val_acc: 0.9750\n",
      "Epoch 15/20\n",
      "12/11 [================================] - 6s 503ms/step - loss: 0.2084 - acc: 0.9208 - val_loss: 0.0740 - val_acc: 0.9750\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 16/20\n",
      "12/11 [================================] - 7s 544ms/step - loss: 0.2123 - acc: 0.9208 - val_loss: 2.0220 - val_acc: 0.9750\n",
      "Epoch 17/20\n",
      "12/11 [================================] - 6s 530ms/step - loss: 0.1984 - acc: 0.9250 - val_loss: 4.9467 - val_acc: 0.9750\n",
      "Epoch 18/20\n",
      "12/11 [================================] - 6s 512ms/step - loss: 0.2185 - acc: 0.9125 - val_loss: 9.6388e-13 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 19/20\n",
      "12/11 [================================] - 7s 616ms/step - loss: 0.2160 - acc: 0.8917 - val_loss: 1.5576 - val_acc: 0.9750\n",
      "Epoch 20/20\n",
      "12/11 [================================] - 6s 538ms/step - loss: 0.1830 - acc: 0.9333 - val_loss: 0.2790 - val_acc: 0.9750\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97        20\n",
      "           1       0.95      1.00      0.98        20\n",
      "\n",
      "    accuracy                           0.97        40\n",
      "   macro avg       0.98      0.97      0.97        40\n",
      "weighted avg       0.98      0.97      0.97        40\n",
      "\n",
      "Kfold iteration 3/10\n",
      "Total images: 400 ---- Train images: 360 ---- Test images: 40\n",
      "Test index start:77 and finish index: 122\n",
      "Distribution of labels: Counter({1: 20, 0: 20})\n",
      "Epoch 1/20\n",
      "12/11 [================================] - 8s 677ms/step - loss: 0.7016 - acc: 0.5292 - val_loss: 11.7031 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      "12/11 [================================] - 6s 505ms/step - loss: 0.6653 - acc: 0.6250 - val_loss: 12.9763 - val_acc: 0.5750\n",
      "Epoch 3/20\n",
      "12/11 [================================] - 6s 511ms/step - loss: 0.6256 - acc: 0.6750 - val_loss: 6.1101 - val_acc: 0.8750\n",
      "Epoch 4/20\n",
      "12/11 [================================] - 6s 534ms/step - loss: 0.5503 - acc: 0.7375 - val_loss: 10.8485 - val_acc: 0.9000\n",
      "Epoch 5/20\n",
      "12/11 [================================] - 6s 535ms/step - loss: 0.4421 - acc: 0.7833 - val_loss: 126.7539 - val_acc: 0.5750\n",
      "Epoch 6/20\n",
      "12/11 [================================] - 6s 508ms/step - loss: 0.4253 - acc: 0.8042 - val_loss: 34.7510 - val_acc: 0.8750\n",
      "Epoch 7/20\n",
      "12/11 [================================] - 6s 531ms/step - loss: 0.2726 - acc: 0.9250 - val_loss: 53.4065 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/11 [================================] - 6s 515ms/step - loss: 0.2792 - acc: 0.9167 - val_loss: 47.9421 - val_acc: 0.8500\n",
      "Epoch 9/20\n",
      "12/11 [================================] - 6s 500ms/step - loss: 0.2159 - acc: 0.9083 - val_loss: 40.2088 - val_acc: 0.9000\n",
      "Epoch 10/20\n",
      "12/11 [================================] - 6s 539ms/step - loss: 0.2172 - acc: 0.9208 - val_loss: 48.3475 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Epoch 11/20\n",
      "12/11 [================================] - 6s 535ms/step - loss: 0.1921 - acc: 0.9167 - val_loss: 46.6037 - val_acc: 0.9000\n",
      "Epoch 12/20\n",
      "12/11 [================================] - 6s 514ms/step - loss: 0.1666 - acc: 0.9375 - val_loss: 50.1123 - val_acc: 0.9000\n",
      "Epoch 13/20\n",
      "12/11 [================================] - 7s 547ms/step - loss: 0.1830 - acc: 0.9292 - val_loss: 52.1014 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 14/20\n",
      "12/11 [================================] - 6s 514ms/step - loss: 0.1821 - acc: 0.9458 - val_loss: 52.4461 - val_acc: 0.9000\n",
      "Epoch 15/20\n",
      "12/11 [================================] - 6s 503ms/step - loss: 0.1963 - acc: 0.9208 - val_loss: 53.5513 - val_acc: 0.9000\n",
      "Epoch 16/20\n",
      "12/11 [================================] - 6s 535ms/step - loss: 0.1785 - acc: 0.9208 - val_loss: 53.5648 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 17/20\n",
      "12/11 [================================] - 6s 518ms/step - loss: 0.1874 - acc: 0.9250 - val_loss: 53.6722 - val_acc: 0.9000\n",
      "Epoch 18/20\n",
      "12/11 [================================] - 6s 503ms/step - loss: 0.1782 - acc: 0.9375 - val_loss: 53.7869 - val_acc: 0.9000\n",
      "Epoch 19/20\n",
      "12/11 [================================] - 6s 538ms/step - loss: 0.1526 - acc: 0.9458 - val_loss: 54.0515 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "Epoch 20/20\n",
      "12/11 [================================] - 6s 515ms/step - loss: 0.1286 - acc: 0.9458 - val_loss: 54.5933 - val_acc: 0.9000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.90      0.90        20\n",
      "           1       0.90      0.90      0.90        20\n",
      "\n",
      "    accuracy                           0.90        40\n",
      "   macro avg       0.90      0.90      0.90        40\n",
      "weighted avg       0.90      0.90      0.90        40\n",
      "\n",
      "Kfold iteration 4/10\n",
      "Total images: 400 ---- Train images: 360 ---- Test images: 40\n",
      "Test index start:116 and finish index: 162\n",
      "Distribution of labels: Counter({1: 20, 0: 20})\n",
      "Epoch 1/20\n",
      "12/11 [================================] - 8s 677ms/step - loss: 0.6796 - acc: 0.5500 - val_loss: 3.6814 - val_acc: 0.8000\n",
      "Epoch 2/20\n",
      "12/11 [================================] - 6s 522ms/step - loss: 0.6435 - acc: 0.6292 - val_loss: 3.2633 - val_acc: 0.8750\n",
      "Epoch 3/20\n",
      "12/11 [================================] - 6s 508ms/step - loss: 0.5895 - acc: 0.6875 - val_loss: 1.9187 - val_acc: 0.9500\n",
      "Epoch 4/20\n",
      "12/11 [================================] - 6s 541ms/step - loss: 0.4925 - acc: 0.7875 - val_loss: 6.3180e-12 - val_acc: 1.0000\n",
      "Epoch 5/20\n",
      "12/11 [================================] - 6s 532ms/step - loss: 0.4449 - acc: 0.8167 - val_loss: 24.8539 - val_acc: 0.7750\n",
      "Epoch 6/20\n",
      "12/11 [================================] - 6s 515ms/step - loss: 0.3437 - acc: 0.8667 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 7/20\n",
      "12/11 [================================] - 7s 545ms/step - loss: 0.3061 - acc: 0.8708 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 8/20\n",
      "12/11 [================================] - 6s 517ms/step - loss: 0.2115 - acc: 0.9333 - val_loss: 1.4457 - val_acc: 0.9750\n",
      "Epoch 9/20\n",
      "12/11 [================================] - 6s 514ms/step - loss: 0.2608 - acc: 0.8833 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 10/20\n",
      "12/11 [================================] - 7s 560ms/step - loss: 0.2438 - acc: 0.9042 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Epoch 11/20\n",
      "12/11 [================================] - 6s 531ms/step - loss: 0.2092 - acc: 0.9250 - val_loss: 5.7243e-36 - val_acc: 1.0000\n",
      "Epoch 12/20\n",
      "12/11 [================================] - 6s 514ms/step - loss: 0.2034 - acc: 0.9083 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 13/20\n",
      "12/11 [================================] - 7s 552ms/step - loss: 0.1790 - acc: 0.9375 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 14/20\n",
      "12/11 [================================] - 6s 525ms/step - loss: 0.1842 - acc: 0.9292 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 15/20\n",
      "12/11 [================================] - 6s 507ms/step - loss: 0.1636 - acc: 0.9375 - val_loss: 1.9193e-19 - val_acc: 1.0000\n",
      "Epoch 16/20\n",
      "12/11 [================================] - 6s 538ms/step - loss: 0.1683 - acc: 0.9208 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 17/20\n",
      "12/11 [================================] - 6s 526ms/step - loss: 0.2105 - acc: 0.9292 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 18/20\n",
      "12/11 [================================] - 6s 517ms/step - loss: 0.1456 - acc: 0.9375 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 19/20\n",
      "12/11 [================================] - 6s 541ms/step - loss: 0.1775 - acc: 0.9333 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "Epoch 20/20\n",
      "12/11 [================================] - 6s 516ms/step - loss: 0.1491 - acc: 0.9500 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        20\n",
      "           1       1.00      1.00      1.00        20\n",
      "\n",
      "    accuracy                           1.00        40\n",
      "   macro avg       1.00      1.00      1.00        40\n",
      "weighted avg       1.00      1.00      1.00        40\n",
      "\n",
      "Kfold iteration 5/10\n",
      "Total images: 400 ---- Train images: 360 ---- Test images: 40\n",
      "Test index start:159 and finish index: 202\n",
      "Distribution of labels: Counter({1: 20, 0: 20})\n",
      "Epoch 1/20\n",
      "12/11 [================================] - 8s 687ms/step - loss: 0.7116 - acc: 0.5375 - val_loss: 17.0632 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      "12/11 [================================] - 6s 524ms/step - loss: 0.6643 - acc: 0.5958 - val_loss: 9.7816 - val_acc: 0.5500\n",
      "Epoch 3/20\n",
      "12/11 [================================] - 6s 506ms/step - loss: 0.6227 - acc: 0.6458 - val_loss: 19.0392 - val_acc: 0.6750\n",
      "Epoch 4/20\n",
      "12/11 [================================] - 7s 562ms/step - loss: 0.5784 - acc: 0.7292 - val_loss: 10.3782 - val_acc: 0.8250\n",
      "Epoch 5/20\n",
      "12/11 [================================] - 6s 534ms/step - loss: 0.4325 - acc: 0.8750 - val_loss: 7.1820e-05 - val_acc: 1.0000\n",
      "Epoch 6/20\n",
      "12/11 [================================] - 6s 521ms/step - loss: 0.3831 - acc: 0.8458 - val_loss: 2.9814 - val_acc: 0.9500\n",
      "Epoch 7/20\n",
      "12/11 [================================] - 7s 545ms/step - loss: 0.3149 - acc: 0.8625 - val_loss: 1.3915e-16 - val_acc: 1.0000\n",
      "Epoch 8/20\n",
      "12/11 [================================] - 6s 529ms/step - loss: 0.3068 - acc: 0.8917 - val_loss: 3.5779e-18 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 9/20\n",
      "12/11 [================================] - 6s 535ms/step - loss: 0.2251 - acc: 0.9208 - val_loss: 2.6417 - val_acc: 0.9750\n",
      "Epoch 10/20\n",
      "12/11 [================================] - 7s 590ms/step - loss: 0.2028 - acc: 0.9292 - val_loss: 0.3670 - val_acc: 0.9750\n",
      "Epoch 11/20\n",
      "12/11 [================================] - 7s 549ms/step - loss: 0.2226 - acc: 0.9125 - val_loss: 4.0828 - val_acc: 0.9750\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Epoch 12/20\n",
      "12/11 [================================] - 6s 538ms/step - loss: 0.2017 - acc: 0.9167 - val_loss: 7.7061 - val_acc: 0.9750\n",
      "Epoch 13/20\n",
      "12/11 [================================] - 7s 614ms/step - loss: 0.2289 - acc: 0.9125 - val_loss: 12.8614 - val_acc: 0.9750\n",
      "Epoch 14/20\n",
      "12/11 [================================] - 7s 547ms/step - loss: 0.1741 - acc: 0.9333 - val_loss: 3.0277 - val_acc: 0.9750\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 15/20\n",
      "12/11 [================================] - 6s 499ms/step - loss: 0.1586 - acc: 0.9417 - val_loss: 8.7632 - val_acc: 0.9750\n",
      "Epoch 16/20\n",
      "12/11 [================================] - 6s 525ms/step - loss: 0.1747 - acc: 0.9333 - val_loss: 9.1228 - val_acc: 0.9750\n",
      "Epoch 17/20\n",
      "12/11 [================================] - 6s 504ms/step - loss: 0.1765 - acc: 0.9208 - val_loss: 2.4700 - val_acc: 0.9750\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 18/20\n",
      "12/11 [================================] - 6s 499ms/step - loss: 0.1655 - acc: 0.9333 - val_loss: 5.3459 - val_acc: 0.9750\n",
      "Epoch 19/20\n",
      "12/11 [================================] - 6s 531ms/step - loss: 0.1598 - acc: 0.9500 - val_loss: 5.2645 - val_acc: 0.9750\n",
      "Epoch 20/20\n",
      "12/11 [================================] - 10s 818ms/step - loss: 0.1175 - acc: 0.9583 - val_loss: 5.5123 - val_acc: 0.9750\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97        20\n",
      "           1       0.95      1.00      0.98        20\n",
      "\n",
      "    accuracy                           0.97        40\n",
      "   macro avg       0.98      0.97      0.97        40\n",
      "weighted avg       0.98      0.97      0.97        40\n",
      "\n",
      "Kfold iteration 6/10\n",
      "Total images: 400 ---- Train images: 360 ---- Test images: 40\n",
      "Test index start:197 and finish index: 243\n",
      "Distribution of labels: Counter({1: 20, 0: 20})\n",
      "Epoch 1/20\n",
      "12/11 [================================] - 12s 1s/step - loss: 0.6855 - acc: 0.5458 - val_loss: 39.0687 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      "12/11 [================================] - 9s 721ms/step - loss: 0.6777 - acc: 0.5750 - val_loss: 11.7698 - val_acc: 0.7500\n",
      "Epoch 3/20\n",
      "12/11 [================================] - 9s 711ms/step - loss: 0.6557 - acc: 0.6583 - val_loss: 11.0436 - val_acc: 0.8000\n",
      "Epoch 4/20\n",
      "12/11 [================================] - 9s 742ms/step - loss: 0.5605 - acc: 0.8000 - val_loss: 46.2251 - val_acc: 0.6500\n",
      "Epoch 5/20\n",
      "12/11 [================================] - 9s 718ms/step - loss: 0.5036 - acc: 0.7750 - val_loss: 12.6308 - val_acc: 0.9000\n",
      "Epoch 6/20\n",
      "12/11 [================================] - 9s 716ms/step - loss: 0.4651 - acc: 0.8042 - val_loss: 60.0013 - val_acc: 0.8000\n",
      "Epoch 7/20\n",
      "12/11 [================================] - 9s 749ms/step - loss: 0.3446 - acc: 0.8917 - val_loss: 33.7507 - val_acc: 0.8500\n",
      "Epoch 8/20\n",
      "12/11 [================================] - 9s 732ms/step - loss: 0.3623 - acc: 0.8375 - val_loss: 21.8644 - val_acc: 0.9500\n",
      "Epoch 9/20\n",
      "12/11 [================================] - 9s 715ms/step - loss: 0.2965 - acc: 0.8833 - val_loss: 35.7680 - val_acc: 0.8750\n",
      "Epoch 10/20\n",
      "12/11 [================================] - 9s 746ms/step - loss: 0.2502 - acc: 0.9167 - val_loss: 35.4164 - val_acc: 0.8750\n",
      "Epoch 11/20\n",
      "12/11 [================================] - 9s 721ms/step - loss: 0.3334 - acc: 0.8667 - val_loss: 21.7713 - val_acc: 0.9250\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 12/20\n",
      "12/11 [================================] - 8s 691ms/step - loss: 0.1848 - acc: 0.9375 - val_loss: 25.9520 - val_acc: 0.9250\n",
      "Epoch 13/20\n",
      "12/11 [================================] - 9s 754ms/step - loss: 0.1646 - acc: 0.9417 - val_loss: 31.6962 - val_acc: 0.9250\n",
      "Epoch 14/20\n",
      "12/11 [================================] - 9s 743ms/step - loss: 0.2279 - acc: 0.8917 - val_loss: 25.1876 - val_acc: 0.9250\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Epoch 15/20\n",
      "12/11 [================================] - 9s 725ms/step - loss: 0.1805 - acc: 0.9167 - val_loss: 29.4352 - val_acc: 0.9500\n",
      "Epoch 16/20\n",
      "12/11 [================================] - 9s 755ms/step - loss: 0.1771 - acc: 0.9250 - val_loss: 33.5999 - val_acc: 0.9500\n",
      "Epoch 17/20\n",
      "12/11 [================================] - 9s 732ms/step - loss: 0.1932 - acc: 0.9167 - val_loss: 45.1983 - val_acc: 0.9250\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 18/20\n",
      "12/11 [================================] - 9s 713ms/step - loss: 0.1512 - acc: 0.9375 - val_loss: 39.6879 - val_acc: 0.9250\n",
      "Epoch 19/20\n",
      "12/11 [================================] - 9s 755ms/step - loss: 0.1612 - acc: 0.9458 - val_loss: 36.7996 - val_acc: 0.9500\n",
      "Epoch 20/20\n",
      "12/11 [================================] - 9s 739ms/step - loss: 0.1360 - acc: 0.9500 - val_loss: 41.9007 - val_acc: 0.9250\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.85      0.92        20\n",
      "           1       0.87      1.00      0.93        20\n",
      "\n",
      "    accuracy                           0.93        40\n",
      "   macro avg       0.93      0.93      0.92        40\n",
      "weighted avg       0.93      0.93      0.92        40\n",
      "\n",
      "Kfold iteration 7/10\n",
      "Total images: 400 ---- Train images: 360 ---- Test images: 40\n",
      "Test index start:237 and finish index: 279\n",
      "Distribution of labels: Counter({1: 20, 0: 20})\n",
      "Epoch 1/20\n",
      "12/11 [================================] - 12s 1s/step - loss: 0.6900 - acc: 0.5208 - val_loss: 6.4943 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      "12/11 [================================] - 9s 729ms/step - loss: 0.6470 - acc: 0.6667 - val_loss: 1.9315 - val_acc: 0.9500\n",
      "Epoch 3/20\n",
      "12/11 [================================] - 9s 714ms/step - loss: 0.5258 - acc: 0.7583 - val_loss: 15.1467 - val_acc: 0.8750\n",
      "Epoch 4/20\n",
      "12/11 [================================] - 9s 755ms/step - loss: 0.4146 - acc: 0.8625 - val_loss: 56.4486 - val_acc: 0.7750\n",
      "Epoch 5/20\n",
      "12/11 [================================] - 9s 740ms/step - loss: 0.3582 - acc: 0.8542 - val_loss: 11.1646 - val_acc: 0.9750\n",
      "Epoch 6/20\n",
      "12/11 [================================] - 9s 720ms/step - loss: 0.2249 - acc: 0.9292 - val_loss: 27.2889 - val_acc: 0.9250\n",
      "Epoch 7/20\n",
      "12/11 [================================] - 9s 771ms/step - loss: 0.2912 - acc: 0.8750 - val_loss: 10.9380 - val_acc: 0.9500\n",
      "Epoch 8/20\n",
      "12/11 [================================] - 9s 772ms/step - loss: 0.2491 - acc: 0.9083 - val_loss: 20.4261 - val_acc: 0.9750\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 9/20\n",
      "12/11 [================================] - 10s 873ms/step - loss: 0.2406 - acc: 0.8958 - val_loss: 13.8821 - val_acc: 0.9500\n",
      "Epoch 10/20\n",
      "12/11 [================================] - 11s 906ms/step - loss: 0.2202 - acc: 0.9083 - val_loss: 16.6564 - val_acc: 0.9250\n",
      "Epoch 11/20\n",
      "12/11 [================================] - 11s 932ms/step - loss: 0.1535 - acc: 0.9542 - val_loss: 16.8958 - val_acc: 0.9500\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Epoch 12/20\n",
      "12/11 [================================] - 12s 1s/step - loss: 0.1605 - acc: 0.9417 - val_loss: 17.7099 - val_acc: 0.9500\n",
      "Epoch 13/20\n",
      "12/11 [================================] - 11s 930ms/step - loss: 0.1315 - acc: 0.9583 - val_loss: 18.8265 - val_acc: 0.9500\n",
      "Epoch 14/20\n",
      "12/11 [================================] - 9s 731ms/step - loss: 0.2272 - acc: 0.9042 - val_loss: 17.7568 - val_acc: 0.9500\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 15/20\n",
      "12/11 [================================] - 9s 710ms/step - loss: 0.1275 - acc: 0.9458 - val_loss: 18.2983 - val_acc: 0.9500\n",
      "Epoch 16/20\n",
      "12/11 [================================] - 9s 772ms/step - loss: 0.1737 - acc: 0.9375 - val_loss: 18.6090 - val_acc: 0.9500\n",
      "Epoch 17/20\n",
      "12/11 [================================] - 9s 740ms/step - loss: 0.1275 - acc: 0.9625 - val_loss: 18.7806 - val_acc: 0.9500\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 18/20\n",
      "12/11 [================================] - 9s 721ms/step - loss: 0.1386 - acc: 0.9417 - val_loss: 18.7602 - val_acc: 0.9500\n",
      "Epoch 19/20\n",
      "12/11 [================================] - 9s 757ms/step - loss: 0.1346 - acc: 0.9625 - val_loss: 18.9556 - val_acc: 0.9500\n",
      "Epoch 20/20\n",
      "12/11 [================================] - 9s 734ms/step - loss: 0.1234 - acc: 0.9542 - val_loss: 19.0563 - val_acc: 0.9500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95        20\n",
      "           1       0.95      0.95      0.95        20\n",
      "\n",
      "    accuracy                           0.95        40\n",
      "   macro avg       0.95      0.95      0.95        40\n",
      "weighted avg       0.95      0.95      0.95        40\n",
      "\n",
      "Kfold iteration 8/10\n",
      "Total images: 400 ---- Train images: 360 ---- Test images: 40\n",
      "Test index start:280 and finish index: 323\n",
      "Distribution of labels: Counter({0: 20, 1: 20})\n",
      "Epoch 1/20\n",
      "12/11 [================================] - 13s 1s/step - loss: 0.6940 - acc: 0.4875 - val_loss: 8.8172 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      "12/11 [================================] - 9s 731ms/step - loss: 0.6605 - acc: 0.6500 - val_loss: 24.0278 - val_acc: 0.5250\n",
      "Epoch 3/20\n",
      "12/11 [================================] - 9s 717ms/step - loss: 0.6335 - acc: 0.7042 - val_loss: 22.0322 - val_acc: 0.5750\n",
      "Epoch 4/20\n",
      "12/11 [================================] - 9s 747ms/step - loss: 0.5425 - acc: 0.7583 - val_loss: 24.9001 - val_acc: 0.8250\n",
      "Epoch 5/20\n",
      "12/11 [================================] - 9s 719ms/step - loss: 0.4901 - acc: 0.7667 - val_loss: 15.6097 - val_acc: 0.8750\n",
      "Epoch 6/20\n",
      "12/11 [================================] - 9s 724ms/step - loss: 0.4078 - acc: 0.8333 - val_loss: 32.8011 - val_acc: 0.8750\n",
      "Epoch 7/20\n",
      "12/11 [================================] - 9s 757ms/step - loss: 0.3320 - acc: 0.8917 - val_loss: 15.2166 - val_acc: 0.9000\n",
      "Epoch 8/20\n",
      "12/11 [================================] - 9s 733ms/step - loss: 0.3162 - acc: 0.8875 - val_loss: 27.3770 - val_acc: 0.9000\n",
      "Epoch 9/20\n",
      "12/11 [================================] - 9s 738ms/step - loss: 0.2438 - acc: 0.9000 - val_loss: 63.4471 - val_acc: 0.8250\n",
      "Epoch 10/20\n",
      "12/11 [================================] - 10s 848ms/step - loss: 0.2903 - acc: 0.8625 - val_loss: 9.8049 - val_acc: 0.9500\n",
      "Epoch 11/20\n",
      "12/11 [================================] - 9s 737ms/step - loss: 0.1980 - acc: 0.9125 - val_loss: 54.7652 - val_acc: 0.8250\n",
      "Epoch 12/20\n",
      "12/11 [================================] - 9s 738ms/step - loss: 0.1647 - acc: 0.9250 - val_loss: 63.9341 - val_acc: 0.8500\n",
      "Epoch 13/20\n",
      "12/11 [================================] - 9s 763ms/step - loss: 0.2172 - acc: 0.9250 - val_loss: 13.6880 - val_acc: 0.9250\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 14/20\n",
      "12/11 [================================] - 10s 833ms/step - loss: 0.1477 - acc: 0.9500 - val_loss: 10.8789 - val_acc: 0.9500\n",
      "Epoch 15/20\n",
      "12/11 [================================] - 9s 759ms/step - loss: 0.1805 - acc: 0.9208 - val_loss: 15.6516 - val_acc: 0.9000\n",
      "Epoch 16/20\n",
      "12/11 [================================] - 9s 770ms/step - loss: 0.1798 - acc: 0.9292 - val_loss: 10.1178 - val_acc: 0.9250\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Epoch 17/20\n",
      "12/11 [================================] - 9s 751ms/step - loss: 0.1661 - acc: 0.9375 - val_loss: 11.8508 - val_acc: 0.9750\n",
      "Epoch 18/20\n",
      "12/11 [================================] - 9s 730ms/step - loss: 0.1217 - acc: 0.9625 - val_loss: 13.2732 - val_acc: 0.9250\n",
      "Epoch 19/20\n",
      "12/11 [================================] - 10s 803ms/step - loss: 0.1318 - acc: 0.9583 - val_loss: 9.7118 - val_acc: 0.9250\n",
      "Epoch 20/20\n",
      "12/11 [================================] - 9s 790ms/step - loss: 0.1048 - acc: 0.9625 - val_loss: 12.9210 - val_acc: 0.9750\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98        20\n",
      "           1       1.00      0.95      0.97        20\n",
      "\n",
      "    accuracy                           0.97        40\n",
      "   macro avg       0.98      0.97      0.97        40\n",
      "weighted avg       0.98      0.97      0.97        40\n",
      "\n",
      "Kfold iteration 9/10\n",
      "Total images: 400 ---- Train images: 360 ---- Test images: 40\n",
      "Test index start:316 and finish index: 361\n",
      "Distribution of labels: Counter({0: 20, 1: 20})\n",
      "Epoch 1/20\n",
      "12/11 [================================] - 12s 1s/step - loss: 0.6911 - acc: 0.5417 - val_loss: 1.9946 - val_acc: 0.6500\n",
      "Epoch 2/20\n",
      "12/11 [================================] - 9s 738ms/step - loss: 0.6675 - acc: 0.6542 - val_loss: 23.7357 - val_acc: 0.5000\n",
      "Epoch 3/20\n",
      "12/11 [================================] - 9s 731ms/step - loss: 0.6134 - acc: 0.7042 - val_loss: 149.0656 - val_acc: 0.5000\n",
      "Epoch 4/20\n",
      "12/11 [================================] - 9s 773ms/step - loss: 0.5536 - acc: 0.7667 - val_loss: 1.8751 - val_acc: 0.9750\n",
      "Epoch 5/20\n",
      "12/11 [================================] - 9s 748ms/step - loss: 0.4643 - acc: 0.8292 - val_loss: 36.0435 - val_acc: 0.8500\n",
      "Epoch 6/20\n",
      "12/11 [================================] - 9s 728ms/step - loss: 0.4196 - acc: 0.8292 - val_loss: 25.3674 - val_acc: 0.8500\n",
      "Epoch 7/20\n",
      "12/11 [================================] - 9s 759ms/step - loss: 0.3905 - acc: 0.8250 - val_loss: 32.6297 - val_acc: 0.8500\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 8/20\n",
      "12/11 [================================] - 9s 746ms/step - loss: 0.2794 - acc: 0.9125 - val_loss: 1.3728 - val_acc: 0.9750\n",
      "Epoch 9/20\n",
      "12/11 [================================] - 9s 721ms/step - loss: 0.2437 - acc: 0.9125 - val_loss: 4.3555 - val_acc: 0.9500\n",
      "Epoch 10/20\n",
      "12/11 [================================] - 9s 773ms/step - loss: 0.2235 - acc: 0.9167 - val_loss: 6.3959 - val_acc: 0.9500\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Epoch 11/20\n",
      "12/11 [================================] - 9s 749ms/step - loss: 0.1943 - acc: 0.9333 - val_loss: 9.8227 - val_acc: 0.9500\n",
      "Epoch 12/20\n",
      "12/11 [================================] - 9s 725ms/step - loss: 0.2424 - acc: 0.8917 - val_loss: 3.4035e-07 - val_acc: 1.0000\n",
      "Epoch 13/20\n",
      "12/11 [================================] - 9s 767ms/step - loss: 0.2485 - acc: 0.8875 - val_loss: 0.2411 - val_acc: 0.9750\n",
      "Epoch 14/20\n",
      "12/11 [================================] - 9s 754ms/step - loss: 0.1836 - acc: 0.9333 - val_loss: 0.5398 - val_acc: 0.9750\n",
      "Epoch 15/20\n",
      "12/11 [================================] - 9s 726ms/step - loss: 0.1976 - acc: 0.9375 - val_loss: 33.0392 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 16/20\n",
      "12/11 [================================] - 9s 775ms/step - loss: 0.1993 - acc: 0.9292 - val_loss: 8.9867 - val_acc: 0.9250\n",
      "Epoch 17/20\n",
      "12/11 [================================] - 9s 738ms/step - loss: 0.1820 - acc: 0.9292 - val_loss: 25.4758 - val_acc: 0.8750\n",
      "Epoch 18/20\n",
      "12/11 [================================] - 9s 720ms/step - loss: 0.1657 - acc: 0.9375 - val_loss: 16.2163 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 19/20\n",
      "12/11 [================================] - 9s 766ms/step - loss: 0.1924 - acc: 0.9167 - val_loss: 12.4713 - val_acc: 0.9250\n",
      "Epoch 20/20\n",
      "12/11 [================================] - 9s 745ms/step - loss: 0.2094 - acc: 0.9083 - val_loss: 8.1367 - val_acc: 0.9250\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.85      0.92        20\n",
      "           1       0.87      1.00      0.93        20\n",
      "\n",
      "    accuracy                           0.93        40\n",
      "   macro avg       0.93      0.93      0.92        40\n",
      "weighted avg       0.93      0.93      0.92        40\n",
      "\n",
      "Kfold iteration 10/10\n",
      "Total images: 400 ---- Train images: 360 ---- Test images: 40\n",
      "Test index start:355 and finish index: 399\n",
      "Distribution of labels: Counter({0: 20, 1: 20})\n",
      "Epoch 1/20\n",
      "12/11 [================================] - 12s 1s/step - loss: 0.6917 - acc: 0.5542 - val_loss: 2.4229 - val_acc: 0.7750\n",
      "Epoch 2/20\n",
      "12/11 [================================] - 9s 747ms/step - loss: 0.6628 - acc: 0.5917 - val_loss: 7.6644 - val_acc: 0.7250\n",
      "Epoch 3/20\n",
      "12/11 [================================] - 10s 808ms/step - loss: 0.6074 - acc: 0.7042 - val_loss: 3.0387 - val_acc: 0.9000\n",
      "Epoch 4/20\n",
      "12/11 [================================] - 9s 766ms/step - loss: 0.5303 - acc: 0.8125 - val_loss: 42.3014 - val_acc: 0.7500\n",
      "Epoch 5/20\n",
      "12/11 [================================] - 9s 758ms/step - loss: 0.4478 - acc: 0.7875 - val_loss: 8.0611 - val_acc: 0.9250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20\n",
      "12/11 [================================] - 11s 908ms/step - loss: 0.3401 - acc: 0.8875 - val_loss: 43.5970 - val_acc: 0.8750\n",
      "Epoch 7/20\n",
      "12/11 [================================] - 10s 819ms/step - loss: 0.3552 - acc: 0.8417 - val_loss: 14.3053 - val_acc: 0.9000\n",
      "Epoch 8/20\n",
      "12/11 [================================] - 9s 760ms/step - loss: 0.2544 - acc: 0.8875 - val_loss: 50.3714 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 9/20\n",
      "12/11 [================================] - 8s 692ms/step - loss: 0.2384 - acc: 0.9167 - val_loss: 41.9693 - val_acc: 0.8500\n",
      "Epoch 10/20\n",
      "12/11 [================================] - 9s 791ms/step - loss: 0.2065 - acc: 0.9125 - val_loss: 57.6821 - val_acc: 0.8750\n",
      "Epoch 11/20\n",
      "12/11 [================================] - 9s 778ms/step - loss: 0.1795 - acc: 0.9167 - val_loss: 27.6161 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Epoch 12/20\n",
      "12/11 [================================] - 10s 806ms/step - loss: 0.1791 - acc: 0.9292 - val_loss: 29.1743 - val_acc: 0.9250\n",
      "Epoch 13/20\n",
      "12/11 [================================] - 12s 1s/step - loss: 0.1685 - acc: 0.9375 - val_loss: 27.3731 - val_acc: 0.9000\n",
      "Epoch 14/20\n",
      "12/11 [================================] - 10s 818ms/step - loss: 0.2033 - acc: 0.9292 - val_loss: 36.9768 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 15/20\n",
      "12/11 [================================] - 11s 901ms/step - loss: 0.1486 - acc: 0.9458 - val_loss: 29.8240 - val_acc: 0.9000\n",
      "Epoch 16/20\n",
      "12/11 [================================] - 9s 783ms/step - loss: 0.1802 - acc: 0.9375 - val_loss: 29.7594 - val_acc: 0.9000\n",
      "Epoch 17/20\n",
      "12/11 [================================] - 9s 765ms/step - loss: 0.1769 - acc: 0.9333 - val_loss: 28.7960 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 18/20\n",
      "12/11 [================================] - 9s 754ms/step - loss: 0.1726 - acc: 0.9375 - val_loss: 30.4278 - val_acc: 0.9000\n",
      "Epoch 19/20\n",
      "12/11 [================================] - 10s 824ms/step - loss: 0.1483 - acc: 0.9375 - val_loss: 36.1628 - val_acc: 0.9000\n",
      "Epoch 20/20\n",
      "12/11 [================================] - 11s 901ms/step - loss: 0.1897 - acc: 0.9125 - val_loss: 27.3772 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.85      0.89        20\n",
      "           1       0.86      0.95      0.90        20\n",
      "\n",
      "    accuracy                           0.90        40\n",
      "   macro avg       0.90      0.90      0.90        40\n",
      "weighted avg       0.90      0.90      0.90        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reports = []\n",
    "\n",
    "kf = StratifiedKFold(n_splits=10)\n",
    "\n",
    "# shuffle data\n",
    "id = 1\n",
    "for train_index, test_index in kf.split(x,y):\n",
    "    print('Kfold iteration {}/10'.format(id))\n",
    "    id += 1\n",
    "    print('Total images: {} ---- Train images: {} ---- Test images: {}'.format(len(x),len(train_index),len(test_index)))\n",
    "    #print(\"Train index start:{} and finish index: {}\".format(train_index[0],train_index[-1]))\n",
    "    print(\"Test index start:{} and finish index: {}\".format(test_index[0],test_index[-1]))\n",
    "    \n",
    "    X_train, X_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    print('Distribution of labels: {}'.format(collections.Counter(y_test)))\n",
    "    \n",
    "    model = build_model1()\n",
    "\n",
    "    datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                 rotation_range=10, # randomly rotate images in the range (degrees, 0 to 180)\n",
    "                                 width_shift_range=0.1, # randomly shift images horizontally (fraction of total width)\n",
    "                                 height_shift_range=0.1, \n",
    "                                 shear_range=0.1,\n",
    "                                 zoom_range=0.1)   \n",
    "    \n",
    "    datagen.fit(X_train)\n",
    "\n",
    "    learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00001)\n",
    "    \n",
    "    history = model.fit_generator(datagen.flow(X_train, y_train, batch_size = 20), epochs = 20, \n",
    "                              validation_data = (X_test,y_test), steps_per_epoch=len(X_train) / 32,\n",
    "                              callbacks=[learning_rate_reduction])\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = [np.round(p[0]) for p in y_pred]\n",
    "    \n",
    "    print(classification_report(y_test, y_pred))\n",
    "    reports.append(classification_report(y_test, y_pred,output_dict=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1s4 = []\n",
    "for rep in reports:\n",
    "    f1s4.append(rep['0']['f1-score'])\n",
    "    f1s4.append(rep['1']['f1-score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.949874686716792"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reports[0]['weighted avg']['f1-score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9473728632893634"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(f1s4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9410731116866572"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9361187755781935"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(f1s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9599498981976516"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(f1s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9545454545454545,\n",
       " 0.9444444444444444,\n",
       " 0.9523809523809523,\n",
       " 0.9473684210526315,\n",
       " 0.9032258064516129,\n",
       " 0.9387755102040817,\n",
       " 0.8292682926829269,\n",
       " 0.8205128205128205,\n",
       " 0.9444444444444444,\n",
       " 0.9545454545454546,\n",
       " 0.9583333333333334,\n",
       " 0.9375,\n",
       " 0.896551724137931,\n",
       " 0.9411764705882353,\n",
       " 0.975609756097561,\n",
       " 0.9743589743589743,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9787234042553191,\n",
       " 0.9696969696969697]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9777777777777777,\n",
       " 0.9714285714285714,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.9032258064516129,\n",
       " 0.9387755102040817,\n",
       " 0.8205128205128205,\n",
       " 0.8292682926829269,\n",
       " 0.972972972972973,\n",
       " 0.9767441860465117,\n",
       " 0.9795918367346939,\n",
       " 0.967741935483871,\n",
       " 0.8387096774193549,\n",
       " 0.8979591836734694,\n",
       " 0.975609756097561,\n",
       " 0.9743589743589743,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9565217391304348,\n",
       " 0.9411764705882353]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.8947368421052632,\n",
       " 0.9047619047619048,\n",
       " 0.8717948717948718,\n",
       " 0.8780487804878048,\n",
       " 0.975609756097561,\n",
       " 0.9743589743589743,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9743589743589743,\n",
       " 0.975609756097561,\n",
       " 0.975609756097561,\n",
       " 0.9743589743589743,\n",
       " 0.9523809523809523,\n",
       " 0.9473684210526316,\n",
       " 0.9500000000000001,\n",
       " 0.9500000000000001]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
